{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Tailor Agent\n",
    "\n",
    "An intelligent agent that tailors your LaTeX resume to specific job postings while preserving formatting and maintaining accuracy.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **LaTeX-Safe**: Preserves LaTeX formatting and syntax\n",
    "- **Iterative**: Supports multiple revision rounds\n",
    "- **Job-Focused**: Analyzes job postings and matches requirements\n",
    "- **ATS-Optimized**: Uses keywords naturally for applicant tracking systems\n",
    "- **Validation**: Checks LaTeX syntax before output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## API Provider Configuration\n\nThis notebook supports multiple AI providers. Configure your credentials in the `.env` file:\n\n### Option 1: OpenAI (Recommended for getting started)\n```bash\nOPENAI_API_KEY=sk-your-openai-key-here\n```\n\n### Option 2: AWS Bedrock (Production-ready)\n```bash\n# Using long-term API key (recommended)\nAWS_BEARER_TOKEN_BEDROCK=your-long-term-bedrock-key\nAWS_REGION=us-east-1\n\n# OR using standard AWS credentials\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nAWS_REGION=us-east-1\n```\n\nThe notebook will automatically detect which credentials are available and use them.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Strands SDK\nfrom strands import Agent, tool\n\n# Utilities\nimport json\nfrom datetime import datetime\n\n# Load environment variables\nload_dotenv()\n\nprint(\"‚úÖ Imports successful!\")\nprint(f\"Python Path: {Path.cwd()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and verify environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project paths\nPROJECT_ROOT = Path.cwd()\nPROMPTS_DIR = PROJECT_ROOT / \"prompts\"\nDATA_DIR = PROJECT_ROOT / \"data\"\nORIGINAL_RESUME_DIR = DATA_DIR / \"original\"\nJOB_POSTINGS_DIR = DATA_DIR / \"job_postings\"\nOUTPUT_DIR = DATA_DIR / \"tailored_versions\"\n\n# Detect which API credentials are available\nprint(\"üîç Checking API credentials...\")\nprint()\n\nhas_openai = bool(os.getenv('OPENAI_API_KEY'))\nhas_bedrock_token = bool(os.getenv('AWS_BEARER_TOKEN_BEDROCK'))\nhas_aws_creds = bool(os.getenv('AWS_ACCESS_KEY_ID'))\n\nif has_openai:\n    print(\"‚úÖ OpenAI API key found\")\n    MODEL_PROVIDER = \"openai\"\n    MODEL_ID = \"gpt-4o\"  # or gpt-4-turbo, gpt-4\nelif has_bedrock_token:\n    print(\"‚úÖ AWS Bedrock bearer token found\")\n    MODEL_PROVIDER = \"bedrock\"\n    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\nelif has_aws_creds:\n    print(\"‚úÖ AWS credentials found\")\n    MODEL_PROVIDER = \"bedrock\"\n    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\nelse:\n    print(\"‚ö†Ô∏è  Warning: No API credentials found!\")\n    print(\"Please set one of the following in .env file:\")\n    print(\"  - OPENAI_API_KEY (for OpenAI)\")\n    print(\"  - AWS_BEARER_TOKEN_BEDROCK (for Bedrock)\")\n    print(\"  - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY (for AWS)\")\n    MODEL_PROVIDER = None\n    MODEL_ID = None\n\nprint()\nprint(f\"üì° Selected Provider: {MODEL_PROVIDER}\")\nprint(f\"ü§ñ Selected Model: {MODEL_ID}\")\n\n# Verify directories exist\nprint()\nprint(f\"üìÅ Project directories:\")\nprint(f\"  Prompts: {PROMPTS_DIR.exists()} - {PROMPTS_DIR}\")\nprint(f\"  Data: {DATA_DIR.exists()} - {DATA_DIR}\")\nprint(f\"  Output: {OUTPUT_DIR.exists()} - {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts\n",
    "\n",
    "Load agent instructions from separate files for easy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(filename: str) -> str:\n",
    "    \"\"\"Load a prompt from the prompts directory.\"\"\"\n",
    "    prompt_path = PROMPTS_DIR / filename\n",
    "    if not prompt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found. Using default prompt.\")\n",
    "        return \"\"\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ Loaded {filename} ({len(content)} chars)\")\n",
    "    return content\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = load_prompt(\"system_prompt.txt\")\n",
    "latex_rules = load_prompt(\"latex_rules.txt\")\n",
    "\n",
    "# Combine prompts\n",
    "full_prompt = f\"{system_prompt}\\n\\n{latex_rules}\".strip()\n",
    "\n",
    "print(f\"\\nüìù Full system prompt: {len(full_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools for Resume Tailoring\n",
    "\n",
    "Define specialized tools for LaTeX resume processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a file and return its contents.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "    \n",
    "    Returns:\n",
    "        The file contents as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write content to a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "        content: Content to write\n",
    "    \n",
    "    Returns:\n",
    "        Success message with file path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return f\"Successfully wrote {len(content)} characters to {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_latex(latex_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Validate LaTeX syntax by checking for common issues.\n",
    "    \n",
    "    Args:\n",
    "        latex_content: The LaTeX content to validate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results (is_valid, errors, warnings)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for balanced braces\n",
    "    if latex_content.count('{') != latex_content.count('}'):\n",
    "        errors.append(\"Unbalanced curly braces { }\")\n",
    "    \n",
    "    # Check for balanced brackets\n",
    "    if latex_content.count('[') != latex_content.count(']'):\n",
    "        errors.append(\"Unbalanced square brackets [ ]\")\n",
    "    \n",
    "    # Check for document structure\n",
    "    if '\\\\documentclass' not in latex_content:\n",
    "        warnings.append(\"No \\\\documentclass found\")\n",
    "    \n",
    "    if '\\\\begin{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\begin{document}\")\n",
    "    \n",
    "    if '\\\\end{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\end{document}\")\n",
    "    \n",
    "    # Check for common LaTeX commands\n",
    "    lines = latex_content.split('\\n')\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        # Check for unescaped special characters in regular text\n",
    "        if '%' in line and '\\\\%' not in line:\n",
    "            # This might be a comment, so it's just a warning\n",
    "            pass\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": is_valid,\n",
    "        \"errors\": errors,\n",
    "        \"warnings\": warnings,\n",
    "        \"summary\": f\"{'‚úÖ Valid' if is_valid else '‚ùå Invalid'} LaTeX ({len(errors)} errors, {len(warnings)} warnings)\"\n",
    "    }\n",
    "\n",
    "\n",
    "@tool\n",
    "def extract_keywords(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract important keywords from text (job posting or resume section).\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract keywords from\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords (skills, technologies, requirements)\n",
    "    \"\"\"\n",
    "    # Common technical keywords and skills\n",
    "    import re\n",
    "    \n",
    "    # Simple keyword extraction (can be enhanced with NLP)\n",
    "    keywords = set()\n",
    "    \n",
    "    # Common technical skills patterns\n",
    "    patterns = [\n",
    "        r'\\b(Python|Java|JavaScript|TypeScript|C\\+\\+|Ruby|Go|Rust|Swift)\\b',\n",
    "        r'\\b(AWS|Azure|GCP|Docker|Kubernetes|Jenkins)\\b',\n",
    "        r'\\b(React|Angular|Vue|Node\\.js|Django|Flask|Spring)\\b',\n",
    "        r'\\b(SQL|PostgreSQL|MySQL|MongoDB|Redis)\\b',\n",
    "        r'\\b(Git|CI/CD|Agile|Scrum|DevOps|REST|API)\\b',\n",
    "        r'\\b(Machine Learning|AI|Data Science|Analytics)\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            keywords.add(match.group(1))\n",
    "    \n",
    "    return sorted(list(keywords))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom tools defined:\")\n",
    "print(\"  - read_file()\")\n",
    "print(\"  - write_file()\")\n",
    "print(\"  - validate_latex()\")\n",
    "print(\"  - extract_keywords()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Resume Tailor Agent\n",
    "\n",
    "Initialize the agent with system prompts and custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create agent with automatic provider detection\nif MODEL_PROVIDER is None:\n    print(\"‚ùå Cannot create agent: No API credentials found\")\n    print(\"Please configure API credentials in .env file\")\nelse:\n    # Create agent with detected provider\n    resume_agent = Agent(\n        model_provider=MODEL_PROVIDER,\n        model_id=MODEL_ID,\n        system_prompt=full_prompt if full_prompt else \"You are a helpful resume tailoring assistant.\",\n        tools=[\n            read_file,\n            write_file,\n            validate_latex,\n            extract_keywords\n        ]\n    )\n\n    print(\"‚úÖ Resume Tailor Agent created!\")\n    print(f\"   Provider: {MODEL_PROVIDER}\")\n    print(f\"   Model: {MODEL_ID}\")\n    print(f\"   Tools: {len(resume_agent.tools)} custom tools\")\n    print(f\"   System prompt: {len(full_prompt) if full_prompt else 0} characters\")\n    print()\n    print(\"üí° Tip: You can change the model by editing MODEL_PROVIDER and MODEL_ID in the configuration cell above\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "Below are examples of how to use the resume tailor agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Test Agent Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "response = resume_agent(\"Hello! Can you help me tailor my resume?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Analyze a Job Posting\n",
    "\n",
    "First, create a sample job posting file or use an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample job posting for testing\n",
    "sample_job = \"\"\"\n",
    "Senior Software Engineer - Machine Learning\n",
    "\n",
    "Requirements:\n",
    "- 5+ years of experience in Python and machine learning\n",
    "- Strong background in AWS cloud services\n",
    "- Experience with PyTorch or TensorFlow\n",
    "- Proficiency in SQL and data processing\n",
    "- Excellent communication and teamwork skills\n",
    "\n",
    "Preferred:\n",
    "- PhD in Computer Science or related field\n",
    "- Experience with MLOps and model deployment\n",
    "- Knowledge of Docker and Kubernetes\n",
    "\"\"\"\n",
    "\n",
    "# Save sample job posting\n",
    "job_file = JOB_POSTINGS_DIR / \"sample_ml_job.txt\"\n",
    "job_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(job_file, 'w') as f:\n",
    "    f.write(sample_job)\n",
    "\n",
    "print(f\"‚úÖ Sample job posting created at: {job_file}\")\n",
    "\n",
    "# Ask agent to analyze the job posting\n",
    "response = resume_agent(\n",
    "    f\"Read the job posting from '{job_file.relative_to(PROJECT_ROOT)}' \"\n",
    "    \"and extract the key requirements and skills.\"\n",
    ")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGENT ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Resume Tailoring Workflow\n",
    "\n",
    "**Note**: You'll need to place your actual LaTeX resume in `data/original/resume.tex`"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Reset the agent - creates a fresh instance with no conversation history\nif MODEL_PROVIDER is None:\n    print(\"‚ùå Cannot create agent: No API credentials found\")\nelse:\n    resume_agent = Agent(\n        model_provider=MODEL_PROVIDER,\n        model_id=MODEL_ID,\n        system_prompt=full_prompt if full_prompt else \"You are a helpful resume tailoring assistant.\",\n        tools=[\n            read_file,\n            write_file,\n            validate_latex,\n            extract_keywords\n        ]\n    )\n    \n    print(\"üîÑ Agent reset successfully!\")\n    print(f\"   Provider: {MODEL_PROVIDER}\")\n    print(f\"   Model: {MODEL_ID}\")\n    print(\"   ‚úÖ Fresh conversation - no previous context\")\n    print()\n    print(\"üí° You can now start a new tailoring project with a clean slate\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üîÑ Reset Agent (Optional)\n\nRun this cell to reset the agent's conversation history without restarting the kernel. This clears all previous conversations while keeping your setup intact.\n\n**When to use this:**\n- Starting a new job posting/tailoring project\n- Agent seems confused or has too much context\n- You want a fresh conversation without re-running Examples 1-2",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "original_resume = \"data/original/resume.tex\"  # Your resume here\n",
    "job_posting = \"data/job_postings/sample_ml_job.txt\"  # Job posting\n",
    "output_file = \"data/tailored_versions/resume_ml_engineer.tex\"  # Output\n",
    "\n",
    "# Instructions for the agent\n",
    "tailoring_request = f\"\"\"\n",
    "I need to tailor my resume for a specific job posting. Please:\n",
    "\n",
    "1. Read my resume from: {original_resume}\n",
    "2. Read the job posting from: {job_posting}\n",
    "3. Analyze the job requirements and match them to my experience\n",
    "4. Suggest specific improvements to tailor my resume\n",
    "5. IMPORTANT: Preserve all LaTeX formatting and syntax\n",
    "\n",
    "Do NOT generate the full tailored resume yet - just provide analysis and suggestions first.\n",
    "\"\"\"\n",
    "\n",
    "# Get initial analysis\n",
    "print(\"Analyzing resume and job posting...\\n\")\n",
    "analysis = resume_agent(tailoring_request)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Iterative Refinement\n",
    "\n",
    "After getting suggestions, you can iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation for refinement\n",
    "refinement_request = \"\"\"\n",
    "Based on your analysis, please:\n",
    "1. Focus on highlighting my AWS and Python experience\n",
    "2. Emphasize any machine learning projects\n",
    "3. Ensure keywords match the job posting for ATS\n",
    "4. Keep the resume to 1 page if possible\n",
    "\n",
    "Show me the specific sections that should change.\n",
    "\"\"\"\n",
    "\n",
    "refinement = resume_agent(refinement_request)\n",
    "print(refinement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Generate Final Tailored Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final version\n",
    "final_request = f\"\"\"\n",
    "Please generate the final tailored resume based on our discussion.\n",
    "\n",
    "Requirements:\n",
    "1. Apply all the improvements we discussed\n",
    "2. PRESERVE all LaTeX syntax and formatting\n",
    "3. Validate the LaTeX before saving\n",
    "4. Save the result to: {output_file}\n",
    "\n",
    "After saving, confirm that the LaTeX is valid.\n",
    "\"\"\"\n",
    "\n",
    "result = resume_agent(final_request)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct tool invocation for validation\n",
    "if Path(output_file).exists():\n",
    "    with open(output_file, 'r') as f:\n",
    "        tailored_content = f.read()\n",
    "    \n",
    "    # Validate using tool directly\n",
    "    validation = resume_agent.tool.validate_latex(latex_content=tailored_content)\n",
    "    \n",
    "    print(\"Validation Results:\")\n",
    "    print(f\"  Valid: {validation['is_valid']}\")\n",
    "    print(f\"  Errors: {len(validation['errors'])}\")\n",
    "    print(f\"  Warnings: {len(validation['warnings'])}\")\n",
    "    \n",
    "    if validation['errors']:\n",
    "        print(\"\\nErrors found:\")\n",
    "        for error in validation['errors']:\n",
    "            print(f\"  - {error}\")\n",
    "else:\n",
    "    print(f\"File not found: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "Utility functions for common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_tailor(resume_path: str, job_path: str, output_path: str, instructions: str = \"\"):\n",
    "    \"\"\"\n",
    "    Quick one-shot resume tailoring.\n",
    "    \n",
    "    Args:\n",
    "        resume_path: Path to original resume\n",
    "        job_path: Path to job posting\n",
    "        output_path: Path for tailored resume\n",
    "        instructions: Additional instructions for the agent\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Tailor my resume for this job posting.\n",
    "\n",
    "Resume: {resume_path}\n",
    "Job Posting: {job_path}\n",
    "Output: {output_path}\n",
    "\n",
    "Steps:\n",
    "1. Read both files\n",
    "2. Analyze job requirements\n",
    "3. Tailor resume content (preserve LaTeX formatting)\n",
    "4. Validate LaTeX syntax\n",
    "5. Save to output path\n",
    "\n",
    "{instructions if instructions else ''}\n",
    "\"\"\"\n",
    "    \n",
    "    response = resume_agent(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def batch_tailor(resume_path: str, job_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Tailor resume for multiple job postings.\n",
    "    \n",
    "    Args:\n",
    "        resume_path: Path to original resume\n",
    "        job_folder: Folder containing job posting files\n",
    "        output_folder: Folder for tailored resumes\n",
    "    \"\"\"\n",
    "    job_dir = Path(job_folder)\n",
    "    output_dir = Path(output_folder)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for job_file in job_dir.glob(\"*.txt\"):\n",
    "        output_name = f\"resume_{job_file.stem}.tex\"\n",
    "        output_path = output_dir / output_name\n",
    "        \n",
    "        print(f\"\\nTailoring for: {job_file.name}\")\n",
    "        result = quick_tailor(resume_path, str(job_file), str(output_path))\n",
    "        results.append({\"job\": job_file.name, \"output\": output_name, \"result\": result})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined:\")\n",
    "print(\"  - quick_tailor()\")\n",
    "print(\"  - batch_tailor()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add your resume**: Place your LaTeX resume in `data/original/resume.tex`\n",
    "2. **Add job postings**: Save job postings as `.txt` files in `data/job_postings/`\n",
    "3. **Run tailoring**: Use the examples above to tailor your resume\n",
    "4. **Iterate**: Work with the agent to refine the output\n",
    "5. **Validate**: Check LaTeX syntax before compiling\n",
    "6. **Compile**: Use `pdflatex` or your LaTeX editor to generate PDF\n",
    "\n",
    "### Tips for Best Results\n",
    "\n",
    "- Start with analysis and suggestions before generating the full resume\n",
    "- Be specific about what aspects to highlight\n",
    "- Review the agent's suggestions before applying them\n",
    "- Always validate LaTeX syntax\n",
    "- Keep conversation context for iterative improvements\n",
    "- Save different versions for different job types\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **LaTeX errors**: Use `validate_latex()` tool to check syntax\n",
    "- **Agent not following instructions**: Refine the system prompt in `prompts/system_prompt.txt`\n",
    "- **Missing features**: Add custom tools as needed\n",
    "- **Context lost**: Use conversation memory or save intermediate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Strands Agent SDK",
   "language": "python",
   "name": "strands-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}