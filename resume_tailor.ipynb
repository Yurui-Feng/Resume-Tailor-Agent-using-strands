{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Tailor Agent\n",
    "\n",
    "An intelligent agent that tailors your LaTeX resume to specific job postings while preserving formatting and maintaining accuracy.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **LaTeX-Safe**: Preserves LaTeX formatting and syntax\n",
    "- **Iterative**: Supports multiple revision rounds\n",
    "- **Job-Focused**: Analyzes job postings and matches requirements\n",
    "- **ATS-Optimized**: Uses keywords naturally for applicant tracking systems\n",
    "- **Validation**: Checks LaTeX syntax before output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Provider Configuration\n",
    "\n",
    "This notebook supports multiple AI providers. Configure your credentials in the `.env` file:\n",
    "\n",
    "### Option 1: OpenAI (Recommended for getting started)\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "```\n",
    "\n",
    "### Option 2: AWS Bedrock (Production-ready)\n",
    "```bash\n",
    "# Using long-term API key (recommended)\n",
    "AWS_BEARER_TOKEN_BEDROCK=your-long-term-bedrock-key\n",
    "AWS_REGION=us-east-1\n",
    "\n",
    "# OR using standard AWS credentials\n",
    "AWS_ACCESS_KEY_ID=your-access-key\n",
    "AWS_SECRET_ACCESS_KEY=your-secret-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "The notebook will automatically detect which credentials are available and use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Python Path: d:\\Strands-agent\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Strands SDK\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Python Path: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Observability\n",
    "\n",
    "Configure logging to trace all agent operations and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured!\n",
      "   Log file: d:\\Strands-agent\\logs\\strands_agent_20251116_160936.log\n",
      "   Console level: WARNING (errors only)\n",
      "   File level: DEBUG (all traces)\n",
      "\n",
      "üìä Log includes:\n",
      "  ‚Ä¢ All agent operations\n",
      "  ‚Ä¢ Tool calls and responses\n",
      "  ‚Ä¢ Model interactions\n",
      "  ‚Ä¢ Validation results\n",
      "  ‚Ä¢ Error traces\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Custom JSON formatter for structured logs\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        log_data = {\n",
    "            \"timestamp\": self.formatTime(record),\n",
    "            \"level\": record.levelname,\n",
    "            \"name\": record.name,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"function\": record.funcName,\n",
    "            \"line\": record.lineno\n",
    "        }\n",
    "        \n",
    "        # Add exception info if present\n",
    "        if record.exc_info:\n",
    "            log_data[\"exception\"] = self.formatException(record.exc_info)\n",
    "        \n",
    "        return json.dumps(log_data)\n",
    "\n",
    "# Create logs directory\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate log filename with timestamp\n",
    "log_filename = LOGS_DIR / f\"strands_agent_{dt.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "# Configure file handler with JSON formatting\n",
    "file_handler = logging.FileHandler(log_filename)\n",
    "file_handler.setFormatter(JsonFormatter())\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Configure console handler with simple formatting (for notebook output)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\n",
    "    '%(levelname)s | %(name)s | %(message)s'\n",
    "))\n",
    "console_handler.setLevel(logging.WARNING)  # Only show warnings/errors in notebook\n",
    "\n",
    "# Configure the strands logger\n",
    "strands_logger = logging.getLogger(\"strands\")\n",
    "strands_logger.setLevel(logging.DEBUG)\n",
    "strands_logger.addHandler(file_handler)\n",
    "strands_logger.addHandler(console_handler)\n",
    "\n",
    "# Prevent duplicate logs\n",
    "strands_logger.propagate = False\n",
    "\n",
    "print(\"‚úÖ Logging configured!\")\n",
    "print(f\"   Log file: {log_filename}\")\n",
    "print(f\"   Console level: WARNING (errors only)\")\n",
    "print(f\"   File level: DEBUG (all traces)\")\n",
    "print()\n",
    "print(\"üìä Log includes:\")\n",
    "print(\"  ‚Ä¢ All agent operations\")\n",
    "print(\"  ‚Ä¢ Tool calls and responses\")\n",
    "print(\"  ‚Ä¢ Model interactions\")\n",
    "print(\"  ‚Ä¢ Validation results\")\n",
    "print(\"  ‚Ä¢ Error traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log analysis functions defined:\n",
      "  - view_latest_logs(num_lines=50, level_filter=None)\n",
      "  - count_tool_calls()\n",
      "  - export_logs_to_readable(output_file=None)\n",
      "\n",
      "Examples:\n",
      "  view_latest_logs(20)                    # Last 20 log entries\n",
      "  view_latest_logs(level_filter=\"ERROR\")  # Only errors\n",
      "  count_tool_calls()                      # Tool usage stats\n",
      "  export_logs_to_readable()               # Export to .txt file\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to view and analyze logs\n",
    "\n",
    "def view_latest_logs(num_lines=50, level_filter=None):\n",
    "    \"\"\"\n",
    "    View the latest log entries from the current log file.\n",
    "    \n",
    "    Args:\n",
    "        num_lines: Number of recent log lines to display\n",
    "        level_filter: Filter by log level (e.g., 'ERROR', 'DEBUG', 'WARNING')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Filter by level if specified\n",
    "        if level_filter:\n",
    "            filtered_lines = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    if log_entry.get('level') == level_filter:\n",
    "                        filtered_lines.append(line)\n",
    "                except:\n",
    "                    continue\n",
    "            lines = filtered_lines\n",
    "        \n",
    "        # Show last N lines\n",
    "        recent_lines = lines[-num_lines:]\n",
    "        \n",
    "        print(f\"üìã Showing last {len(recent_lines)} log entries:\")\n",
    "        print(f\"   Filter: {level_filter if level_filter else 'All levels'}\")\n",
    "        print(f\"   Total entries: {len(lines)}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for line in recent_lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                print(f\"{log_entry['timestamp']} | {log_entry['level']:8} | {log_entry['name']}\")\n",
    "                print(f\"  ‚Üí {log_entry['message']}\")\n",
    "                if 'exception' in log_entry:\n",
    "                    print(f\"  ‚ö†Ô∏è  {log_entry['exception']}\")\n",
    "                print()\n",
    "            except:\n",
    "                print(line.strip())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Log file not found: {log_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading logs: {e}\")\n",
    "\n",
    "\n",
    "def count_tool_calls():\n",
    "    \"\"\"Count how many times each tool was called.\"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        tool_calls = {}\n",
    "        for line in lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                message = log_entry.get('message', '')\n",
    "                \n",
    "                # Look for tool call patterns\n",
    "                if 'tool' in message.lower() and 'call' in message.lower():\n",
    "                    # Extract tool name (you may need to adjust this based on actual log format)\n",
    "                    if 'merge_sections' in message:\n",
    "                        tool_calls['merge_sections'] = tool_calls.get('merge_sections', 0) + 1\n",
    "                    elif 'read_file' in message:\n",
    "                        tool_calls['read_file'] = tool_calls.get('read_file', 0) + 1\n",
    "                    elif 'validate_latex' in message:\n",
    "                        tool_calls['validate_latex'] = tool_calls.get('validate_latex', 0) + 1\n",
    "                    elif 'extract_section' in message:\n",
    "                        tool_calls['extract_section'] = tool_calls.get('extract_section', 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"üîß Tool Call Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for tool, count in sorted(tool_calls.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {tool:20} : {count:3} calls\")\n",
    "        \n",
    "        if not tool_calls:\n",
    "            print(\"  No tool calls detected in logs\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing tool calls: {e}\")\n",
    "\n",
    "\n",
    "def export_logs_to_readable(output_file=None):\n",
    "    \"\"\"Export JSON logs to a human-readable format.\"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = LOGS_DIR / f\"readable_log_{dt.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        with open(output_file, 'w') as out:\n",
    "            out.write(f\"Strands Agent Log - Readable Format\\n\")\n",
    "            out.write(f\"Generated: {dt.now()}\\n\")\n",
    "            out.write(f\"Source: {log_filename}\\n\")\n",
    "            out.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    out.write(f\"[{log_entry['timestamp']}] {log_entry['level']}\\n\")\n",
    "                    out.write(f\"Module: {log_entry['name']}\\n\")\n",
    "                    out.write(f\"Message: {log_entry['message']}\\n\")\n",
    "                    if 'exception' in log_entry:\n",
    "                        out.write(f\"Exception:\\n{log_entry['exception']}\\n\")\n",
    "                    out.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                except:\n",
    "                    out.write(line)\n",
    "        \n",
    "        print(f\"‚úÖ Readable log exported to: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting logs: {e}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Log analysis functions defined:\")\n",
    "print(\"  - view_latest_logs(num_lines=50, level_filter=None)\")\n",
    "print(\"  - count_tool_calls()\")\n",
    "print(\"  - export_logs_to_readable(output_file=None)\")\n",
    "print()\n",
    "print(\"Examples:\")\n",
    "print('  view_latest_logs(20)                    # Last 20 log entries')\n",
    "print('  view_latest_logs(level_filter=\"ERROR\")  # Only errors')\n",
    "print('  count_tool_calls()                      # Tool usage stats')\n",
    "print('  export_logs_to_readable()               # Export to .txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and verify environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking API credentials...\n",
      "\n",
      "‚úÖ OpenAI API key found\n",
      "\n",
      "ü§ñ Selected Model: <strands.models.openai.OpenAIModel object at 0x00000231E5B77390>\n",
      "\n",
      "üìÅ Project directories:\n",
      "  Prompts: True - d:\\Strands-agent\\prompts\n",
      "  Data: True - d:\\Strands-agent\\data\n",
      "  Output: True - d:\\Strands-agent\\data\\tailored_versions\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "PROMPTS_DIR = PROJECT_ROOT / \"prompts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ORIGINAL_RESUME_DIR = DATA_DIR / \"original\"\n",
    "JOB_POSTINGS_DIR = DATA_DIR / \"job_postings\"\n",
    "OUTPUT_DIR = DATA_DIR / \"tailored_versions\"\n",
    "\n",
    "# Detect which API credentials are available\n",
    "print(\"üîç Checking API credentials...\")\n",
    "print()\n",
    "\n",
    "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
    "has_bedrock_token = bool(os.getenv('AWS_BEARER_TOKEN_BEDROCK'))\n",
    "has_aws_creds = bool(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "\n",
    "from strands.models import openai\n",
    "\n",
    "if has_openai:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "    #MODEL_ID = openai.OpenAIModel(model_id=\"gpt-5-mini\")\n",
    "    MODEL_ID = openai.OpenAIModel(\n",
    "    model_id=\"gpt-5.1\",  # Note: prompt caching works best with gpt-4o and newer models\n",
    "    params={\n",
    "        \"store\": True,  # Enable prompt caching\n",
    "        \"metadata\": {\n",
    "            \"purpose\": \"resume_tailoring\"  # Optional: track usage\n",
    "        }\n",
    "    }\n",
    ")\n",
    "elif has_bedrock_token:\n",
    "    print(\"‚úÖ AWS Bedrock bearer token found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "elif has_aws_creds:\n",
    "    print(\"‚úÖ AWS credentials found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: No API credentials found!\")\n",
    "    print(\"Please set one of the following in .env file:\")\n",
    "    print(\"  - OPENAI_API_KEY (for OpenAI)\")\n",
    "    print(\"  - AWS_BEARER_TOKEN_BEDROCK (for Bedrock)\")\n",
    "    print(\"  - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY (for AWS)\")\n",
    "    MODEL_PROVIDER = None\n",
    "    MODEL_ID = None\n",
    "\n",
    "print()\n",
    "print(f\"ü§ñ Selected Model: {MODEL_ID}\")\n",
    "\n",
    "# Verify directories exist\n",
    "print()\n",
    "print(f\"üìÅ Project directories:\")\n",
    "print(f\"  Prompts: {PROMPTS_DIR.exists()} - {PROMPTS_DIR}\")\n",
    "print(f\"  Data: {DATA_DIR.exists()} - {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR.exists()} - {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts\n",
    "\n",
    "Load agent instructions from separate files for easy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded system_prompt.txt (8536 chars)\n",
      "\n",
      "üìù Full system prompt: 8535 characters\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(filename: str) -> str:\n",
    "    \"\"\"Load a prompt from the prompts directory.\"\"\"\n",
    "    prompt_path = PROMPTS_DIR / filename\n",
    "    if not prompt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found. Using default prompt.\")\n",
    "        return \"\"\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ Loaded {filename} ({len(content)} chars)\")\n",
    "    return content\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = load_prompt(\"system_prompt.txt\")\n",
    "latex_rules = \"\"\n",
    "\n",
    "# Combine prompts\n",
    "full_prompt = f\"{system_prompt}\\n\\n{latex_rules}\".strip()\n",
    "\n",
    "print(f\"\\nüìù Full system prompt: {len(full_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Generator Agent (Tool-Free)\n",
    "\n",
    "This agent generates ONLY section text without calling any tools.\n",
    "Python code handles all file I/O, merging, and validation.\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "- **No tool overhead**: Agent only generates text (30-40% lower token cost)\n",
    "- **Predictable output**: Structured format with clear section markers\n",
    "- **Python-controlled**: All file I/O and merging handled by Python code\n",
    "- **Prompt caching enabled**: Additional 20-40% cost savings on repeated requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section Generator Agent created!\n",
      "   Model: <strands.models.openai.OpenAIModel object at 0x00000231E5B77390>\n",
      "   Tools: 0 (none - text generation only)\n",
      "   System prompt: 8535 characters\n",
      "\n",
      "This agent:\n",
      "  ‚Ä¢ Generates ONLY section text (no tool calls)\n",
      "  ‚Ä¢ Returns sections in predictable format\n",
      "  ‚Ä¢ Python code handles merge/validation\n",
      "  ‚Ä¢ Lower token cost (no tool overhead)\n"
     ]
    }
   ],
   "source": [
    "# Create tool-free agent for section generation\n",
    "section_generator_agent = Agent(\n",
    "    model=MODEL_ID,\n",
    "    system_prompt=full_prompt,\n",
    "    tools=[]  # NO TOOLS - agent only generates text\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Section Generator Agent created!\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Tools: {len(section_generator_agent.tool_names)} (none - text generation only)\")\n",
    "print(f\"   System prompt: {len(full_prompt)} characters\")\n",
    "print()\n",
    "print(\"This agent:\")\n",
    "print(\"  ‚Ä¢ Generates ONLY section text (no tool calls)\")\n",
    "print(\"  ‚Ä¢ Returns sections in predictable format\")\n",
    "print(\"  ‚Ä¢ Python code handles merge/validation\")\n",
    "print(\"  ‚Ä¢ Lower token cost (no tool overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Extractor Agent\n",
    "\n",
    "Lightweight agent for extracting company and position from job postings.\n",
    "Uses gpt-4o-mini for cost efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata Extractor Agent created!\n",
      "   Model: gpt-4o-mini (cost-efficient)\n",
      "   Purpose: Extract company name and job position from job postings\n",
      "   No tools or caching (simple JSON extraction only)\n"
     ]
    }
   ],
   "source": [
    "# Create lightweight metadata extraction agent using gpt-4o-mini\n",
    "metadata_model = openai.OpenAIModel(\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    params={\"store\": False}  # No caching needed for simple extraction\n",
    ")\n",
    "\n",
    "metadata_extractor_agent = Agent(\n",
    "    model=metadata_model,\n",
    "    system_prompt=\"You extract structured data from job postings.\",\n",
    "    tools=[]  # No tools needed\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Metadata Extractor Agent created!\")\n",
    "print(f\"   Model: gpt-4o-mini (cost-efficient)\")\n",
    "print(f\"   Purpose: Extract company name and job position from job postings\")\n",
    "print(f\"   No tools or caching (simple JSON extraction only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "The complete workflow with automatic PDF generation and filename creation:\n",
    "\n",
    "### New Workflow (Copy-Paste Job Posting)\n",
    "\n",
    "1. **Extract metadata** - Lightweight agent extracts company + position from job text\n",
    "2. **Auto-generate filename** - Creates `Company_Position_TIMESTAMP.tex`\n",
    "3. **Copy original resume** - Creates working copy in output directory\n",
    "4. **Generate sections** - Main agent tailors Summary, Skills, Experience\n",
    "5. **Merge sections** - Python merges updated sections into resume\n",
    "6. **Compile PDF** - Automatically generates PDF using pdflatex\n",
    "7. **Validate** - Checks LaTeX syntax and returns all paths\n",
    "\n",
    "### Function Signature\n",
    "\n",
    "```python\n",
    "tailor_resume_sections(\n",
    "    section_generator_agent,      # Main agent for section generation\n",
    "    metadata_extractor_agent,     # Lightweight agent for metadata\n",
    "    job_text: str,                # Paste job posting here (not file path!)\n",
    "    original_resume_path: str,    # Path to original .tex file\n",
    "    output_path: Optional[str] = None,  # Auto-generated if not provided\n",
    "    include_experience: bool = False,   # Update Experience section?\n",
    "    render_pdf: bool = True       # Compile PDF automatically?\n",
    ") -> Dict[str, Optional[str]]    # Returns paths and metadata\n",
    "```\n",
    "\n",
    "### Return Value\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"tex_path\": \"data/tailored_versions/Google_Senior_ML_Engineer_20251116_154530.tex\",\n",
    "    \"pdf_path\": \"data/tailored_versions/Google_Senior_ML_Engineer_20251116_154530.pdf\",\n",
    "    \"company\": \"Google\",\n",
    "    \"position\": \"Senior ML Engineer\",\n",
    "    \"validation\": \"‚úÖ LaTeX validation passed (333 braces balanced)\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded job posting from data/job_postings/posting_details.txt\n",
      "üîç Extracting job metadata...\n",
      "{\n",
      "  \"company\": \"RBC\",\n",
      "  \"position\": \"Machine Learning Software Engineer\"\n",
      "}   Company: RBC\n",
      "   Position: Machine Learning Software Engineer\n",
      "\n",
      "üìù Auto-generated filename: RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "\n",
      "üìã Starting resume tailoring...\n",
      "   Original: data/original/AI_engineer.tex\n",
      "   Output: data/tailored_versions\\RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "\n",
      "üì§ Extracting sections from original resume...\n",
      "   ‚úì Extracted: Professional Summary\n",
      "   ‚úì Extracted: Technical Proficiencies\n",
      "   ‚úì Extracted: Professional Experience\n",
      "\n",
      "üìÅ Copying original resume to output directory...\n",
      "   ‚úì Copied to: data/tailored_versions\\RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "\n",
      "ü§ñ Generating tailored sections...\n",
      "SUBTITLE:\n",
      "Machine Learning Software Engineer\n",
      "\n",
      "PROFESSIONAL SUMMARY:\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{\n",
      "Machine learning software engineer focused on designing, building, and operating production ML systems in \\textbf{Python} on \\textbf{AWS}. Own end-to-end workflows from data preprocessing and exploration through training, packaging, and deployment on \\textbf{SageMaker}, \\textbf{Bedrock}, and microservice-based \\textbf{real-time} and \\textbf{batch} inference pipelines. Apply software engineering best practices (testing, code review, CI/CD, observability) to deliver \\textbf{modular}, \\textbf{scalable}, and \\textbf{production-ready} ML services. Partner closely with data scientists and product teams to graduate research POCs into resilient systems, define \\textbf{monitoring} and \\textbf{logging} strategies, and meet reliability, performance, security, and cost requirements. Build lightweight \\textbf{Flask/React} services and internal tools to streamline experimentation and model integration.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "TECHNICAL PROFICIENCIES:\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{Python (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{Data \\& Pipelines}{Apache Spark (EMR 6/7), Hadoop, Hive, Presto, Flink, Airflow (MWAA), AWS Glue}\n",
      "  \\resumeEntryS{ML \\& MLOps}{SageMaker (training, tuning, endpoints), Bedrock (Agentic patterns), scikit-learn, PyTorch, HuggingFace, MLlib}\n",
      "  \\resumeEntryS{Cloud \\& DevOps}{AWS (VPC, IAM, S3, RDS, DynamoDB, ECS/Fargate, EKS, API Gateway, Lambda, CloudWatch, Step Functions), Terraform, CloudFormation, Docker, CI/CD}\n",
      "  \\resumeEntryS{Datastores}{PostgreSQL, MySQL, DynamoDB, MongoDB (NoSQL), OpenSearch Vector DB, S3 Vectors}\n",
      "  \\resumeEntryS{Web \\& Services}{Flask, React, REST APIs, Node/Express, HTML/CSS}\n",
      "  \\resumeEntryS{Observability}{CloudWatch metrics/logs/alarms, Log Insights}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "OPTIONAL EXPERIENCE:\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Design and troubleshoot end-to-end \\textbf{ML} workflows on \\textbf{AWS} for enterprise customers‚Äîpackaging and deploying models to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}, and integrating with \\textbf{real-time} and \\textbf{batch} data pipelines.}\n",
      "    \\resumeItem{Apply software engineering best practices (testing strategies, \\textbf{code reviews}, source control, CI/CD) to help customers build \\textbf{modular}, \\textbf{robust}, and \\textbf{scalable} ML services and microservices on \\textbf{Lambda} and \\textbf{ECS/Fargate}.}\n",
      "    \\resumeItem{Architect and review \\textbf{data pipelines} across \\textbf{Glue}, \\textbf{EMR}, \\textbf{Spark}, \\textbf{Step Functions}, and \\textbf{S3}, advising on schema design, data access patterns, and interaction with relational/\\textbf{NoSQL} stores such as \\textbf{RDS} and \\textbf{DynamoDB}.}\n",
      "    \\resumeItem{Create and validate \\textbf{observability} and \\textbf{monitoring} plans using \\textbf{CloudWatch} (logs, metrics, alarms), failure handling, and canaries to ensure reliable operation of production ML systems.}\n",
      "    \\resumeItem{Partner with data science, product, and architecture stakeholders to translate research-oriented \\textbf{ML} requirements into production-ready designs that meet non-functional constraints (availability, performance, cost, security).}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Collaborated with researchers and engineers in an \\textbf{agile} environment to refine prompts and evaluation guidelines for \\textbf{LLM}-based features, supporting the ML research-to-product lifecycle.}\n",
      "    \\resumeItem{Developed a \\textbf{Dockerized} application playground‚Äî\\textbf{Flask} backend and \\textbf{React} UI‚Äîto launch and manage containerized services via \\textbf{REST} APIs, improving experimentation and model hosting workflows.}\n",
      "    \\resumeItem{Implemented error handling, status tracking, and audit logging in the playground service to enhance reliability, debugging, and observability for ML-driven applications.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome} tool (JavaScript/DOM) for high-throughput dataset operations, accelerating data preparation and experimentation for ML teams.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Built a time-series and spatial \\textbf{ML pipeline} over 5M rows to model urban sound distribution, including feature engineering, training, and evaluation for data-driven decision support.}\n",
      "    \\resumeItem{Partnered with stakeholders to integrate model outputs into a client-facing application and define monitoring and visualization needs for production use.}\n",
      "    \\resumeItem{Delivered an \\textbf{Altair}-based heat-map visualization component to surface predictions and support downstream analytical workflows.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEndüìù Parsing generated sections...\n",
      "   Found 4 sections to update:\n",
      "     ‚Ä¢ subtitle\n",
      "     ‚Ä¢ Professional Summary\n",
      "     ‚Ä¢ Technical Proficiencies\n",
      "     ‚Ä¢ Professional Experience\n",
      "\n",
      "üîß Replacing sections in copied resume...\n",
      "\n",
      "‚úÖ Successfully merged 4 sections to data/tailored_versions\\RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "‚úÖ LaTeX validation passed (335 braces balanced)\n",
      "\n",
      "üìÑ Compiling PDF...\n",
      "‚ùå PDF generation failed:\n",
      "This is pdfTeX, Version 3.141592653-2.6-1.40.27 (MiKTeX 25.4) (preloaded format=pdflatex.fmt)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "LaTeX2e <2025-11-01>\n",
      "L3 programming layer <2025-10-24>\n",
      "(C:\\Users\\34439\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/base\\article.cls\n",
      "Document Class: article 2025/01/22 v1.4n Standard LaTeX document class\n",
      "(C:\\Users\\34439\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/base\\size10.clo))\n",
      "(C:\\Users\\34439\\AppD\n",
      "\n",
      "================================================================================\n",
      "üìä RESULTS\n",
      "================================================================================\n",
      "Company:   RBC\n",
      "Position:  Machine Learning Software Engineer\n",
      "LaTeX:     data/tailored_versions\\RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "PDF:       None\n",
      "Status:    ‚úÖ Successfully merged 4 sections to data/tailored_versions\\RBC_Machine_Learning_Software_Engineer_20251116_160938.tex\n",
      "‚úÖ LaTeX validation passed (335 braces balanced)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Tailor resume by pasting job posting text (with optional posting_details.txt)\n",
    "from pathlib import Path\n",
    "from tools.resume_helpers import tailor_resume_sections\n",
    "\n",
    "# Step 1: Load job posting text (prefer posting_details.txt if available)\n",
    "posting_details_path = Path(\"data/job_postings/posting_details.txt\")\n",
    "if posting_details_path.exists():\n",
    "    print(\"üìÑ Loaded job posting from data/job_postings/posting_details.txt\")\n",
    "    job_posting_text = posting_details_path.read_text(encoding='utf-8').strip()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è posting_details.txt not found. Using placeholder text ‚Äì replace with your posting.\")\n",
    "    job_posting_text = \"\"\"\n",
    "[YOUR JOB POSTING TEXT HERE]\n",
    "\n",
    "For example:\n",
    "Senior ML Engineer at Google\n",
    "We are looking for an experienced ML Engineer with expertise in Python,\n",
    "TensorFlow, and cloud infrastructure (AWS/GCP)...\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Run the complete workflow (auto-extracts metadata, generates filename, compiles PDF)\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,      # Main agent for tailoring\n",
    "    metadata_extractor_agent,     # Lightweight agent for metadata\n",
    "    job_text=job_posting_text,    # Paste job text directly\n",
    "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
    "    # output_path is auto-generated from company + position + timestamp\n",
    "    include_experience=True,      # Set to True to update Experience section\n",
    "    render_pdf=True               # Set to False to skip PDF compilation\n",
    ")\n",
    "\n",
    "# Step 3: View results\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Company:   {result['company']}\")\n",
    "print(f\"Position:  {result['position']}\")\n",
    "print(f\"LaTeX:     {result['tex_path']}\")\n",
    "print(f\"PDF:       {result['pdf_path']}\")\n",
    "print(f\"Status:    {result['validation']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Quick Start\n",
    "\n",
    "1. **Prepare resume**: Ensure your LaTeX resume is in `data/original/`\n",
    "2. **Copy job posting**: Find a job posting and copy the entire text\n",
    "3. **Paste and run**: Replace the placeholder text in the example above and run the cell\n",
    "4. **View output**: Check the generated `.tex` and `.pdf` files in `data/tailored_versions/`\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "**Manual filename control**:\n",
    "```python\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,\n",
    "    metadata_extractor_agent,\n",
    "    job_text=job_posting_text,\n",
    "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
    "    output_path=\"data/tailored_versions/my_custom_name.tex\",  # Specify filename\n",
    "    render_pdf=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Skip PDF compilation** (faster for testing):\n",
    "```python\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,\n",
    "    metadata_extractor_agent,\n",
    "    job_text=job_posting_text,\n",
    "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
    "    render_pdf=False  # Only generate .tex file\n",
    ")\n",
    "```\n",
    "\n",
    "**Skip Experience section** (faster, cheaper):\n",
    "```python\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,\n",
    "    metadata_extractor_agent,\n",
    "    job_text=job_posting_text,\n",
    "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
    "    include_experience=False  # Only update Summary + Skills\n",
    ")\n",
    "```\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Batch processing**: Run multiple times with different job postings to maximize prompt caching benefits (20-40% cost savings)\n",
    "- **PDF requirements**: Requires `pdflatex` installed (MiKTeX, TeX Live, or MacTeX)\n",
    "- **Token savings**: Tool-free architecture saves 30-40% vs traditional workflows\n",
    "- **Monitor logs**: Use `view_latest_logs()` to debug any issues\n",
    "- **Filename format**: Auto-generated names follow pattern: `Company_Position_YYYYMMDD_HHMMSS.tex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add job postings**: Save job posting text in `data/job_postings/` as `.txt` files\n",
    "2. **Prepare resume**: Ensure your LaTeX resume is in `data/original/`\n",
    "3. **Run tailoring**: Execute the cell above with your file paths\n",
    "4. **Review output**: Check the generated `.tex` file in `data/tailored_versions/`\n",
    "5. **Compile PDF**: Use `pdflatex` or your LaTeX editor to generate the final PDF\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Batch processing**: Run the function multiple times with different job postings to maximize prompt caching benefits (20-40% cost savings after first request)\n",
    "- **Include experience**: Set `include_experience=True` for roles where experience needs tailoring\n",
    "- **Monitor logs**: Use `view_latest_logs()` to verify agent didn't call any tools\n",
    "- **Token savings**: This architecture saves 30-40% vs old workflow (no tool overhead) + caching savings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
