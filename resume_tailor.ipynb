{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Tailor Agent\n",
    "\n",
    "An intelligent agent that tailors your LaTeX resume to specific job postings while preserving formatting and maintaining accuracy.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **LaTeX-Safe**: Preserves LaTeX formatting and syntax\n",
    "- **Iterative**: Supports multiple revision rounds\n",
    "- **Job-Focused**: Analyzes job postings and matches requirements\n",
    "- **ATS-Optimized**: Uses keywords naturally for applicant tracking systems\n",
    "- **Validation**: Checks LaTeX syntax before output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Provider Configuration\n",
    "\n",
    "This notebook supports multiple AI providers. Configure your credentials in the `.env` file:\n",
    "\n",
    "### Option 1: OpenAI (Recommended for getting started)\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "```\n",
    "\n",
    "### Option 2: AWS Bedrock (Production-ready)\n",
    "```bash\n",
    "# Using long-term API key (recommended)\n",
    "AWS_BEARER_TOKEN_BEDROCK=your-long-term-bedrock-key\n",
    "AWS_REGION=us-east-1\n",
    "\n",
    "# OR using standard AWS credentials\n",
    "AWS_ACCESS_KEY_ID=your-access-key\n",
    "AWS_SECRET_ACCESS_KEY=your-secret-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "The notebook will automatically detect which credentials are available and use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Python Path: d:\\Strands-agent\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Strands SDK\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Python Path: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and verify environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking API credentials...\n",
      "\n",
      "‚úÖ OpenAI API key found\n",
      "\n",
      "ü§ñ Selected Model: <strands.models.openai.OpenAIModel object at 0x000001E2554E2BA0>\n",
      "\n",
      "üìÅ Project directories:\n",
      "  Prompts: True - d:\\Strands-agent\\prompts\n",
      "  Data: True - d:\\Strands-agent\\data\n",
      "  Output: True - d:\\Strands-agent\\data\\tailored_versions\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "PROMPTS_DIR = PROJECT_ROOT / \"prompts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ORIGINAL_RESUME_DIR = DATA_DIR / \"original\"\n",
    "JOB_POSTINGS_DIR = DATA_DIR / \"job_postings\"\n",
    "OUTPUT_DIR = DATA_DIR / \"tailored_versions\"\n",
    "\n",
    "# Detect which API credentials are available\n",
    "print(\"üîç Checking API credentials...\")\n",
    "print()\n",
    "\n",
    "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
    "has_bedrock_token = bool(os.getenv('AWS_BEARER_TOKEN_BEDROCK'))\n",
    "has_aws_creds = bool(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "\n",
    "from strands.models import openai\n",
    "\n",
    "if has_openai:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "    MODEL_ID = openai.OpenAIModel(model_id=\"gpt-5.1\")\n",
    "elif has_bedrock_token:\n",
    "    print(\"‚úÖ AWS Bedrock bearer token found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "elif has_aws_creds:\n",
    "    print(\"‚úÖ AWS credentials found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: No API credentials found!\")\n",
    "    print(\"Please set one of the following in .env file:\")\n",
    "    print(\"  - OPENAI_API_KEY (for OpenAI)\")\n",
    "    print(\"  - AWS_BEARER_TOKEN_BEDROCK (for Bedrock)\")\n",
    "    print(\"  - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY (for AWS)\")\n",
    "    MODEL_PROVIDER = None\n",
    "    MODEL_ID = None\n",
    "\n",
    "print()\n",
    "print(f\"ü§ñ Selected Model: {MODEL_ID}\")\n",
    "\n",
    "# Verify directories exist\n",
    "print()\n",
    "print(f\"üìÅ Project directories:\")\n",
    "print(f\"  Prompts: {PROMPTS_DIR.exists()} - {PROMPTS_DIR}\")\n",
    "print(f\"  Data: {DATA_DIR.exists()} - {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR.exists()} - {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts\n",
    "\n",
    "Load agent instructions from separate files for easy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded system_prompt.txt (7048 chars)\n",
      "\n",
      "üìù Full system prompt: 7047 characters\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(filename: str) -> str:\n",
    "    \"\"\"Load a prompt from the prompts directory.\"\"\"\n",
    "    prompt_path = PROMPTS_DIR / filename\n",
    "    if not prompt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found. Using default prompt.\")\n",
    "        return \"\"\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ Loaded {filename} ({len(content)} chars)\")\n",
    "    return content\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = load_prompt(\"system_prompt.txt\")\n",
    "latex_rules = \"\"\n",
    "\n",
    "# Combine prompts\n",
    "full_prompt = f\"{system_prompt}\\n\\n{latex_rules}\".strip()\n",
    "\n",
    "print(f\"\\nüìù Full system prompt: {len(full_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools for Resume Tailoring\n",
    "\n",
    "Define specialized tools for LaTeX resume processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom tools defined:\n",
      "  - read_file()\n",
      "  - write_file()\n",
      "  - validate_latex()\n",
      "  - extract_keywords()\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a file and return its contents.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "    \n",
    "    Returns:\n",
    "        The file contents as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write content to a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "        content: Content to write\n",
    "    \n",
    "    Returns:\n",
    "        Success message with file path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return f\"Successfully wrote {len(content)} characters to {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_latex(latex_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Validate LaTeX syntax by checking for common issues.\n",
    "    \n",
    "    Args:\n",
    "        latex_content: The LaTeX content to validate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results (is_valid, errors, warnings)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for balanced braces\n",
    "    if latex_content.count('{') != latex_content.count('}'):\n",
    "        errors.append(\"Unbalanced curly braces { }\")\n",
    "    \n",
    "    # Check for balanced brackets\n",
    "    if latex_content.count('[') != latex_content.count(']'):\n",
    "        errors.append(\"Unbalanced square brackets [ ]\")\n",
    "    \n",
    "    # Check for document structure\n",
    "    if '\\\\documentclass' not in latex_content:\n",
    "        warnings.append(\"No \\\\documentclass found\")\n",
    "    \n",
    "    if '\\\\begin{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\begin{document}\")\n",
    "    \n",
    "    if '\\\\end{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\end{document}\")\n",
    "    \n",
    "    # Check for common LaTeX commands\n",
    "    lines = latex_content.split('\\n')\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        # Check for unescaped special characters in regular text\n",
    "        if '%' in line and '\\\\%' not in line:\n",
    "            # This might be a comment, so it's just a warning\n",
    "            pass\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": is_valid,\n",
    "        \"errors\": errors,\n",
    "        \"warnings\": warnings,\n",
    "        \"summary\": f\"{'‚úÖ Valid' if is_valid else '‚ùå Invalid'} LaTeX ({len(errors)} errors, {len(warnings)} warnings)\"\n",
    "    }\n",
    "\n",
    "\n",
    "@tool\n",
    "def extract_keywords(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract important keywords from text (job posting or resume section).\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract keywords from\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords (skills, technologies, requirements)\n",
    "    \"\"\"\n",
    "    # Common technical keywords and skills\n",
    "    import re\n",
    "    \n",
    "    # Simple keyword extraction (can be enhanced with NLP)\n",
    "    keywords = set()\n",
    "    \n",
    "    # Common technical skills patterns\n",
    "    patterns = [\n",
    "        r'\\b(Python|Java|JavaScript|TypeScript|C\\+\\+|Ruby|Go|Rust|Swift)\\b',\n",
    "        r'\\b(AWS|Azure|GCP|Docker|Kubernetes|Jenkins)\\b',\n",
    "        r'\\b(React|Angular|Vue|Node\\.js|Django|Flask|Spring)\\b',\n",
    "        r'\\b(SQL|PostgreSQL|MySQL|MongoDB|Redis)\\b',\n",
    "        r'\\b(Git|CI/CD|Agile|Scrum|DevOps|REST|API)\\b',\n",
    "        r'\\b(Machine Learning|AI|Data Science|Analytics)\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            keywords.add(match.group(1))\n",
    "    \n",
    "    return sorted(list(keywords))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom tools defined:\")\n",
    "print(\"  - read_file()\")\n",
    "print(\"  - write_file()\")\n",
    "print(\"  - validate_latex()\")\n",
    "print(\"  - extract_keywords()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Resume Tailor Agent\n",
    "\n",
    "Initialize the agent with system prompts and custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resume Tailor Agent created!\n",
      "   Provider: OpenAI\n",
      "   Model: <strands.models.openai.OpenAIModel object at 0x000001E2554E2BA0>\n",
      "   Tools: 9 tools available\n",
      "   System prompt: 7047 characters\n",
      "\n",
      "Available tools:\n",
      "  ‚Ä¢ read_file, write_file, validate_latex, extract_keywords\n",
      "  ‚Ä¢ extract_section, replace_section, update_subtitle\n",
      "  ‚Ä¢ merge_sections, get_section_names\n",
      "\n",
      "üí° Tip: You can change the model by editing MODEL_PROVIDER and MODEL_ID in the configuration cell above\n"
     ]
    }
   ],
   "source": [
    "# Create agent with automatic provider detection\n",
    "MODEL_PROVIDER = \"OpenAI\"\n",
    "if MODEL_PROVIDER is None:\n",
    "    print(\"‚ùå Cannot create agent: No API credentials found\")\n",
    "    print(\"Please configure API credentials in .env file\")\n",
    "else:\n",
    "    # Import section updater tools\n",
    "    from tools.section_updater import (\n",
    "        extract_section,\n",
    "        replace_section,\n",
    "        update_subtitle,\n",
    "        merge_sections,\n",
    "        get_section_names\n",
    "    )\n",
    "    \n",
    "    # Create agent with detected provider and ALL tools\n",
    "    resume_agent = Agent(\n",
    "        model=MODEL_ID,\n",
    "        system_prompt=full_prompt if full_prompt else \"You are a helpful resume tailoring assistant.\",\n",
    "        tools=[\n",
    "            read_file,\n",
    "            write_file,\n",
    "            validate_latex,\n",
    "            extract_keywords,\n",
    "            # Section updater tools\n",
    "            extract_section,\n",
    "            replace_section,\n",
    "            update_subtitle,\n",
    "            merge_sections,\n",
    "            get_section_names\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Resume Tailor Agent created!\")\n",
    "    print(f\"   Provider: {MODEL_PROVIDER}\")\n",
    "    print(f\"   Model: {MODEL_ID}\")\n",
    "    print(f\"   Tools: {len(resume_agent.tool_names)} tools available\")\n",
    "    print(f\"   System prompt: {len(full_prompt) if full_prompt else 0} characters\")\n",
    "    print()\n",
    "    print(\"Available tools:\")\n",
    "    print(\"  ‚Ä¢ read_file, write_file, validate_latex, extract_keywords\")\n",
    "    print(\"  ‚Ä¢ extract_section, replace_section, update_subtitle\")\n",
    "    print(\"  ‚Ä¢ merge_sections, get_section_names\")\n",
    "    print()\n",
    "    print(\"üí° Tip: You can change the model by editing MODEL_PROVIDER and MODEL_ID in the configuration cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "Below are examples of how to use the resume tailor agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Resume Tailoring Workflow\n",
    "\n",
    "**Note**: You'll need to place your actual LaTeX resume in `data/original/resume.tex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing resume and job posting...\n",
      "\n",
      "\n",
      "Tool #1: read_file\n",
      "\n",
      "Tool #2: read_file\n",
      "Here‚Äôs a structured analysis and concrete edit suggestions tailored to the Coinbase Machine Learning Engineer, Risk AI/ML role.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Key requirements, responsibilities, and skills from the job posting\n",
      "\n",
      "**Core responsibilities**\n",
      "\n",
      "- Own end-to-end development of ML models on a self-service ML platform, from ideation to production.\n",
      "- Improve core **risk models**:\n",
      "  - Scam models\n",
      "  - Transfer/transaction risk models\n",
      "  - Withdrawal limit models\n",
      "  - Account takeover models\n",
      "- Rapidly respond to new threats:\n",
      "  - Turn new threat data into permanent ML models\n",
      "  - Replace rule-based systems\n",
      "  - Deploy to production in under a week\n",
      "- Build & deploy scalable, **real-time** production models and pipelines using CI/CD and centralized feature stores.\n",
      "- Apply **advanced ML**:\n",
      "  - Deep learning\n",
      "  - NLP, LLMs for NLP and conversational agents\n",
      "  - Graph Neural Networks (GNNs)\n",
      "  - Sequence modeling\n",
      "- Build **context-aware risk systems**:\n",
      "  - Adaptive logic applying the right ‚Äúfriction‚Äù (quiz, LLM agent, human review) depending on user profile.\n",
      "- Collaborate with Risk Operations, Platform Engineering, Product Management.\n",
      "\n",
      "**What they look for**\n",
      "\n",
      "- 4+ years professional experience in SWE and/or AI/ML, including production deployment.\n",
      "- Strong **Python** and ML frameworks (TensorFlow, **PyTorch**).\n",
      "- Experience building backend systems for data processing/analytics.\n",
      "- Familiarity with applied AI/ML domains: **Risk ML**, deep learning, NLP, recommender systems, anomaly detection.\n",
      "- Strong communication, collaboration; ability to explain technical concepts.\n",
      "\n",
      "**Nice to have**\n",
      "\n",
      "- Degree in CS/AI/ML/Data Science (you exceed this with masters).\n",
      "- Familiarity with modern data/ML infra:\n",
      "  - Feature stores (e.g., Tecton)\n",
      "  - Model serving (RayServe)\n",
      "  - Airflow, Spark, Kafka\n",
      "- Experience with **GNNs**, sequence models (LSTMs).\n",
      "- Experience with **LLMs**, agentic systems, RL.\n",
      "- Understanding of **MLOps** best practices for monitoring & improving production models.\n",
      "- Data analysis & visualization tools.\n",
      "\n",
      "**Domain focus**\n",
      "\n",
      "- **Fraud / risk / security** in crypto:\n",
      "  - Fraud prevention\n",
      "  - Account takeover prevention\n",
      "  - Scam detection\n",
      "  - Transaction/withdrawal risk.\n",
      "- Real-time predictions and adaptive risk systems.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Comparison with your resume (fit & gaps)\n",
      "\n",
      "### Strong matches\n",
      "\n",
      "- **Production ML & MLOps on cloud**:\n",
      "  - AWS role: designing, deploying, and operating production ML systems on AWS.\n",
      "  - Strong experience with **SageMaker**, **Bedrock**, real-time and batch inference, observability (CloudWatch), CI/CD-ish patterns, IaC.\n",
      "- **Python, SQL, ML frameworks**:\n",
      "  - Python (primary), SQL, PyTorch, scikit-learn, HuggingFace.\n",
      "- **Big Data & pipelines**:\n",
      "  - Spark, Hadoop, Hive, Flink, Airflow, Glue; aligns with ‚ÄúSpark, Airflow‚Äù nice-to-have.\n",
      "- **LLMs, Agentic patterns, RAG**:\n",
      "  - Bedrock + Agentic patterns.\n",
      "  - Prompt Engineer experience; LLM evaluation, internal tools, model hosting.\n",
      "- **Monitoring/observability & production mindset**:\n",
      "  - Monitoring plans, CloudWatch, non-functional requirements, production readiness.\n",
      "- **Collaboration**:\n",
      "  - Partnering with architects/POs, data scientists, stakeholders.\n",
      "\n",
      "### Partial or missing alignment\n",
      "\n",
      "- **Explicit Risk / Fraud ML domain**:\n",
      "  - Resume does not mention risk modeling, fraud, scam, transaction risk, account takeover, or security-focused modeling.\n",
      "  - Your experience is domain-agnostic / infra-focused.\n",
      "- **Rapid response to threats / fast iteration**:\n",
      "  - You talk about designing workflows and feasibility guidance, but not explicitly about rapid iteration/redeployment in response to new threats or changing data.\n",
      "- **Advanced modeling angle**:\n",
      "  - You mention ML pipelines, models, and LLMs, but little detail on:\n",
      "    - Deep learning models in production.\n",
      "    - Anomaly detection, risk ML, or sequence modeling.\n",
      "    - Explicit use of GNNs, LSTMs, etc. (you should not add these; they are gaps).\n",
      "- **Domain: crypto / financial systems**:\n",
      "  - No mention of crypto, blockchain, or financial transaction domains.\n",
      "- **Fraud/risk-specific concepts**:\n",
      "  - No mention of feature stores, risk scores, rules to ML migration, or adaptive friction/decisioning logic.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Concrete edit suggestions\n",
      "\n",
      "### A. Subtitle\n",
      "\n",
      "Current: `AI Engineer`\n",
      "\n",
      "Suggestion: keep or slightly sharpen to reflect ML production focus; do **not** claim risk if you haven‚Äôt done it.\n",
      "\n",
      "- ‚ÄúMachine Learning Engineer‚Äù  \n",
      "or  \n",
      "- ‚ÄúAI/ML Engineer‚Äù\n",
      "\n",
      "Given the posting title is ‚ÄúMachine Learning Engineer, Risk AI/ML‚Äù, I‚Äôd use:\n",
      "\n",
      "- **‚ÄúMachine Learning Engineer‚Äù**\n",
      "\n",
      "### B. Professional Summary: content & wording\n",
      "\n",
      "Current summary is very infra- and AWS-focused. Coinbase role is model-centric and risk-focused. You should:\n",
      "\n",
      "- Emphasize:\n",
      "  - End-to-end **model development and deployment**.\n",
      "  - Real-time **risk-style** or decisioning use cases where you applied ML for automated decisions.\n",
      "  - **LLMs / agentic** experience in a way that maps to ‚ÄúLLMs for NLP and conversational agents‚Äù and ‚Äúagentic systems‚Äù.\n",
      "  - Collaboration with risk-like stakeholders (operations/security/compliance) if any can be fairly described as ‚Äúoperations‚Äù or ‚Äúsupport‚Äù teams.\n",
      "\n",
      "- De-emphasize:\n",
      "  - Too much detail about IaC and networking; keep but reduce.\n",
      "  - Bedrock/Agentic as purely infra; frame them as applied model delivery.\n",
      "\n",
      "Specific rephrasing ideas (still truthful):\n",
      "\n",
      "- Current: ‚ÄúAI/ML Engineer with hands-on experience designing, deploying, and operating production ML systems on AWS.‚Äù\n",
      "  - Suggested: ‚ÄúMachine Learning Engineer with hands-on experience **designing, training, and deploying** production ML systems on AWS, including real-time and batch inference services.‚Äù\n",
      "\n",
      "- Add a sentence that emphasizes risk-like decisioning without inventing fraud:\n",
      "  - If your AWS work includes reliability/security/cost/operational risk, you can rephrase:\n",
      "    - ‚ÄúHelp customers build **automated, ML-driven decision systems** with strong reliability, security, and observability requirements.‚Äù\n",
      "  - If you have any exposure to abuse detection, anomaly alerts, or operational risk, mention it explicitly.\n",
      "\n",
      "- Current: ‚ÄúFamiliar with RAG concepts (‚Ä¶) and Agentic AI patterns (tool use, orchestration, memory).‚Äù\n",
      "  - Refine to align with their LLM/agentic use:\n",
      "    - ‚ÄúFamiliar with **LLM-based applications**, including RAG (retrievers, embeddings, vector indexes) and **agentic patterns** (tool use, orchestration, memory) for building context-aware AI assistants.‚Äù\n",
      "\n",
      "- Explicitly call out hands-on modeling (not just infra):\n",
      "  - Add: ‚ÄúComfortable **owning the full lifecycle from feature engineering and model training to deployment and monitoring in production**.‚Äù\n",
      "\n",
      "Summary target: 3‚Äì4 sentences (~100‚Äì130 words), with keywords: ‚Äúproduction ML models‚Äù, ‚Äúreal-time predictions‚Äù, ‚ÄúLLMs‚Äù, ‚ÄúPyTorch‚Äù, ‚Äúmonitoring‚Äù, ‚Äúcollaborating with cross-functional stakeholders.‚Äù\n",
      "\n",
      "### C. Technical Proficiencies: emphasis & tweaks\n",
      "\n",
      "Your skills section is strong; tune it to the Coinbase posting:\n",
      "\n",
      "1. **Languages**\n",
      "   - Already good: Python (primary), SQL, Bash, JavaScript.\n",
      "   - Consider adding ‚Äúpandas, NumPy‚Äù only if present elsewhere (not currently; don‚Äôt fabricate). If you normally use them but didn‚Äôt write them, you can safely add as they‚Äôre implicit in Python ML; but per instructions, don‚Äôt introduce new tech that isn‚Äôt in the resume, so keep as is.\n",
      "\n",
      "2. **MLOps & ML**\n",
      "   - You already list: SageMaker, Bedrock, scikit-learn, PyTorch, HuggingFace, MLlib.\n",
      "   - To better match ‚Äúdeep learning, NLP, LLMs, agentic systems, monitoring‚Äù:\n",
      "     - Reorder to emphasize **PyTorch**, **LLMs**, and monitoring:\n",
      "       - ‚ÄúPyTorch, scikit-learn, HuggingFace; **SageMaker** (training, tuning, endpoints), **Bedrock** (LLM & agentic patterns), MLlib‚Äù\n",
      "   - If you have done anomaly detection/time-series in the Citysage project, you can phrase:\n",
      "       - ‚Äútime-series and spatial modeling (Citysage capstone)‚Äù but that would need to be added in Experience rather than Skills; keep Skills generic.\n",
      "\n",
      "3. **Big Data & Pipelines**\n",
      "   - Already highly relevant: Spark, Airflow, Glue.\n",
      "   - To align with ‚Äúfeature stores / risk ML pipelines‚Äù, you can lightly emphasize feature engineering/pipelines:\n",
      "     - ‚ÄúApache Spark (EMR 6/7), Hadoop, Hive, Presto, Flink; **Airflow** (MWAA); AWS Glue for **ETL & feature pipelines**.‚Äù\n",
      "\n",
      "4. **Cloud & Infra**\n",
      "   - Good; just ensure it doesn‚Äôt overshadow modeling:\n",
      "     - Keep as-is but maybe move this category after MLOps & ML.\n",
      "\n",
      "5. **Observability**\n",
      "   - Very relevant to ‚Äúmonitoring and improving production models‚Äù.\n",
      "   - You might expand slightly:\n",
      "     - ‚ÄúCloudWatch metrics/logs/alarms, Log Insights; experience defining **monitoring plans for ML services**.‚Äù\n",
      "\n",
      "6. **Add a category for ‚ÄúData Science & Analytics‚Äù**\n",
      "   - Based on your MDS and Citysage project, you can add:\n",
      "     - ‚ÄúData Science & Analytics: exploratory data analysis, feature engineering, model evaluation, visualization (Altair).‚Äù\n",
      "   - Altair is in Experience already, so this is truthful.\n",
      "\n",
      "### D. Experience section: role-by-role suggestions\n",
      "\n",
      "#### 1) Cloud Support Engineer ‚Äì Big Data Specialty (AWS)\n",
      "\n",
      "This is your closest ‚Äúproduction ML + MLOps‚Äù experience. To align with Coinbase:\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- End-to-end model lifecycle: feature pipelines, training, deployment, monitoring.\n",
      "- Real-time predictions / low-latency services.\n",
      "- Rapid troubleshooting and iteration, particularly in response to incidents or changing requirements.\n",
      "- Any security / risk / compliance constraints, even if not fraud-specific.\n",
      "\n",
      "**Concrete bullet adjustments**\n",
      "\n",
      "Original bullets (summarized):\n",
      "\n",
      "1. Design/troubleshoot end-to-end ML workflows on AWS for enterprise customers; packaging & deploying models to SageMaker/Bedrock; batch (Glue/EMR + Step Functions) and real-time (API Gateway + Lambda/ECS).\n",
      "2. Translate product/DS requirements into cloud work packages and IaC.\n",
      "3. Create/review monitoring plans: CloudWatch, failure handling, etc.\n",
      "4. Partner with architects/POs on non-functional requirements.\n",
      "5. Explore GenAI/Agentic patterns on Bedrock; feasibility guidance for RAG-ready designs.\n",
      "\n",
      "**Suggested rephrasing and ordering:**\n",
      "\n",
      "Top bullets should scream: ‚Äúowning end-to-end ML models in production, real-time, with monitoring‚Äù:\n",
      "\n",
      "1. Rephrase bullet 1 to stress **real-time predictions, model performance, and production**:\n",
      "   - ‚ÄúDesign, troubleshoot, and optimize **end-to-end ML workflows** on AWS for enterprise customers‚Äîowning model packaging and deployment to **SageMaker** endpoints and **Bedrock**, with both **batch pipelines** (Glue/EMR + Step Functions) and **real-time prediction APIs** (API Gateway + Lambda / ECS/Fargate).‚Äù\n",
      "\n",
      "2. Bullet 3: bring ‚Äúmonitoring & improving production models‚Äù up:\n",
      "   - ‚ÄúDefine and review **monitoring plans for production ML services**, including CloudWatch logs/metrics/alarms, failure handling, retries, and canary strategies to detect model/service degradation before impacting users.‚Äù\n",
      "\n",
      "3. Bullet 2: tie to feature pipelines / modeling:\n",
      "   - ‚ÄúTranslate product and data science requirements into **deployable ML solutions** and cloud work packages using IaC (Terraform/CloudFormation, familiar with CDK), covering data pipelines, networking (VPC, IAM), storage (S3), security, scaling, and cost controls.‚Äù\n",
      "\n",
      "4. Bullet 4: connect to risk-like non-functional requirements:\n",
      "   - ‚ÄúCollaborate with architects and product owners to design ML-enabled services that meet stringent **non-functional requirements** (availability, performance, security, observability) typical of risk- and compliance-sensitive workloads.‚Äù\n",
      "\n",
      "5. Bullet 5: align with ‚ÄúLLMs for NLP and agentic systems‚Äù:\n",
      "   - ‚ÄúExplore **GenAI and agentic patterns** on **Bedrock**, advising customers on feasibility of LLM-based assistants and **RAG-ready designs** integrated with existing microservices and operational workflows.‚Äù\n",
      "\n",
      "If you have any ticket patterns where you helped with fraud/risk use cases (even high-level), you can (truthfully) add a phrase like ‚Äúincluding security- and risk-oriented workloads‚Äù, but only if accurate.\n",
      "\n",
      "#### 2) Prompt Engineer & Software Engineer ‚Äì Meta (Contract)\n",
      "\n",
      "This maps well to ‚ÄúLLMs, NLP, agentic systems, evaluation.‚Äù\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- LLM evaluation, metrics, and iteration.\n",
      "- Working in a pod, fast experimentation loops.\n",
      "- Tools that support high-throughput data labeling / experimentation.\n",
      "\n",
      "**Concrete adjustments:**\n",
      "\n",
      "1. First bullet:\n",
      "   - Original: ‚ÄúImproved LLM outcomes by refining prompts/evaluation guidelines; collaborated with researchers/engineers in an agile environment to productize evaluation workflows.‚Äù\n",
      "   - Suggested:\n",
      "     - ‚ÄúImproved **LLM model quality** by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an **agile pod** to design and productize **evaluation workflows** for large-scale experimentation.‚Äù\n",
      "\n",
      "2. Second bullet: explicitly connect to model hosting/pipelines:\n",
      "   - Original: Dockerized app playground with Flask + React.\n",
      "   - Suggested:\n",
      "     - ‚ÄúBuilt a **Dockerized experiment playground**‚Äî**Flask** backend + **React** UI‚Äîto launch/stop containerized services via REST, including status tracking, error handling, and audit logs, facilitating rapid **LLM/model experimentation and hosting**.‚Äù\n",
      "\n",
      "3. Third bullet:\n",
      "   - Original: Enhanced Chrome tool for dataset operations.\n",
      "   - Suggested:\n",
      "     - ‚ÄúEnhanced an internal **Chrome-based tooling** suite (JavaScript/DOM) for high-throughput dataset operations, improving throughput and consistency for **training and evaluation data** used in LLM experiments.‚Äù\n",
      "\n",
      "#### 3) ML Scientist (MDS Capstone) ‚Äì Citysage\n",
      "\n",
      "This is your best place to show **modeling** (time-series/spatial, anomaly-like).\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- Time-series / spatial modeling, large datasets.\n",
      "- End-to-end pipeline: data prep, modeling, evaluation, deployment/visualization.\n",
      "- Link to ‚Äúsequence modeling / anomaly detection‚Äù conceptually, but do not claim LSTMs if you didn‚Äôt use them.\n",
      "\n",
      "**Concrete bullet tweaks:**\n",
      "\n",
      "1. First bullet:\n",
      "   - Original: ‚ÄúDeveloped a time-series/spatial ML pipeline over 5M rows to model urban sound distribution; partnered with stakeholders to move analyses into a client-facing app and define monitoring/visualization needs.‚Äù\n",
      "   - Suggested:\n",
      "     - ‚ÄúDeveloped a **time-series and spatial ML pipeline** on 5M+ rows to model urban sound distribution, including feature engineering, model training, and evaluation; partnered with stakeholders to integrate outputs into a client-facing app and define **monitoring/visualization** requirements.‚Äù\n",
      "\n",
      "2. Second bullet:\n",
      "   - Original: Altair heat-map.\n",
      "   - Suggested:\n",
      "     - ‚ÄúDelivered an **Altair-based interactive heat-map** component to support decision-making and downstream analytics for non-technical users.‚Äù\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Reordering suggestions\n",
      "\n",
      "- Keep overall section order (Summary ‚Üí Education ‚Üí Experience ‚Üí Technical Proficiencies ‚Üí Certifications) as your template, but:\n",
      "  - For Coinbase, **Experience** is more important than Education; if your template allows, you could move ‚ÄúProfessional Experience‚Äù above ‚ÄúEducation.‚Äù If the layout doesn‚Äôt break, I recommend:\n",
      "    - Professional Summary  \n",
      "    - Professional Experience  \n",
      "    - Technical Proficiencies  \n",
      "    - Education  \n",
      "    - Certifications\n",
      "\n",
      "- Within Experience, ordering is already chronological; keep as is.\n",
      "\n",
      "- Within the AWS role, reorder bullets to prioritize:\n",
      "  1. End-to-end ML workflows & real-time predictions.\n",
      "  2. Monitoring & production reliability.\n",
      "  3. Translating DS requirements into deployable solutions.\n",
      "  4. Non-functional requirements & risk-like constraints.\n",
      "  5. GenAI/LLM/agentic patterns.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Major gaps relative to the Coinbase role\n",
      "\n",
      "These are areas you should not claim but can mention as interests in a cover letter or interview:\n",
      "\n",
      "1. **Domain-specific Risk/Fraud ML**\n",
      "   - No explicit experience in fraud detection, scam modeling, transaction risk, withdrawal limit models, or account takeover models.\n",
      "   - No explicit ‚ÄúRisk ML,‚Äù ‚Äúanomaly detection for fraud,‚Äù or similar in the resume.\n",
      "\n",
      "2. **Crypto / Blockchain / Financial Systems**\n",
      "   - No mention of crypto, blockchain, on-chain data, exchanges, or financial transaction workflows.\n",
      "   - For Coinbase, this is a domain gap you can address with a short statement of interest in the cover letter, not the resume.\n",
      "\n",
      "3. **GNNs, sequence models (LSTMs), RL**\n",
      "   - Not present in resume; required/desired in posting.\n",
      "   - Do not add them.\n",
      "\n",
      "4. **Feature store / centralized feature platform**\n",
      "   - They mention ‚Äúfeature stores like Tecton‚Äù; you don‚Äôt list any feature store.\n",
      "   - You do have ETL/feature-pipeline tools (Spark, Glue, Airflow); you can emphasize those as feature pipelines, but not feature stores.\n",
      "\n",
      "5. **Risk systems with adaptive friction**\n",
      "   - They look for experience building **context-aware risk systems** that choose friction (quiz, LLM agent, human review).\n",
      "   - Your resume doesn‚Äôt mention adaptive user flows or risk scoring logic.\n",
      "   - You could emphasize that you build systems with ‚ÄúB2C-style AI services‚Äù and production decisioning, but not specific friction logic.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Summary of key edits to make\n",
      "\n",
      "1. **Subtitle**: change to ‚ÄúMachine Learning Engineer‚Äù.\n",
      "2. **Professional Summary**:\n",
      "   - Emphasize end-to-end modeling, real-time predictions, production deployment, LLM applications, monitoring.\n",
      "   - Slightly shift from pure infra to ‚Äúowning models + infra.‚Äù\n",
      "3. **Technical Proficiencies**:\n",
      "   - Reorder MLOps & ML section to highlight PyTorch, LLMs, monitoring.\n",
      "   - Clarify Big Data & Pipelines as ETL/feature pipelines.\n",
      "   - Add a ‚ÄúData Science & Analytics‚Äù row using existing technologies (Altair, etc.).\n",
      "4. **AWS Experience bullets**:\n",
      "   - Rephrase to focus on production ML models, real-time predictions, monitoring, and integrating ML into user-facing services.\n",
      "   - Bring monitoring, reliability, and non-functional requirements forward.\n",
      "5. **Meta Experience bullets**:\n",
      "   - Highlight LLM evaluation, experimentation, and tooling.\n",
      "6. **Citysage Experience**:\n",
      "   - Emphasize time-series/spatial modeling and end-to-end pipeline work, tying to complex modeling experience.\n",
      "7. **Do not add**:\n",
      "   - Any mention of fraud, risk models, crypto, GNNs, LSTMs, Kafka, feature stores, or RL unless they are genuinely part of your experience and appear somewhere in your materials.\n",
      "\n",
      "If you‚Äôd like, I can next produce updated **Summary** and **Technical Proficiencies** LaTeX sections in GENERATE MODE that incorporate these edits while preserving your template.Here‚Äôs a structured analysis and concrete edit suggestions tailored to the Coinbase Machine Learning Engineer, Risk AI/ML role.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Key requirements, responsibilities, and skills from the job posting\n",
      "\n",
      "**Core responsibilities**\n",
      "\n",
      "- Own end-to-end development of ML models on a self-service ML platform, from ideation to production.\n",
      "- Improve core **risk models**:\n",
      "  - Scam models\n",
      "  - Transfer/transaction risk models\n",
      "  - Withdrawal limit models\n",
      "  - Account takeover models\n",
      "- Rapidly respond to new threats:\n",
      "  - Turn new threat data into permanent ML models\n",
      "  - Replace rule-based systems\n",
      "  - Deploy to production in under a week\n",
      "- Build & deploy scalable, **real-time** production models and pipelines using CI/CD and centralized feature stores.\n",
      "- Apply **advanced ML**:\n",
      "  - Deep learning\n",
      "  - NLP, LLMs for NLP and conversational agents\n",
      "  - Graph Neural Networks (GNNs)\n",
      "  - Sequence modeling\n",
      "- Build **context-aware risk systems**:\n",
      "  - Adaptive logic applying the right ‚Äúfriction‚Äù (quiz, LLM agent, human review) depending on user profile.\n",
      "- Collaborate with Risk Operations, Platform Engineering, Product Management.\n",
      "\n",
      "**What they look for**\n",
      "\n",
      "- 4+ years professional experience in SWE and/or AI/ML, including production deployment.\n",
      "- Strong **Python** and ML frameworks (TensorFlow, **PyTorch**).\n",
      "- Experience building backend systems for data processing/analytics.\n",
      "- Familiarity with applied AI/ML domains: **Risk ML**, deep learning, NLP, recommender systems, anomaly detection.\n",
      "- Strong communication, collaboration; ability to explain technical concepts.\n",
      "\n",
      "**Nice to have**\n",
      "\n",
      "- Degree in CS/AI/ML/Data Science (you exceed this with masters).\n",
      "- Familiarity with modern data/ML infra:\n",
      "  - Feature stores (e.g., Tecton)\n",
      "  - Model serving (RayServe)\n",
      "  - Airflow, Spark, Kafka\n",
      "- Experience with **GNNs**, sequence models (LSTMs).\n",
      "- Experience with **LLMs**, agentic systems, RL.\n",
      "- Understanding of **MLOps** best practices for monitoring & improving production models.\n",
      "- Data analysis & visualization tools.\n",
      "\n",
      "**Domain focus**\n",
      "\n",
      "- **Fraud / risk / security** in crypto:\n",
      "  - Fraud prevention\n",
      "  - Account takeover prevention\n",
      "  - Scam detection\n",
      "  - Transaction/withdrawal risk.\n",
      "- Real-time predictions and adaptive risk systems.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Comparison with your resume (fit & gaps)\n",
      "\n",
      "### Strong matches\n",
      "\n",
      "- **Production ML & MLOps on cloud**:\n",
      "  - AWS role: designing, deploying, and operating production ML systems on AWS.\n",
      "  - Strong experience with **SageMaker**, **Bedrock**, real-time and batch inference, observability (CloudWatch), CI/CD-ish patterns, IaC.\n",
      "- **Python, SQL, ML frameworks**:\n",
      "  - Python (primary), SQL, PyTorch, scikit-learn, HuggingFace.\n",
      "- **Big Data & pipelines**:\n",
      "  - Spark, Hadoop, Hive, Flink, Airflow, Glue; aligns with ‚ÄúSpark, Airflow‚Äù nice-to-have.\n",
      "- **LLMs, Agentic patterns, RAG**:\n",
      "  - Bedrock + Agentic patterns.\n",
      "  - Prompt Engineer experience; LLM evaluation, internal tools, model hosting.\n",
      "- **Monitoring/observability & production mindset**:\n",
      "  - Monitoring plans, CloudWatch, non-functional requirements, production readiness.\n",
      "- **Collaboration**:\n",
      "  - Partnering with architects/POs, data scientists, stakeholders.\n",
      "\n",
      "### Partial or missing alignment\n",
      "\n",
      "- **Explicit Risk / Fraud ML domain**:\n",
      "  - Resume does not mention risk modeling, fraud, scam, transaction risk, account takeover, or security-focused modeling.\n",
      "  - Your experience is domain-agnostic / infra-focused.\n",
      "- **Rapid response to threats / fast iteration**:\n",
      "  - You talk about designing workflows and feasibility guidance, but not explicitly about rapid iteration/redeployment in response to new threats or changing data.\n",
      "- **Advanced modeling angle**:\n",
      "  - You mention ML pipelines, models, and LLMs, but little detail on:\n",
      "    - Deep learning models in production.\n",
      "    - Anomaly detection, risk ML, or sequence modeling.\n",
      "    - Explicit use of GNNs, LSTMs, etc. (you should not add these; they are gaps).\n",
      "- **Domain: crypto / financial systems**:\n",
      "  - No mention of crypto, blockchain, or financial transaction domains.\n",
      "- **Fraud/risk-specific concepts**:\n",
      "  - No mention of feature stores, risk scores, rules to ML migration, or adaptive friction/decisioning logic.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Concrete edit suggestions\n",
      "\n",
      "### A. Subtitle\n",
      "\n",
      "Current: `AI Engineer`\n",
      "\n",
      "Suggestion: keep or slightly sharpen to reflect ML production focus; do **not** claim risk if you haven‚Äôt done it.\n",
      "\n",
      "- ‚ÄúMachine Learning Engineer‚Äù  \n",
      "or  \n",
      "- ‚ÄúAI/ML Engineer‚Äù\n",
      "\n",
      "Given the posting title is ‚ÄúMachine Learning Engineer, Risk AI/ML‚Äù, I‚Äôd use:\n",
      "\n",
      "- **‚ÄúMachine Learning Engineer‚Äù**\n",
      "\n",
      "### B. Professional Summary: content & wording\n",
      "\n",
      "Current summary is very infra- and AWS-focused. Coinbase role is model-centric and risk-focused. You should:\n",
      "\n",
      "- Emphasize:\n",
      "  - End-to-end **model development and deployment**.\n",
      "  - Real-time **risk-style** or decisioning use cases where you applied ML for automated decisions.\n",
      "  - **LLMs / agentic** experience in a way that maps to ‚ÄúLLMs for NLP and conversational agents‚Äù and ‚Äúagentic systems‚Äù.\n",
      "  - Collaboration with risk-like stakeholders (operations/security/compliance) if any can be fairly described as ‚Äúoperations‚Äù or ‚Äúsupport‚Äù teams.\n",
      "\n",
      "- De-emphasize:\n",
      "  - Too much detail about IaC and networking; keep but reduce.\n",
      "  - Bedrock/Agentic as purely infra; frame them as applied model delivery.\n",
      "\n",
      "Specific rephrasing ideas (still truthful):\n",
      "\n",
      "- Current: ‚ÄúAI/ML Engineer with hands-on experience designing, deploying, and operating production ML systems on AWS.‚Äù\n",
      "  - Suggested: ‚ÄúMachine Learning Engineer with hands-on experience **designing, training, and deploying** production ML systems on AWS, including real-time and batch inference services.‚Äù\n",
      "\n",
      "- Add a sentence that emphasizes risk-like decisioning without inventing fraud:\n",
      "  - If your AWS work includes reliability/security/cost/operational risk, you can rephrase:\n",
      "    - ‚ÄúHelp customers build **automated, ML-driven decision systems** with strong reliability, security, and observability requirements.‚Äù\n",
      "  - If you have any exposure to abuse detection, anomaly alerts, or operational risk, mention it explicitly.\n",
      "\n",
      "- Current: ‚ÄúFamiliar with RAG concepts (‚Ä¶) and Agentic AI patterns (tool use, orchestration, memory).‚Äù\n",
      "  - Refine to align with their LLM/agentic use:\n",
      "    - ‚ÄúFamiliar with **LLM-based applications**, including RAG (retrievers, embeddings, vector indexes) and **agentic patterns** (tool use, orchestration, memory) for building context-aware AI assistants.‚Äù\n",
      "\n",
      "- Explicitly call out hands-on modeling (not just infra):\n",
      "  - Add: ‚ÄúComfortable **owning the full lifecycle from feature engineering and model training to deployment and monitoring in production**.‚Äù\n",
      "\n",
      "Summary target: 3‚Äì4 sentences (~100‚Äì130 words), with keywords: ‚Äúproduction ML models‚Äù, ‚Äúreal-time predictions‚Äù, ‚ÄúLLMs‚Äù, ‚ÄúPyTorch‚Äù, ‚Äúmonitoring‚Äù, ‚Äúcollaborating with cross-functional stakeholders.‚Äù\n",
      "\n",
      "### C. Technical Proficiencies: emphasis & tweaks\n",
      "\n",
      "Your skills section is strong; tune it to the Coinbase posting:\n",
      "\n",
      "1. **Languages**\n",
      "   - Already good: Python (primary), SQL, Bash, JavaScript.\n",
      "   - Consider adding ‚Äúpandas, NumPy‚Äù only if present elsewhere (not currently; don‚Äôt fabricate). If you normally use them but didn‚Äôt write them, you can safely add as they‚Äôre implicit in Python ML; but per instructions, don‚Äôt introduce new tech that isn‚Äôt in the resume, so keep as is.\n",
      "\n",
      "2. **MLOps & ML**\n",
      "   - You already list: SageMaker, Bedrock, scikit-learn, PyTorch, HuggingFace, MLlib.\n",
      "   - To better match ‚Äúdeep learning, NLP, LLMs, agentic systems, monitoring‚Äù:\n",
      "     - Reorder to emphasize **PyTorch**, **LLMs**, and monitoring:\n",
      "       - ‚ÄúPyTorch, scikit-learn, HuggingFace; **SageMaker** (training, tuning, endpoints), **Bedrock** (LLM & agentic patterns), MLlib‚Äù\n",
      "   - If you have done anomaly detection/time-series in the Citysage project, you can phrase:\n",
      "       - ‚Äútime-series and spatial modeling (Citysage capstone)‚Äù but that would need to be added in Experience rather than Skills; keep Skills generic.\n",
      "\n",
      "3. **Big Data & Pipelines**\n",
      "   - Already highly relevant: Spark, Airflow, Glue.\n",
      "   - To align with ‚Äúfeature stores / risk ML pipelines‚Äù, you can lightly emphasize feature engineering/pipelines:\n",
      "     - ‚ÄúApache Spark (EMR 6/7), Hadoop, Hive, Presto, Flink; **Airflow** (MWAA); AWS Glue for **ETL & feature pipelines**.‚Äù\n",
      "\n",
      "4. **Cloud & Infra**\n",
      "   - Good; just ensure it doesn‚Äôt overshadow modeling:\n",
      "     - Keep as-is but maybe move this category after MLOps & ML.\n",
      "\n",
      "5. **Observability**\n",
      "   - Very relevant to ‚Äúmonitoring and improving production models‚Äù.\n",
      "   - You might expand slightly:\n",
      "     - ‚ÄúCloudWatch metrics/logs/alarms, Log Insights; experience defining **monitoring plans for ML services**.‚Äù\n",
      "\n",
      "6. **Add a category for ‚ÄúData Science & Analytics‚Äù**\n",
      "   - Based on your MDS and Citysage project, you can add:\n",
      "     - ‚ÄúData Science & Analytics: exploratory data analysis, feature engineering, model evaluation, visualization (Altair).‚Äù\n",
      "   - Altair is in Experience already, so this is truthful.\n",
      "\n",
      "### D. Experience section: role-by-role suggestions\n",
      "\n",
      "#### 1) Cloud Support Engineer ‚Äì Big Data Specialty (AWS)\n",
      "\n",
      "This is your closest ‚Äúproduction ML + MLOps‚Äù experience. To align with Coinbase:\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- End-to-end model lifecycle: feature pipelines, training, deployment, monitoring.\n",
      "- Real-time predictions / low-latency services.\n",
      "- Rapid troubleshooting and iteration, particularly in response to incidents or changing requirements.\n",
      "- Any security / risk / compliance constraints, even if not fraud-specific.\n",
      "\n",
      "**Concrete bullet adjustments**\n",
      "\n",
      "Original bullets (summarized):\n",
      "\n",
      "1. Design/troubleshoot end-to-end ML workflows on AWS for enterprise customers; packaging & deploying models to SageMaker/Bedrock; batch (Glue/EMR + Step Functions) and real-time (API Gateway + Lambda/ECS).\n",
      "2. Translate product/DS requirements into cloud work packages and IaC.\n",
      "3. Create/review monitoring plans: CloudWatch, failure handling, etc.\n",
      "4. Partner with architects/POs on non-functional requirements.\n",
      "5. Explore GenAI/Agentic patterns on Bedrock; feasibility guidance for RAG-ready designs.\n",
      "\n",
      "**Suggested rephrasing and ordering:**\n",
      "\n",
      "Top bullets should scream: ‚Äúowning end-to-end ML models in production, real-time, with monitoring‚Äù:\n",
      "\n",
      "1. Rephrase bullet 1 to stress **real-time predictions, model performance, and production**:\n",
      "   - ‚ÄúDesign, troubleshoot, and optimize **end-to-end ML workflows** on AWS for enterprise customers‚Äîowning model packaging and deployment to **SageMaker** endpoints and **Bedrock**, with both **batch pipelines** (Glue/EMR + Step Functions) and **real-time prediction APIs** (API Gateway + Lambda / ECS/Fargate).‚Äù\n",
      "\n",
      "2. Bullet 3: bring ‚Äúmonitoring & improving production models‚Äù up:\n",
      "   - ‚ÄúDefine and review **monitoring plans for production ML services**, including CloudWatch logs/metrics/alarms, failure handling, retries, and canary strategies to detect model/service degradation before impacting users.‚Äù\n",
      "\n",
      "3. Bullet 2: tie to feature pipelines / modeling:\n",
      "   - ‚ÄúTranslate product and data science requirements into **deployable ML solutions** and cloud work packages using IaC (Terraform/CloudFormation, familiar with CDK), covering data pipelines, networking (VPC, IAM), storage (S3), security, scaling, and cost controls.‚Äù\n",
      "\n",
      "4. Bullet 4: connect to risk-like non-functional requirements:\n",
      "   - ‚ÄúCollaborate with architects and product owners to design ML-enabled services that meet stringent **non-functional requirements** (availability, performance, security, observability) typical of risk- and compliance-sensitive workloads.‚Äù\n",
      "\n",
      "5. Bullet 5: align with ‚ÄúLLMs for NLP and agentic systems‚Äù:\n",
      "   - ‚ÄúExplore **GenAI and agentic patterns** on **Bedrock**, advising customers on feasibility of LLM-based assistants and **RAG-ready designs** integrated with existing microservices and operational workflows.‚Äù\n",
      "\n",
      "If you have any ticket patterns where you helped with fraud/risk use cases (even high-level), you can (truthfully) add a phrase like ‚Äúincluding security- and risk-oriented workloads‚Äù, but only if accurate.\n",
      "\n",
      "#### 2) Prompt Engineer & Software Engineer ‚Äì Meta (Contract)\n",
      "\n",
      "This maps well to ‚ÄúLLMs, NLP, agentic systems, evaluation.‚Äù\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- LLM evaluation, metrics, and iteration.\n",
      "- Working in a pod, fast experimentation loops.\n",
      "- Tools that support high-throughput data labeling / experimentation.\n",
      "\n",
      "**Concrete adjustments:**\n",
      "\n",
      "1. First bullet:\n",
      "   - Original: ‚ÄúImproved LLM outcomes by refining prompts/evaluation guidelines; collaborated with researchers/engineers in an agile environment to productize evaluation workflows.‚Äù\n",
      "   - Suggested:\n",
      "     - ‚ÄúImproved **LLM model quality** by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an **agile pod** to design and productize **evaluation workflows** for large-scale experimentation.‚Äù\n",
      "\n",
      "2. Second bullet: explicitly connect to model hosting/pipelines:\n",
      "   - Original: Dockerized app playground with Flask + React.\n",
      "   - Suggested:\n",
      "     - ‚ÄúBuilt a **Dockerized experiment playground**‚Äî**Flask** backend + **React** UI‚Äîto launch/stop containerized services via REST, including status tracking, error handling, and audit logs, facilitating rapid **LLM/model experimentation and hosting**.‚Äù\n",
      "\n",
      "3. Third bullet:\n",
      "   - Original: Enhanced Chrome tool for dataset operations.\n",
      "   - Suggested:\n",
      "     - ‚ÄúEnhanced an internal **Chrome-based tooling** suite (JavaScript/DOM) for high-throughput dataset operations, improving throughput and consistency for **training and evaluation data** used in LLM experiments.‚Äù\n",
      "\n",
      "#### 3) ML Scientist (MDS Capstone) ‚Äì Citysage\n",
      "\n",
      "This is your best place to show **modeling** (time-series/spatial, anomaly-like).\n",
      "\n",
      "**Emphasize:**\n",
      "\n",
      "- Time-series / spatial modeling, large datasets.\n",
      "- End-to-end pipeline: data prep, modeling, evaluation, deployment/visualization.\n",
      "- Link to ‚Äúsequence modeling / anomaly detection‚Äù conceptually, but do not claim LSTMs if you didn‚Äôt use them.\n",
      "\n",
      "**Concrete bullet tweaks:**\n",
      "\n",
      "1. First bullet:\n",
      "   - Original: ‚ÄúDeveloped a time-series/spatial ML pipeline over 5M rows to model urban sound distribution; partnered with stakeholders to move analyses into a client-facing app and define monitoring/visualization needs.‚Äù\n",
      "   - Suggested:\n",
      "     - ‚ÄúDeveloped a **time-series and spatial ML pipeline** on 5M+ rows to model urban sound distribution, including feature engineering, model training, and evaluation; partnered with stakeholders to integrate outputs into a client-facing app and define **monitoring/visualization** requirements.‚Äù\n",
      "\n",
      "2. Second bullet:\n",
      "   - Original: Altair heat-map.\n",
      "   - Suggested:\n",
      "     - ‚ÄúDelivered an **Altair-based interactive heat-map** component to support decision-making and downstream analytics for non-technical users.‚Äù\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Reordering suggestions\n",
      "\n",
      "- Keep overall section order (Summary ‚Üí Education ‚Üí Experience ‚Üí Technical Proficiencies ‚Üí Certifications) as your template, but:\n",
      "  - For Coinbase, **Experience** is more important than Education; if your template allows, you could move ‚ÄúProfessional Experience‚Äù above ‚ÄúEducation.‚Äù If the layout doesn‚Äôt break, I recommend:\n",
      "    - Professional Summary  \n",
      "    - Professional Experience  \n",
      "    - Technical Proficiencies  \n",
      "    - Education  \n",
      "    - Certifications\n",
      "\n",
      "- Within Experience, ordering is already chronological; keep as is.\n",
      "\n",
      "- Within the AWS role, reorder bullets to prioritize:\n",
      "  1. End-to-end ML workflows & real-time predictions.\n",
      "  2. Monitoring & production reliability.\n",
      "  3. Translating DS requirements into deployable solutions.\n",
      "  4. Non-functional requirements & risk-like constraints.\n",
      "  5. GenAI/LLM/agentic patterns.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Major gaps relative to the Coinbase role\n",
      "\n",
      "These are areas you should not claim but can mention as interests in a cover letter or interview:\n",
      "\n",
      "1. **Domain-specific Risk/Fraud ML**\n",
      "   - No explicit experience in fraud detection, scam modeling, transaction risk, withdrawal limit models, or account takeover models.\n",
      "   - No explicit ‚ÄúRisk ML,‚Äù ‚Äúanomaly detection for fraud,‚Äù or similar in the resume.\n",
      "\n",
      "2. **Crypto / Blockchain / Financial Systems**\n",
      "   - No mention of crypto, blockchain, on-chain data, exchanges, or financial transaction workflows.\n",
      "   - For Coinbase, this is a domain gap you can address with a short statement of interest in the cover letter, not the resume.\n",
      "\n",
      "3. **GNNs, sequence models (LSTMs), RL**\n",
      "   - Not present in resume; required/desired in posting.\n",
      "   - Do not add them.\n",
      "\n",
      "4. **Feature store / centralized feature platform**\n",
      "   - They mention ‚Äúfeature stores like Tecton‚Äù; you don‚Äôt list any feature store.\n",
      "   - You do have ETL/feature-pipeline tools (Spark, Glue, Airflow); you can emphasize those as feature pipelines, but not feature stores.\n",
      "\n",
      "5. **Risk systems with adaptive friction**\n",
      "   - They look for experience building **context-aware risk systems** that choose friction (quiz, LLM agent, human review).\n",
      "   - Your resume doesn‚Äôt mention adaptive user flows or risk scoring logic.\n",
      "   - You could emphasize that you build systems with ‚ÄúB2C-style AI services‚Äù and production decisioning, but not specific friction logic.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Summary of key edits to make\n",
      "\n",
      "1. **Subtitle**: change to ‚ÄúMachine Learning Engineer‚Äù.\n",
      "2. **Professional Summary**:\n",
      "   - Emphasize end-to-end modeling, real-time predictions, production deployment, LLM applications, monitoring.\n",
      "   - Slightly shift from pure infra to ‚Äúowning models + infra.‚Äù\n",
      "3. **Technical Proficiencies**:\n",
      "   - Reorder MLOps & ML section to highlight PyTorch, LLMs, monitoring.\n",
      "   - Clarify Big Data & Pipelines as ETL/feature pipelines.\n",
      "   - Add a ‚ÄúData Science & Analytics‚Äù row using existing technologies (Altair, etc.).\n",
      "4. **AWS Experience bullets**:\n",
      "   - Rephrase to focus on production ML models, real-time predictions, monitoring, and integrating ML into user-facing services.\n",
      "   - Bring monitoring, reliability, and non-functional requirements forward.\n",
      "5. **Meta Experience bullets**:\n",
      "   - Highlight LLM evaluation, experimentation, and tooling.\n",
      "6. **Citysage Experience**:\n",
      "   - Emphasize time-series/spatial modeling and end-to-end pipeline work, tying to complex modeling experience.\n",
      "7. **Do not add**:\n",
      "   - Any mention of fraud, risk models, crypto, GNNs, LSTMs, Kafka, feature stores, or RL unless they are genuinely part of your experience and appear somewhere in your materials.\n",
      "\n",
      "If you‚Äôd like, I can next produce updated **Summary** and **Technical Proficiencies** LaTeX sections in GENERATE MODE that incorporate these edits while preserving your template.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file paths \n",
    "original_resume = \"data/original/AI_engineer.tex\"      # Your resume here\n",
    "job_posting = \"data/job_postings/coinbase.txt\"         # Job posting\n",
    "output_file = \"data/tailored_versions/resume_ml_engineer.tex\"  # Output\n",
    "\n",
    "# Instructions for the agent\n",
    "tailoring_request = f\"\"\"\n",
    "You are now in ANALYSIS MODE.\n",
    "\n",
    "First, you MUST load the inputs using your tools:\n",
    "\n",
    "1. Call the `read_file` tool with the path \"{original_resume}\" to load my LaTeX resume.\n",
    "2. Call the `read_file` tool with the path \"{job_posting}\" to load the job posting text.\n",
    "\n",
    "After you have loaded BOTH files, do the following:\n",
    "\n",
    "1. Analyze the job posting and extract key requirements, responsibilities, and skills.\n",
    "2. Compare them with the content of the LaTeX resume.\n",
    "3. Suggest specific, concrete edits to the resume, for example:\n",
    "   - Which bullets to rephrase (and provide the new wording).\n",
    "   - Which skills to emphasize or de-emphasize.\n",
    "   - Any reordering of sections or bullets that would improve alignment.\n",
    "4. Point out any major gaps where the job requires something NOT present in the resume.\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT output LaTeX in this step‚Äîonly natural-language analysis and suggestions.\n",
    "- Do NOT assume the resume or job posting are already in the conversation; you must read them using `read_file` first.\n",
    "\"\"\"\n",
    "\n",
    "# Get initial analysis\n",
    "print(\"Analyzing resume and job posting...\\n\")\n",
    "analysis = resume_agent(tailoring_request)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Iterative Refinement\n",
    "\n",
    "After getting suggestions, you can iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation for refinement\n",
    "refinement_request = \"\"\"\n",
    "Based on your analysis, please:\n",
    "1. Focus on highlighting my AWS and Python experience\n",
    "2. Emphasize any machine learning projects\n",
    "3. Ensure keywords match the job posting for ATS\n",
    "4. Keep the resume to 1 page if possible\n",
    "\n",
    "Show me the specific sections that should change.\n",
    "\"\"\"\n",
    "\n",
    "refinement = resume_agent(refinement_request)\n",
    "print(refinement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Generate Final Tailored Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: read_file\n",
      "\n",
      "Tool #4: update_subtitle\n",
      "\n",
      "Tool #5: get_section_names\n",
      "\n",
      "Tool #6: extract_section\n",
      "\n",
      "Tool #7: extract_section\n",
      "\n",
      "Tool #8: extract_section\n",
      "\n",
      "Tool #9: write_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI threw rate limit error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #10: read_file\n",
      "\n",
      "Tool #11: validate_latex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI threw rate limit error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%-------------------------------------\n",
      "% LaTeX Resume for Software Engineers\n",
      "% Author : Leslie Cheng\n",
      "% License : MIT\n",
      "%-------------------------------------\n",
      "\n",
      "\\documentclass[a4paper,10.5pt]{article}[leftmargin=*]\n",
      "\n",
      "\\usepackage[empty]{fullpage}\n",
      "\\usepackage{enumitem}\n",
      "\\usepackage{ifxetex}\n",
      "\\ifxetex\n",
      "  \\usepackage{fontspec}\n",
      "  \\usepackage[xetex]{hyperref}\n",
      "\\else\n",
      "  \\usepackage[utf8]{inputenc}\n",
      "  \\usepackage[T1]{fontenc}\n",
      "  \\usepackage[pdftex]{hyperref}\n",
      "\\fi\n",
      "\\usepackage{fontawesome}\n",
      "\\usepackage[sfdefault,light]{FiraSans}\n",
      "\\usepackage{anyfontsize}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{tabularx}\n",
      "\n",
      "%-------------------------------------------------- SETTINGS HERE --------------------------------------------------\n",
      "% Header settings\n",
      "\\def \\fullname {Yurui Feng}\n",
      "\\def \\subtitle {Machine Learning Engineer}\n",
      "\n",
      "\\def \\linkedinicon {\\faLinkedin}\n",
      "\\def \\linkedinlink {https://linkedin.com/in/yurui-feng/}\n",
      "\\def \\linkedintext {/yurui-feng}\n",
      "\n",
      "\\def \\phoneicon {\\faPhone}\n",
      "\\def \\phonetext {+1 778-323-9562}\n",
      "\n",
      "\\def \\emailicon {\\faEnvelope}\n",
      "\\def \\emaillink {mailto:yurui8rigby@gmail.com}\n",
      "\\def \\emailtext {yurui8rigby@gmail.com}\n",
      "\n",
      "\\def \\githubicon {\\faGithub}\n",
      "\\def \\githublink {https://github.com/Yurui-Feng}\n",
      "\\def \\githubtext {/Yurui-Feng}\n",
      "\n",
      "\\def \\locationicon{\\faHome}\n",
      "\\def \\locationtext {Toronto, ON, M5J 0C3}\n",
      "\n",
      "\\def \\websiteicon {\\faGlobe}\n",
      "\\def \\websitelink {https://www.fuyuri.com/}\n",
      "\\def \\websitetext {fuyuri.com}\n",
      "\n",
      "\n",
      "\\def \\headertype {\\doublecol} % \\singlecol or \\doublecol\n",
      "\n",
      "% Misc settings\n",
      "\\def \\entryspacing {0pt}\n",
      "\n",
      "\\def \\bulletstyle {\\faAngleRight}\n",
      "\n",
      "% Define colours\n",
      "\\definecolor{primary}{HTML}{000000}\n",
      "\\definecolor{secondary}{HTML}{0D47A1}\n",
      "\\definecolor{accent}{HTML}{263238}\n",
      "\\definecolor{links}{HTML}{1565C0}\n",
      "\n",
      "%------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "% Defines to make listing easier\n",
      "\\def \\linkedin {\\linkedinicon \\hspace{3pt}\\href{\\linkedinlink}{\\linkedintext}}\n",
      "\\def \\phone {\\phoneicon \\hspace{3pt}{ \\phonetext}}\n",
      "\\def \\email {\\emailicon \\hspace{3pt}\\href{\\emaillink}{\\emailtext}}\n",
      "\\def \\github {\\githubicon \\hspace{3pt}\\href{\\githublink}{\\githubtext}}\n",
      "\\def \\location {\\locationicon \\hspace{3pt}{ \\locationtext}}\n",
      "\\def \\website {\\websiteicon \\hspace{3pt}\\href{\\websitelink}{\\websitetext}}\n",
      "\n",
      "% Adjust margins\n",
      "\\addtolength{\\oddsidemargin}{-0.55in}\n",
      "\\addtolength{\\evensidemargin}{-0.55in}\n",
      "\\addtolength{\\textwidth}{1.1in}\n",
      "\\addtolength{\\topmargin}{-0.7in}\n",
      "\\addtolength{\\textheight}{1.1in}\n",
      "\n",
      "% Define the link colours\n",
      "\\hypersetup{\n",
      "    colorlinks=true,\n",
      "    urlcolor=links,\n",
      "}\n",
      "\n",
      "% Set the margin alignment \n",
      "\\raggedbottom\n",
      "\\raggedright\n",
      "\\setlength{\\tabcolsep}{0in}\n",
      "\n",
      "%-------------------------\n",
      "% Custom commands\n",
      "\n",
      "\n",
      "% Sections\n",
      "\\renewcommand{\\section}[2]{\\vspace{7pt}\n",
      "  \\colorbox{secondary}{\\color{white}\\raggedbottom\\normalsize\\textbf{{#1}{\\hspace{8pt}#2}}}\n",
      "}\n",
      "\n",
      "% Entry start and end, for spacing\n",
      "\\newcommand{\\resumeEntryStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeEntryEnd}{\\end{itemize}\\vspace{\\entryspacing}}\n",
      "\n",
      "% Itemized list for the bullet points under an entry, if necessary\n",
      "\\newcommand{\\resumeItemListStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeItemListEnd}{\\end{itemize}}\n",
      "\n",
      "% Resume item\n",
      "\\renewcommand{\\labelitemii}{\\bulletstyle}\n",
      "\\newcommand{\\resumeItem}[1]{\n",
      "  \\item\\small{\n",
      "    {#1 \\vspace{-2pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Entry with title, subheading, date(s), and location\n",
      "\\newcommand{\\resumeEntryTSDL}[4]{\n",
      "  \\vspace{-3pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "      \\textit{\\color{accent}\\small#3} & \\textit{\\color{accent}\\small#4} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry with title and date(s)\n",
      "\\newcommand{\\resumeEntryTD}[2]{\n",
      "  \\vspace{-1pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry for special (skills)\n",
      "\\newcommand{\\resumeEntryS}[2]{\n",
      "  \\item[]\\small{\n",
      "    \\textbf{\\color{primary}#1 }{ #2 \\vspace{-6pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Double column header\n",
      "\\newcommand{\\doublecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l@{\\hspace{1.5em}}l}\n",
      "        {\\small#4} & {\\small#1} \\\\\n",
      "        {\\small#5} & {\\small#2} \\\\\n",
      "        {\\small#6} & {\\small#3}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "% Single column header\n",
      "\\newcommand{\\singlecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[b]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        {\\small#1} \\\\\n",
      "        {\\small#2} \\\\\n",
      "        {\\small#3} \\\\\n",
      "        {\\small#4} \\\\\n",
      "        {\\small#5} \\\\\n",
      "        {\\small#6}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "%-------------------------------------------------- BEGIN HERE --------------------------------------------------\n",
      "\n",
      "%---------------------------------------------------- HEADER ----------------------------------------------------\n",
      "\n",
      "\\headertype{\\linkedin}{\\github}{\\website}{\\phone}{\\location}{\\email}{} % Set the order of items here\n",
      "\\vspace{-10pt} % Set a negative value to push the body up, and the opposite\n",
      "\n",
      "% ======= TARGETED SUMMARY =======\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{\n",
      "Machine Learning Engineer with hands-on experience designing, training, and deploying production ML systems on AWS. Proficient in \\textbf{Python} and \\textbf{SQL}; build \\textbf{real-time and batch inference} services with \\textbf{SageMaker, Bedrock, Lambda, Step Functions, ECS/Fargate}, and implement observability with CloudWatch. Partner closely with data scientists to graduate POCs into \\textbf{production-ready} models and services, define \\textbf{monitoring plans}, and meet non-functional requirements (reliability, security, cost). Built lightweight \\textbf{UIs/services} (Flask/React) to host models, support experimentation, and enable automated decisioning. Familiar with \\textbf{LLM}-based applications, including \\textbf{RAG} concepts and \\textbf{agentic patterns} (tool use, orchestration, memory). Comfortable owning the full lifecycle from feature pipelines and model training to deployment and monitoring in production.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- EDUCATION --------------------------------------------------\n",
      "\\section{\\faGraduationCap}{Education}\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of British Columbia}{09/2022 -- 07/2023}\n",
      "      {Master of Data Science (GPA: 4.0/4.0)}{Vancouver, BC}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {Boston University}{09/2020 -- 05/2022}\n",
      "      {Master of Arts, Psychology (GPA: 3.9/4.0)}{Boston, MA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of California, San Diego}{09/2017 -- 03/2020}\n",
      "      {B.S., Cognitive and Behavioral Neuroscience (GPA:3.8/4.0)}{San Diego, CA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- PROFESSIONAL EXPERIENCE --------------------------------------------------\n",
      "\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Design, troubleshoot, and optimize end-to-end ML workflows on AWS for enterprise customers‚Äîowning \\textbf{model packaging \\& deployment} to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}, and building \\textbf{batch} pipelines (Glue/EMR + \\textbf{Step Functions}) and \\textbf{real-time prediction} APIs (API Gateway + \\textbf{Lambda} / \\textbf{ECS/Fargate}).}\n",
      "    \\resumeItem{Define and review \\textbf{monitoring plans} for production ML services, including CloudWatch logs/metrics/alarms, failure handling, retries, and canary strategies to detect and mitigate model/service degradation before impacting users.}\n",
      "    \\resumeItem{Translate product and data science requirements into deployable \\textbf{ML solutions} and cloud work packages using IaC (\\textbf{Terraform/CloudFormation}; familiar with \\textbf{CDK}) covering data pipelines, networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.}\n",
      "    \\resumeItem{Collaborate with architects and product owners to design ML-enabled services that meet stringent \\textbf{non-functional requirements} (availability, performance, security, observability) for B2C-style AI services and downstream data flows.}\n",
      "    \\resumeItem{Explore \\textbf{GenAI} and \\textbf{agentic} patterns on \\textbf{Bedrock}, advising customers on feasibility of LLM-based assistants and \\textbf{RAG}-ready designs integrated with existing microservices and operational workflows.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Improved \\textbf{LLM model quality} by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an \\textbf{agile pod} to design and productize \\textbf{evaluation workflows} for large-scale experimentation.}\n",
      "    \\resumeItem{Built a \\textbf{Dockerized experiment playground}‚Äî\\textbf{Flask} backend + \\textbf{React} UI‚Äîto launch/stop containerized services via REST, including status tracking, error handling, and audit logs that facilitate rapid \\textbf{LLM/model experimentation and hosting}.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome}-based tool (JavaScript/DOM) for high-throughput dataset operations, improving throughput and consistency for training and evaluation data used in LLM experiments.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed a \\textbf{time-series and spatial ML pipeline} on 5M+ rows to model urban sound distribution, including feature engineering, model training, and evaluation; partnered with stakeholders to integrate outputs into a client-facing app and define \\textbf{monitoring/visualization} requirements.}\n",
      "    \\resumeItem{Delivered an \\textbf{Altair}-based interactive heat-map component to support decision-making and downstream analytics for non-technical users.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "% ======= TECHNICAL SKILLS =======\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{\\textbf{Python} (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{MLOps \\& ML}{PyTorch, scikit-learn, HuggingFace; \\textbf{SageMaker} (training, tuning, endpoints), \\textbf{Bedrock} (LLM \\& agentic patterns), MLlib}\n",
      "  \\resumeEntryS{Big Data \\& Pipelines}{\\textbf{Apache Spark} (EMR 6/7), Hadoop, Hive, Presto, Flink; \\textbf{Airflow} (MWAA); AWS Glue for ETL \\& feature pipelines}\n",
      "  \\resumeEntryS{Cloud \\& Infra}{\\textbf{AWS} (VPC, IAM, S3, RDS, DynamoDB, ECS/Fargate, \\textbf{EKS}, API Gateway, Lambda, CloudWatch, Step Functions), \\textbf{Terraform}/CloudFormation, Docker, CI/CD}\n",
      "  \\resumeEntryS{Datastores}{PostgreSQL, MySQL, DynamoDB, MongoDB (\\textit{NoSQL}), OpenSearch Vector DB, S3 Vectors}\n",
      "  \\resumeEntryS{Web/UI}{Flask, React, REST APIs, Node/Express, HTML/CSS}\n",
      "  \\resumeEntryS{Observability}{CloudWatch metrics/logs/alarms, Log Insights; experience defining monitoring for ML services}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\n",
      "%-------------------------------------------------- CERTIFICATIONS --------------------------------------------------\n",
      "\\section{\\faCertificate}{Certifications}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{AWS Certified Machine Learning - Specialty } {Amazon Web Services Training and Certification - 2023}\n",
      "  \\resumeEntryS{AWS Certified Solutions Architect - Associate } {Amazon Web Services Training and Certification - 2024}\n",
      "  \\resumeEntryS{Computing in Python IV: Objects \\& Algorithms } {Georgia Institute of Technology - edX}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\\end{document}%-------------------------------------\n",
      "% LaTeX Resume for Software Engineers\n",
      "% Author : Leslie Cheng\n",
      "% License : MIT\n",
      "%-------------------------------------\n",
      "\n",
      "\\documentclass[a4paper,10.5pt]{article}[leftmargin=*]\n",
      "\n",
      "\\usepackage[empty]{fullpage}\n",
      "\\usepackage{enumitem}\n",
      "\\usepackage{ifxetex}\n",
      "\\ifxetex\n",
      "  \\usepackage{fontspec}\n",
      "  \\usepackage[xetex]{hyperref}\n",
      "\\else\n",
      "  \\usepackage[utf8]{inputenc}\n",
      "  \\usepackage[T1]{fontenc}\n",
      "  \\usepackage[pdftex]{hyperref}\n",
      "\\fi\n",
      "\\usepackage{fontawesome}\n",
      "\\usepackage[sfdefault,light]{FiraSans}\n",
      "\\usepackage{anyfontsize}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{tabularx}\n",
      "\n",
      "%-------------------------------------------------- SETTINGS HERE --------------------------------------------------\n",
      "% Header settings\n",
      "\\def \\fullname {Yurui Feng}\n",
      "\\def \\subtitle {Machine Learning Engineer}\n",
      "\n",
      "\\def \\linkedinicon {\\faLinkedin}\n",
      "\\def \\linkedinlink {https://linkedin.com/in/yurui-feng/}\n",
      "\\def \\linkedintext {/yurui-feng}\n",
      "\n",
      "\\def \\phoneicon {\\faPhone}\n",
      "\\def \\phonetext {+1 778-323-9562}\n",
      "\n",
      "\\def \\emailicon {\\faEnvelope}\n",
      "\\def \\emaillink {mailto:yurui8rigby@gmail.com}\n",
      "\\def \\emailtext {yurui8rigby@gmail.com}\n",
      "\n",
      "\\def \\githubicon {\\faGithub}\n",
      "\\def \\githublink {https://github.com/Yurui-Feng}\n",
      "\\def \\githubtext {/Yurui-Feng}\n",
      "\n",
      "\\def \\locationicon{\\faHome}\n",
      "\\def \\locationtext {Toronto, ON, M5J 0C3}\n",
      "\n",
      "\\def \\websiteicon {\\faGlobe}\n",
      "\\def \\websitelink {https://www.fuyuri.com/}\n",
      "\\def \\websitetext {fuyuri.com}\n",
      "\n",
      "\n",
      "\\def \\headertype {\\doublecol} % \\singlecol or \\doublecol\n",
      "\n",
      "% Misc settings\n",
      "\\def \\entryspacing {0pt}\n",
      "\n",
      "\\def \\bulletstyle {\\faAngleRight}\n",
      "\n",
      "% Define colours\n",
      "\\definecolor{primary}{HTML}{000000}\n",
      "\\definecolor{secondary}{HTML}{0D47A1}\n",
      "\\definecolor{accent}{HTML}{263238}\n",
      "\\definecolor{links}{HTML}{1565C0}\n",
      "\n",
      "%------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "% Defines to make listing easier\n",
      "\\def \\linkedin {\\linkedinicon \\hspace{3pt}\\href{\\linkedinlink}{\\linkedintext}}\n",
      "\\def \\phone {\\phoneicon \\hspace{3pt}{ \\phonetext}}\n",
      "\\def \\email {\\emailicon \\hspace{3pt}\\href{\\emaillink}{\\emailtext}}\n",
      "\\def \\github {\\githubicon \\hspace{3pt}\\href{\\githublink}{\\githubtext}}\n",
      "\\def \\location {\\locationicon \\hspace{3pt}{ \\locationtext}}\n",
      "\\def \\website {\\websiteicon \\hspace{3pt}\\href{\\websitelink}{\\websitetext}}\n",
      "\n",
      "% Adjust margins\n",
      "\\addtolength{\\oddsidemargin}{-0.55in}\n",
      "\\addtolength{\\evensidemargin}{-0.55in}\n",
      "\\addtolength{\\textwidth}{1.1in}\n",
      "\\addtolength{\\topmargin}{-0.7in}\n",
      "\\addtolength{\\textheight}{1.1in}\n",
      "\n",
      "% Define the link colours\n",
      "\\hypersetup{\n",
      "    colorlinks=true,\n",
      "    urlcolor=links,\n",
      "}\n",
      "\n",
      "% Set the margin alignment \n",
      "\\raggedbottom\n",
      "\\raggedright\n",
      "\\setlength{\\tabcolsep}{0in}\n",
      "\n",
      "%-------------------------\n",
      "% Custom commands\n",
      "\n",
      "\n",
      "% Sections\n",
      "\\renewcommand{\\section}[2]{\\vspace{7pt}\n",
      "  \\colorbox{secondary}{\\color{white}\\raggedbottom\\normalsize\\textbf{{#1}{\\hspace{8pt}#2}}}\n",
      "}\n",
      "\n",
      "% Entry start and end, for spacing\n",
      "\\newcommand{\\resumeEntryStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeEntryEnd}{\\end{itemize}\\vspace{\\entryspacing}}\n",
      "\n",
      "% Itemized list for the bullet points under an entry, if necessary\n",
      "\\newcommand{\\resumeItemListStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeItemListEnd}{\\end{itemize}}\n",
      "\n",
      "% Resume item\n",
      "\\renewcommand{\\labelitemii}{\\bulletstyle}\n",
      "\\newcommand{\\resumeItem}[1]{\n",
      "  \\item\\small{\n",
      "    {#1 \\vspace{-2pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Entry with title, subheading, date(s), and location\n",
      "\\newcommand{\\resumeEntryTSDL}[4]{\n",
      "  \\vspace{-3pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "      \\textit{\\color{accent}\\small#3} & \\textit{\\color{accent}\\small#4} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry with title and date(s)\n",
      "\\newcommand{\\resumeEntryTD}[2]{\n",
      "  \\vspace{-1pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry for special (skills)\n",
      "\\newcommand{\\resumeEntryS}[2]{\n",
      "  \\item[]\\small{\n",
      "    \\textbf{\\color{primary}#1 }{ #2 \\vspace{-6pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Double column header\n",
      "\\newcommand{\\doublecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l@{\\hspace{1.5em}}l}\n",
      "        {\\small#4} & {\\small#1} \\\\\n",
      "        {\\small#5} & {\\small#2} \\\\\n",
      "        {\\small#6} & {\\small#3}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "% Single column header\n",
      "\\newcommand{\\singlecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[b]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        {\\small#1} \\\\\n",
      "        {\\small#2} \\\\\n",
      "        {\\small#3} \\\\\n",
      "        {\\small#4} \\\\\n",
      "        {\\small#5} \\\\\n",
      "        {\\small#6}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "%-------------------------------------------------- BEGIN HERE --------------------------------------------------\n",
      "\n",
      "%---------------------------------------------------- HEADER ----------------------------------------------------\n",
      "\n",
      "\\headertype{\\linkedin}{\\github}{\\website}{\\phone}{\\location}{\\email}{} % Set the order of items here\n",
      "\\vspace{-10pt} % Set a negative value to push the body up, and the opposite\n",
      "\n",
      "% ======= TARGETED SUMMARY =======\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{\n",
      "Machine Learning Engineer with hands-on experience designing, training, and deploying production ML systems on AWS. Proficient in \\textbf{Python} and \\textbf{SQL}; build \\textbf{real-time and batch inference} services with \\textbf{SageMaker, Bedrock, Lambda, Step Functions, ECS/Fargate}, and implement observability with CloudWatch. Partner closely with data scientists to graduate POCs into \\textbf{production-ready} models and services, define \\textbf{monitoring plans}, and meet non-functional requirements (reliability, security, cost). Built lightweight \\textbf{UIs/services} (Flask/React) to host models, support experimentation, and enable automated decisioning. Familiar with \\textbf{LLM}-based applications, including \\textbf{RAG} concepts and \\textbf{agentic patterns} (tool use, orchestration, memory). Comfortable owning the full lifecycle from feature pipelines and model training to deployment and monitoring in production.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- EDUCATION --------------------------------------------------\n",
      "\\section{\\faGraduationCap}{Education}\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of British Columbia}{09/2022 -- 07/2023}\n",
      "      {Master of Data Science (GPA: 4.0/4.0)}{Vancouver, BC}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {Boston University}{09/2020 -- 05/2022}\n",
      "      {Master of Arts, Psychology (GPA: 3.9/4.0)}{Boston, MA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of California, San Diego}{09/2017 -- 03/2020}\n",
      "      {B.S., Cognitive and Behavioral Neuroscience (GPA:3.8/4.0)}{San Diego, CA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- PROFESSIONAL EXPERIENCE --------------------------------------------------\n",
      "\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Design, troubleshoot, and optimize end-to-end ML workflows on AWS for enterprise customers‚Äîowning \\textbf{model packaging \\& deployment} to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}, and building \\textbf{batch} pipelines (Glue/EMR + \\textbf{Step Functions}) and \\textbf{real-time prediction} APIs (API Gateway + \\textbf{Lambda} / \\textbf{ECS/Fargate}).}\n",
      "    \\resumeItem{Define and review \\textbf{monitoring plans} for production ML services, including CloudWatch logs/metrics/alarms, failure handling, retries, and canary strategies to detect and mitigate model/service degradation before impacting users.}\n",
      "    \\resumeItem{Translate product and data science requirements into deployable \\textbf{ML solutions} and cloud work packages using IaC (\\textbf{Terraform/CloudFormation}; familiar with \\textbf{CDK}) covering data pipelines, networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.}\n",
      "    \\resumeItem{Collaborate with architects and product owners to design ML-enabled services that meet stringent \\textbf{non-functional requirements} (availability, performance, security, observability) for B2C-style AI services and downstream data flows.}\n",
      "    \\resumeItem{Explore \\textbf{GenAI} and \\textbf{agentic} patterns on \\textbf{Bedrock}, advising customers on feasibility of LLM-based assistants and \\textbf{RAG}-ready designs integrated with existing microservices and operational workflows.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Improved \\textbf{LLM model quality} by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an \\textbf{agile pod} to design and productize \\textbf{evaluation workflows} for large-scale experimentation.}\n",
      "    \\resumeItem{Built a \\textbf{Dockerized experiment playground}‚Äî\\textbf{Flask} backend + \\textbf{React} UI‚Äîto launch/stop containerized services via REST, including status tracking, error handling, and audit logs that facilitate rapid \\textbf{LLM/model experimentation and hosting}.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome}-based tool (JavaScript/DOM) for high-throughput dataset operations, improving throughput and consistency for training and evaluation data used in LLM experiments.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed a \\textbf{time-series and spatial ML pipeline} on 5M+ rows to model urban sound distribution, including feature engineering, model training, and evaluation; partnered with stakeholders to integrate outputs into a client-facing app and define \\textbf{monitoring/visualization} requirements.}\n",
      "    \\resumeItem{Delivered an \\textbf{Altair}-based interactive heat-map component to support decision-making and downstream analytics for non-technical users.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "% ======= TECHNICAL SKILLS =======\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{\\textbf{Python} (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{MLOps \\& ML}{PyTorch, scikit-learn, HuggingFace; \\textbf{SageMaker} (training, tuning, endpoints), \\textbf{Bedrock} (LLM \\& agentic patterns), MLlib}\n",
      "  \\resumeEntryS{Big Data \\& Pipelines}{\\textbf{Apache Spark} (EMR 6/7), Hadoop, Hive, Presto, Flink; \\textbf{Airflow} (MWAA); AWS Glue for ETL \\& feature pipelines}\n",
      "  \\resumeEntryS{Cloud \\& Infra}{\\textbf{AWS} (VPC, IAM, S3, RDS, DynamoDB, ECS/Fargate, \\textbf{EKS}, API Gateway, Lambda, CloudWatch, Step Functions), \\textbf{Terraform}/CloudFormation, Docker, CI/CD}\n",
      "  \\resumeEntryS{Datastores}{PostgreSQL, MySQL, DynamoDB, MongoDB (\\textit{NoSQL}), OpenSearch Vector DB, S3 Vectors}\n",
      "  \\resumeEntryS{Web/UI}{Flask, React, REST APIs, Node/Express, HTML/CSS}\n",
      "  \\resumeEntryS{Observability}{CloudWatch metrics/logs/alarms, Log Insights; experience defining monitoring for ML services}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\n",
      "%-------------------------------------------------- CERTIFICATIONS --------------------------------------------------\n",
      "\\section{\\faCertificate}{Certifications}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{AWS Certified Machine Learning - Specialty } {Amazon Web Services Training and Certification - 2023}\n",
      "  \\resumeEntryS{AWS Certified Solutions Architect - Associate } {Amazon Web Services Training and Certification - 2024}\n",
      "  \\resumeEntryS{Computing in Python IV: Objects \\& Algorithms } {Georgia Institute of Technology - edX}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the final version\n",
    "final_request = f\"\"\"\n",
    "You are now in GENERATE MODE.\n",
    "\n",
    "Your goal is to produce a tailored LaTeX resume for the job posting we analyzed earlier and save it to \"{output_file}\".\n",
    "\n",
    "You have these tools: read_file, write_file, validate_latex, extract_section, replace_section, update_subtitle, merge_sections, get_section_names.\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. Call `read_file` with \"{original_resume}\" to load the ORIGINAL LaTeX resume.\n",
    "2. Identify the sections that should be updated for this role\n",
    "   (typically \"Professional Summary\", \"Professional Experience\", and \"Technical Proficiencies\").\n",
    "   You may use `get_section_names` and `extract_section` to inspect them.\n",
    "3. For each relevant section, rewrite the LaTeX content using the SAME macros and structure\n",
    "   (e.g., \\\\resumeEntryStart, \\\\resumeItem, \\\\resumeEntryS, etc.) so that it better matches\n",
    "   the job posting requirements, based on your previous analysis.\n",
    "4. Use `merge_sections` to merge your updated section LaTeX into the original file:\n",
    "   - original_file = \"{original_resume}\"\n",
    "   - updated_sections = {{\n",
    "       \"Professional Summary\": <your new LaTeX for that section>,\n",
    "       \"Professional Experience\": <your new LaTeX for that section>,\n",
    "       \"Technical Proficiencies\": <your new LaTeX for that section>,\n",
    "       \"subtitle\": <optional new subtitle/job title>\n",
    "     }}\n",
    "   - output_file = \"{output_file}\"\n",
    "5. After merging, call `read_file` on \"{output_file}\" to get the full updated LaTeX content.\n",
    "6. Call `validate_latex` on the full updated LaTeX to check for obvious syntax issues\n",
    "   (unbalanced braces, missing \\\\begin{{document}} / \\\\end{{document}}, etc.).\n",
    "   If validation reports errors, fix the LaTeX and re-run `validate_latex` until it passes.\n",
    "\n",
    "FINAL RESPONSE REQUIREMENTS:\n",
    "- In your final assistant message, RETURN ONLY the COMPLETE UPDATED LATEX SOURCE\n",
    "  (the contents of \"{output_file}\") with NO commentary, NO markdown fences, and NO tool logs.\n",
    "- Do NOT modify the LaTeX preamble (\\\\documentclass, \\\\usepackage, \\\\def, \\\\newcommand).\n",
    "- Only modify text inside existing commands and environments (sections, bullets, skills, etc.).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = resume_agent(final_request)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Valid: N/A\n",
      "  Errors: 0\n",
      "  Warnings: 0\n"
     ]
    }
   ],
   "source": [
    "# Direct tool invocation for validation\n",
    "if Path(output_file).exists():\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        tailored_content = f.read()\n",
    "    \n",
    "    # Validate using tool directly\n",
    "    validation = resume_agent.tool.validate_latex(latex_content=tailored_content)\n",
    "    \n",
    "    # Handle different return formats (dict, string, or other)\n",
    "    import json\n",
    "    if isinstance(validation, dict):\n",
    "        validation_dict = validation\n",
    "    elif isinstance(validation, str):\n",
    "        # Try to parse as JSON if it's a string\n",
    "        try:\n",
    "            validation_dict = json.loads(validation)\n",
    "        except json.JSONDecodeError:\n",
    "            # If not JSON, print the raw result\n",
    "            print(\"Validation Results (raw):\")\n",
    "            print(validation)\n",
    "            validation_dict = None\n",
    "    else:\n",
    "        print(f\"Validation returned unexpected type: {type(validation)}\")\n",
    "        print(f\"Value: {validation}\")\n",
    "        validation_dict = None\n",
    "    \n",
    "    if validation_dict:\n",
    "        print(\"Validation Results:\")\n",
    "        print(f\"  Valid: {validation_dict.get('is_valid', 'N/A')}\")\n",
    "        print(f\"  Errors: {len(validation_dict.get('errors', []))}\")\n",
    "        print(f\"  Warnings: {len(validation_dict.get('warnings', []))}\")\n",
    "        \n",
    "        if validation_dict.get('errors'):\n",
    "            print(\"\\nErrors found:\")\n",
    "            for error in validation_dict['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "        \n",
    "        if validation_dict.get('warnings'):\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in validation_dict['warnings']:\n",
    "                print(f\"  - {warning}\")\n",
    "        \n",
    "        if validation_dict.get('summary'):\n",
    "            print(f\"\\n{validation_dict['summary']}\")\n",
    "else:\n",
    "    print(f\"File not found: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "Utility functions for common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_tailor(resume_path: str, job_path: str, output_path: str, instructions: str = \"\"):\n",
    "    \"\"\"\n",
    "    Quick one-shot resume tailoring.\n",
    "    \n",
    "    Args:\n",
    "        resume_path: Path to original resume\n",
    "        job_path: Path to job posting\n",
    "        output_path: Path for tailored resume\n",
    "        instructions: Additional instructions for the agent\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Tailor my resume for this job posting.\n",
    "\n",
    "Resume: {resume_path}\n",
    "Job Posting: {job_path}\n",
    "Output: {output_path}\n",
    "\n",
    "Steps:\n",
    "1. Read both files\n",
    "2. Analyze job requirements\n",
    "3. Tailor resume content (preserve LaTeX formatting)\n",
    "4. Validate LaTeX syntax\n",
    "5. Save to output path\n",
    "\n",
    "{instructions if instructions else ''}\n",
    "\"\"\"\n",
    "    \n",
    "    response = resume_agent(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def batch_tailor(resume_path: str, job_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Tailor resume for multiple job postings.\n",
    "    \n",
    "    Args:\n",
    "        resume_path: Path to original resume\n",
    "        job_folder: Folder containing job posting files\n",
    "        output_folder: Folder for tailored resumes\n",
    "    \"\"\"\n",
    "    job_dir = Path(job_folder)\n",
    "    output_dir = Path(output_folder)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for job_file in job_dir.glob(\"*.txt\"):\n",
    "        output_name = f\"resume_{job_file.stem}.tex\"\n",
    "        output_path = output_dir / output_name\n",
    "        \n",
    "        print(f\"\\nTailoring for: {job_file.name}\")\n",
    "        result = quick_tailor(resume_path, str(job_file), str(output_path))\n",
    "        results.append({\"job\": job_file.name, \"output\": output_name, \"result\": result})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined:\")\n",
    "print(\"  - quick_tailor()\")\n",
    "print(\"  - batch_tailor()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add your resume**: Place your LaTeX resume in `data/original/resume.tex`\n",
    "2. **Add job postings**: Save job postings as `.txt` files in `data/job_postings/`\n",
    "3. **Run tailoring**: Use the examples above to tailor your resume\n",
    "4. **Iterate**: Work with the agent to refine the output\n",
    "5. **Validate**: Check LaTeX syntax before compiling\n",
    "6. **Compile**: Use `pdflatex` or your LaTeX editor to generate PDF\n",
    "\n",
    "### Tips for Best Results\n",
    "\n",
    "- Start with analysis and suggestions before generating the full resume\n",
    "- Be specific about what aspects to highlight\n",
    "- Review the agent's suggestions before applying them\n",
    "- Always validate LaTeX syntax\n",
    "- Keep conversation context for iterative improvements\n",
    "- Save different versions for different job types\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **LaTeX errors**: Use `validate_latex()` tool to check syntax\n",
    "- **Agent not following instructions**: Refine the system prompt in `prompts/system_prompt.txt`\n",
    "- **Missing features**: Add custom tools as needed\n",
    "- **Context lost**: Use conversation memory or save intermediate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
