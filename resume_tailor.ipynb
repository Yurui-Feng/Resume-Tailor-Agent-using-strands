{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Tailor Agent\n",
    "\n",
    "An intelligent agent that tailors your LaTeX resume to specific job postings while preserving formatting and maintaining accuracy.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **LaTeX-Safe**: Preserves LaTeX formatting and syntax\n",
    "- **Iterative**: Supports multiple revision rounds\n",
    "- **Job-Focused**: Analyzes job postings and matches requirements\n",
    "- **ATS-Optimized**: Uses keywords naturally for applicant tracking systems\n",
    "- **Validation**: Checks LaTeX syntax before output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Provider Configuration\n",
    "\n",
    "This notebook supports multiple AI providers. Configure your credentials in the `.env` file:\n",
    "\n",
    "### Option 1: OpenAI (Recommended for getting started)\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "```\n",
    "\n",
    "### Option 2: AWS Bedrock (Production-ready)\n",
    "```bash\n",
    "# Using long-term API key (recommended)\n",
    "AWS_BEARER_TOKEN_BEDROCK=your-long-term-bedrock-key\n",
    "AWS_REGION=us-east-1\n",
    "\n",
    "# OR using standard AWS credentials\n",
    "AWS_ACCESS_KEY_ID=your-access-key\n",
    "AWS_SECRET_ACCESS_KEY=your-secret-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "The notebook will automatically detect which credentials are available and use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Python Path: d:\\Strands-agent\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Strands SDK\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Python Path: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Observability\n",
    "\n",
    "Configure logging to trace all agent operations and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured!\n",
      "   Log file: d:\\Strands-agent\\logs\\strands_agent_20251116_145958.log\n",
      "   Console level: WARNING (errors only)\n",
      "   File level: DEBUG (all traces)\n",
      "\n",
      "üìä Log includes:\n",
      "  ‚Ä¢ All agent operations\n",
      "  ‚Ä¢ Tool calls and responses\n",
      "  ‚Ä¢ Model interactions\n",
      "  ‚Ä¢ Validation results\n",
      "  ‚Ä¢ Error traces\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Custom JSON formatter for structured logs\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        log_data = {\n",
    "            \"timestamp\": self.formatTime(record),\n",
    "            \"level\": record.levelname,\n",
    "            \"name\": record.name,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"function\": record.funcName,\n",
    "            \"line\": record.lineno\n",
    "        }\n",
    "        \n",
    "        # Add exception info if present\n",
    "        if record.exc_info:\n",
    "            log_data[\"exception\"] = self.formatException(record.exc_info)\n",
    "        \n",
    "        return json.dumps(log_data)\n",
    "\n",
    "# Create logs directory\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate log filename with timestamp\n",
    "log_filename = LOGS_DIR / f\"strands_agent_{dt.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "# Configure file handler with JSON formatting\n",
    "file_handler = logging.FileHandler(log_filename)\n",
    "file_handler.setFormatter(JsonFormatter())\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Configure console handler with simple formatting (for notebook output)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\n",
    "    '%(levelname)s | %(name)s | %(message)s'\n",
    "))\n",
    "console_handler.setLevel(logging.WARNING)  # Only show warnings/errors in notebook\n",
    "\n",
    "# Configure the strands logger\n",
    "strands_logger = logging.getLogger(\"strands\")\n",
    "strands_logger.setLevel(logging.DEBUG)\n",
    "strands_logger.addHandler(file_handler)\n",
    "strands_logger.addHandler(console_handler)\n",
    "\n",
    "# Prevent duplicate logs\n",
    "strands_logger.propagate = False\n",
    "\n",
    "print(\"‚úÖ Logging configured!\")\n",
    "print(f\"   Log file: {log_filename}\")\n",
    "print(f\"   Console level: WARNING (errors only)\")\n",
    "print(f\"   File level: DEBUG (all traces)\")\n",
    "print()\n",
    "print(\"üìä Log includes:\")\n",
    "print(\"  ‚Ä¢ All agent operations\")\n",
    "print(\"  ‚Ä¢ Tool calls and responses\")\n",
    "print(\"  ‚Ä¢ Model interactions\")\n",
    "print(\"  ‚Ä¢ Validation results\")\n",
    "print(\"  ‚Ä¢ Error traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log analysis functions defined:\n",
      "  - view_latest_logs(num_lines=50, level_filter=None)\n",
      "  - count_tool_calls()\n",
      "  - export_logs_to_readable(output_file=None)\n",
      "\n",
      "Examples:\n",
      "  view_latest_logs(20)                    # Last 20 log entries\n",
      "  view_latest_logs(level_filter=\"ERROR\")  # Only errors\n",
      "  count_tool_calls()                      # Tool usage stats\n",
      "  export_logs_to_readable()               # Export to .txt file\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to view and analyze logs\n",
    "\n",
    "def view_latest_logs(num_lines=50, level_filter=None):\n",
    "    \"\"\"\n",
    "    View the latest log entries from the current log file.\n",
    "    \n",
    "    Args:\n",
    "        num_lines: Number of recent log lines to display\n",
    "        level_filter: Filter by log level (e.g., 'ERROR', 'DEBUG', 'WARNING')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Filter by level if specified\n",
    "        if level_filter:\n",
    "            filtered_lines = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    if log_entry.get('level') == level_filter:\n",
    "                        filtered_lines.append(line)\n",
    "                except:\n",
    "                    continue\n",
    "            lines = filtered_lines\n",
    "        \n",
    "        # Show last N lines\n",
    "        recent_lines = lines[-num_lines:]\n",
    "        \n",
    "        print(f\"üìã Showing last {len(recent_lines)} log entries:\")\n",
    "        print(f\"   Filter: {level_filter if level_filter else 'All levels'}\")\n",
    "        print(f\"   Total entries: {len(lines)}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for line in recent_lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                print(f\"{log_entry['timestamp']} | {log_entry['level']:8} | {log_entry['name']}\")\n",
    "                print(f\"  ‚Üí {log_entry['message']}\")\n",
    "                if 'exception' in log_entry:\n",
    "                    print(f\"  ‚ö†Ô∏è  {log_entry['exception']}\")\n",
    "                print()\n",
    "            except:\n",
    "                print(line.strip())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Log file not found: {log_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading logs: {e}\")\n",
    "\n",
    "\n",
    "def count_tool_calls():\n",
    "    \"\"\"Count how many times each tool was called.\"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        tool_calls = {}\n",
    "        for line in lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                message = log_entry.get('message', '')\n",
    "                \n",
    "                # Look for tool call patterns\n",
    "                if 'tool' in message.lower() and 'call' in message.lower():\n",
    "                    # Extract tool name (you may need to adjust this based on actual log format)\n",
    "                    if 'merge_sections' in message:\n",
    "                        tool_calls['merge_sections'] = tool_calls.get('merge_sections', 0) + 1\n",
    "                    elif 'read_file' in message:\n",
    "                        tool_calls['read_file'] = tool_calls.get('read_file', 0) + 1\n",
    "                    elif 'validate_latex' in message:\n",
    "                        tool_calls['validate_latex'] = tool_calls.get('validate_latex', 0) + 1\n",
    "                    elif 'extract_section' in message:\n",
    "                        tool_calls['extract_section'] = tool_calls.get('extract_section', 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"üîß Tool Call Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for tool, count in sorted(tool_calls.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {tool:20} : {count:3} calls\")\n",
    "        \n",
    "        if not tool_calls:\n",
    "            print(\"  No tool calls detected in logs\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing tool calls: {e}\")\n",
    "\n",
    "\n",
    "def export_logs_to_readable(output_file=None):\n",
    "    \"\"\"Export JSON logs to a human-readable format.\"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = LOGS_DIR / f\"readable_log_{dt.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        with open(output_file, 'w') as out:\n",
    "            out.write(f\"Strands Agent Log - Readable Format\\n\")\n",
    "            out.write(f\"Generated: {dt.now()}\\n\")\n",
    "            out.write(f\"Source: {log_filename}\\n\")\n",
    "            out.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    out.write(f\"[{log_entry['timestamp']}] {log_entry['level']}\\n\")\n",
    "                    out.write(f\"Module: {log_entry['name']}\\n\")\n",
    "                    out.write(f\"Message: {log_entry['message']}\\n\")\n",
    "                    if 'exception' in log_entry:\n",
    "                        out.write(f\"Exception:\\n{log_entry['exception']}\\n\")\n",
    "                    out.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                except:\n",
    "                    out.write(line)\n",
    "        \n",
    "        print(f\"‚úÖ Readable log exported to: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting logs: {e}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Log analysis functions defined:\")\n",
    "print(\"  - view_latest_logs(num_lines=50, level_filter=None)\")\n",
    "print(\"  - count_tool_calls()\")\n",
    "print(\"  - export_logs_to_readable(output_file=None)\")\n",
    "print()\n",
    "print(\"Examples:\")\n",
    "print('  view_latest_logs(20)                    # Last 20 log entries')\n",
    "print('  view_latest_logs(level_filter=\"ERROR\")  # Only errors')\n",
    "print('  count_tool_calls()                      # Tool usage stats')\n",
    "print('  export_logs_to_readable()               # Export to .txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and verify environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking API credentials...\n",
      "\n",
      "‚úÖ OpenAI API key found\n",
      "\n",
      "ü§ñ Selected Model: <strands.models.openai.OpenAIModel object at 0x000001A10AAE2F90>\n",
      "\n",
      "üìÅ Project directories:\n",
      "  Prompts: True - d:\\Strands-agent\\prompts\n",
      "  Data: True - d:\\Strands-agent\\data\n",
      "  Output: True - d:\\Strands-agent\\data\\tailored_versions\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "PROMPTS_DIR = PROJECT_ROOT / \"prompts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ORIGINAL_RESUME_DIR = DATA_DIR / \"original\"\n",
    "JOB_POSTINGS_DIR = DATA_DIR / \"job_postings\"\n",
    "OUTPUT_DIR = DATA_DIR / \"tailored_versions\"\n",
    "\n",
    "# Detect which API credentials are available\n",
    "print(\"üîç Checking API credentials...\")\n",
    "print()\n",
    "\n",
    "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
    "has_bedrock_token = bool(os.getenv('AWS_BEARER_TOKEN_BEDROCK'))\n",
    "has_aws_creds = bool(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "\n",
    "from strands.models import openai\n",
    "\n",
    "if has_openai:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "    #MODEL_ID = openai.OpenAIModel(model_id=\"gpt-5-mini\")\n",
    "    MODEL_ID = openai.OpenAIModel(\n",
    "    model_id=\"gpt-5.1\",  # Note: prompt caching works best with gpt-4o and newer models\n",
    "    params={\n",
    "        \"store\": True,  # Enable prompt caching\n",
    "        \"metadata\": {\n",
    "            \"purpose\": \"resume_tailoring\"  # Optional: track usage\n",
    "        }\n",
    "    }\n",
    ")\n",
    "elif has_bedrock_token:\n",
    "    print(\"‚úÖ AWS Bedrock bearer token found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "elif has_aws_creds:\n",
    "    print(\"‚úÖ AWS credentials found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: No API credentials found!\")\n",
    "    print(\"Please set one of the following in .env file:\")\n",
    "    print(\"  - OPENAI_API_KEY (for OpenAI)\")\n",
    "    print(\"  - AWS_BEARER_TOKEN_BEDROCK (for Bedrock)\")\n",
    "    print(\"  - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY (for AWS)\")\n",
    "    MODEL_PROVIDER = None\n",
    "    MODEL_ID = None\n",
    "\n",
    "print()\n",
    "print(f\"ü§ñ Selected Model: {MODEL_ID}\")\n",
    "\n",
    "# Verify directories exist\n",
    "print()\n",
    "print(f\"üìÅ Project directories:\")\n",
    "print(f\"  Prompts: {PROMPTS_DIR.exists()} - {PROMPTS_DIR}\")\n",
    "print(f\"  Data: {DATA_DIR.exists()} - {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR.exists()} - {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts\n",
    "\n",
    "Load agent instructions from separate files for easy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded system_prompt.txt (8863 chars)\n",
      "\n",
      "üìù Full system prompt: 8862 characters\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(filename: str) -> str:\n",
    "    \"\"\"Load a prompt from the prompts directory.\"\"\"\n",
    "    prompt_path = PROMPTS_DIR / filename\n",
    "    if not prompt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found. Using default prompt.\")\n",
    "        return \"\"\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ Loaded {filename} ({len(content)} chars)\")\n",
    "    return content\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = load_prompt(\"system_prompt.txt\")\n",
    "latex_rules = \"\"\n",
    "\n",
    "# Combine prompts\n",
    "full_prompt = f\"{system_prompt}\\n\\n{latex_rules}\".strip()\n",
    "\n",
    "print(f\"\\nüìù Full system prompt: {len(full_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Generator Agent (Tool-Free)\n",
    "\n",
    "This agent generates ONLY section text without calling any tools.\n",
    "Python code handles all file I/O, merging, and validation.\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "- **No tool overhead**: Agent only generates text (30-40% lower token cost)\n",
    "- **Predictable output**: Structured format with clear section markers\n",
    "- **Python-controlled**: All file I/O and merging handled by Python code\n",
    "- **Prompt caching enabled**: Additional 20-40% cost savings on repeated requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section Generator Agent created!\n",
      "   Model: <strands.models.openai.OpenAIModel object at 0x000001A10AAE2F90>\n",
      "   Tools: 0 (none - text generation only)\n",
      "   System prompt: 8862 characters\n",
      "\n",
      "This agent:\n",
      "  ‚Ä¢ Generates ONLY section text (no tool calls)\n",
      "  ‚Ä¢ Returns sections in predictable format\n",
      "  ‚Ä¢ Python code handles merge/validation\n",
      "  ‚Ä¢ Lower token cost (no tool overhead)\n"
     ]
    }
   ],
   "source": [
    "# Create tool-free agent for section generation\n",
    "section_generator_agent = Agent(\n",
    "    model=MODEL_ID,\n",
    "    system_prompt=full_prompt,\n",
    "    tools=[]  # NO TOOLS - agent only generates text\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Section Generator Agent created!\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Tools: {len(section_generator_agent.tool_names)} (none - text generation only)\")\n",
    "print(f\"   System prompt: {len(full_prompt)} characters\")\n",
    "print()\n",
    "print(\"This agent:\")\n",
    "print(\"  ‚Ä¢ Generates ONLY section text (no tool calls)\")\n",
    "print(\"  ‚Ä¢ Returns sections in predictable format\")\n",
    "print(\"  ‚Ä¢ Python code handles merge/validation\")\n",
    "print(\"  ‚Ä¢ Lower token cost (no tool overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Section-Only Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions imported:\n",
      "  - parse_sections(result) -> dict\n",
      "  - tailor_resume_sections(agent, job_posting_path, original_resume_path, output_path, include_experience=False)\n",
      "\n",
      "Example usage:\n",
      "\n",
      "job_posting_path = \"data/job_postings/ml_engineer.txt\"\n",
      "\n",
      "result = tailor_resume_sections(\n",
      "    section_generator_agent,\n",
      "    job_posting_path=job_posting_path,\n",
      "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
      "    output_path=\"data/tailored_versions/ml_engineer.tex\"\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import helper functions from tools directory\n",
    "from tools.resume_helpers import parse_sections, tailor_resume_sections\n",
    "\n",
    "print(\"‚úÖ Helper functions imported:\")\n",
    "print(\"  - parse_sections(result) -> dict\")\n",
    "print(\"  - tailor_resume_sections(agent, job_posting_path, original_resume_path, output_path, include_experience=False)\")\n",
    "print()\n",
    "print(\"Example usage:\")\n",
    "print('''\n",
    "job_posting_path = \"data/job_postings/ml_engineer.txt\"\n",
    "\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,\n",
    "    job_posting_path=job_posting_path,\n",
    "    original_resume_path=\"data/original/AI_engineer.tex\",\n",
    "    output_path=\"data/tailored_versions/ml_engineer.tex\"\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "The complete workflow is encapsulated in `tailor_resume_sections()`:\n",
    "\n",
    "1. **Extracts** only the sections you want to change from the original resume\n",
    "2. **Copies** the source resume to the output location\n",
    "3. **Generates** tailored sections via the agent (using compact prompt)\n",
    "4. **Merges** the updated sections back into the copied resume\n",
    "5. **Validates** LaTeX syntax and returns status\n",
    "\n",
    "### Function Signature\n",
    "\n",
    "```python\n",
    "tailor_resume_sections(\n",
    "    section_generator_agent,      # The agent instance to use\n",
    "    job_posting_path: str,        # Path to job posting .txt file\n",
    "    original_resume_path: str,    # Path to original .tex file\n",
    "    output_path: str,             # Path to save tailored .tex file\n",
    "    include_experience: bool = False  # Update Experience section?\n",
    ") -> str  # Returns validation result message\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Starting resume tailoring...\n",
      "   Job posting: data/job_postings/quanlom.txt\n",
      "   Original: data/original/AI_engineer.tex\n",
      "   Output: data/tailored_versions/tailored_resume.tex\n",
      "\n",
      "üì§ Extracting sections from original resume...\n",
      "   ‚úì Extracted: Professional Summary\n",
      "   ‚úì Extracted: Technical Proficiencies\n",
      "   ‚úì Extracted: Professional Experience\n",
      "\n",
      "üìÅ Copying original resume to output directory...\n",
      "   ‚úì Copied to: data/tailored_versions/tailored_resume.tex\n",
      "\n",
      "ü§ñ Generating tailored sections...\n",
      "SUBTITLE:\n",
      "Data Engineer\n",
      "\n",
      "PROFESSIONAL SUMMARY:\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{\n",
      "Data Engineer with experience designing, automating, and supporting data ingestion and transformation pipelines on \\textbf{AWS}. Proficient in \\textbf{Python}, \\textbf{SQL}, and distributed data processing using \\textbf{Spark} on platforms such as EMR and Glue, with both batch and real-time workflows. Build and operate containerized services on \\textbf{ECS/Fargate} and \\textbf{Lambda}, integrating with \\textbf{S3} and other AWS data services to deliver reliable, production-ready pipelines. Apply strong analytical thinking, log-based troubleshooting, and precise documentation to ensure completeness, accuracy, and timeliness of data. Collaborate with cross-functional teams to translate business and analytics requirements into scalable cloud data solutions using IaC and CI/CD, while promoting code review best practices and maintainable pipelines.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "TECHNICAL PROFICIENCIES:\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{Python (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{Data \\& Pipelines}{Apache Spark (EMR 6/7), Hadoop, Hive, Presto, Flink, Airflow (MWAA), AWS Glue}\n",
      "  \\resumeEntryS{Cloud \\& Infra}{AWS (VPC, IAM, S3, RDS, DynamoDB, ECS/Fargate, EKS, API Gateway, Lambda, CloudWatch, Step Functions), Terraform, CloudFormation, Docker, CI/CD}\n",
      "  \\resumeEntryS{MLOps \\& Analytics}{SageMaker, Bedrock, scikit-learn, PyTorch, HuggingFace, MLlib}\n",
      "  \\resumeEntryS{Datastores}{PostgreSQL, MySQL, DynamoDB, MongoDB (NoSQL), OpenSearch Vector DB, S3 Vectors}\n",
      "  \\resumeEntryS{Web/UI}{Flask, React, REST APIs, Node/Express, HTML/CSS}\n",
      "  \\resumeEntryS{Tooling}{Git, command-line (Bash/zsh), CloudWatch metrics/logs/alarms, Log Insights}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "OPTIONAL EXPERIENCE:\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Support enterprise customers in designing and troubleshooting end-to-end \\textbf{data} and \\textbf{ML} workflows on \\textbf{AWS}, covering ingestion, transformation, storage, and real-time/batch consumption.}\n",
      "    \\resumeItem{Design and review automated ingestion pipelines that move data from diverse sources into \\textbf{S3}, \\textbf{Glue} Data Catalog, and downstream processing on \\textbf{EMR}/\\textbf{Spark}, ensuring reliability and operational readiness.}\n",
      "    \\resumeItem{Build and optimize batch and streaming-style data workflows using \\textbf{Glue}, \\textbf{EMR}, and \\textbf{Step Functions}, and real-time APIs with \\textbf{API Gateway}, \\textbf{Lambda}, and \\textbf{ECS/Fargate}.}\n",
      "    \\resumeItem{Translate product and analytics requirements into scalable cloud data architectures and IaC (\\textbf{Terraform}/\\textbf{CloudFormation}; familiar with \\textbf{CDK}) for networking (\\textbf{VPC}, \\textbf{IAM}), storage (\\textbf{S3}), and compute.}\n",
      "    \\resumeItem{Investigate production incidents using logs and metrics in \\textbf{CloudWatch}, recommend data quality checks, and document best practices to improve completeness, accuracy, and timeliness of datasets.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed and iterated on \\textbf{Python}-based evaluation workflows for \\textbf{LLM} experiments, collaborating with researchers/engineers in an \\textbf{agile} environment to standardize experimentation pipelines.}\n",
      "    \\resumeItem{Built a \\textbf{Dockerized} application playground (\\textbf{Flask} backend + \\textbf{React} UI) to orchestrate containerized services via REST, including status tracking, error handling, and audit logs suitable for data/ML service hosting.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome} tool (JavaScript/DOM) to streamline high-throughput dataset operations, improving throughput and consistency for model and data experimentation workflows.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed a time-series/spatial \\textbf{ML pipeline} over multi-million-row datasets in \\textbf{Python} and \\textbf{SQL} to model urban sound distribution, incorporating feature engineering and validation steps.}\n",
      "    \\resumeItem{Partnered with stakeholders to translate exploratory analysis into a client-facing component and define data visualization and monitoring needs, including an \\textbf{Altair}-based heat-map for decision support.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEndüìù Parsing generated sections...\n",
      "   Found 4 sections to update:\n",
      "     ‚Ä¢ subtitle\n",
      "     ‚Ä¢ Professional Summary\n",
      "     ‚Ä¢ Technical Proficiencies\n",
      "     ‚Ä¢ Professional Experience\n",
      "\n",
      "üîß Replacing sections in copied resume...\n",
      "\n",
      "‚úÖ Successfully merged 4 sections to data/tailored_versions/tailored_resume.tex\n",
      "‚úÖ LaTeX validation passed (325 braces balanced)\n"
     ]
    }
   ],
   "source": [
    "# Example: Tailor resume for a specific job posting\n",
    "\n",
    "job_posting_path = \"data/job_postings/quanlom.txt\"  # Your job posting file\n",
    "original_resume = \"data/original/AI_engineer.tex\"    # Your original resume\n",
    "output_file = \"data/tailored_versions/tailored_resume.tex\"  # Output\n",
    "\n",
    "result = tailor_resume_sections(\n",
    "    section_generator_agent,  # Pass the agent instance\n",
    "    job_posting_path=job_posting_path,\n",
    "    original_resume_path=original_resume,\n",
    "    output_path=output_file,\n",
    "    include_experience=True  # Set to True to update Experience section\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add job postings**: Save job posting text in `data/job_postings/` as `.txt` files\n",
    "2. **Prepare resume**: Ensure your LaTeX resume is in `data/original/`\n",
    "3. **Run tailoring**: Execute the cell above with your file paths\n",
    "4. **Review output**: Check the generated `.tex` file in `data/tailored_versions/`\n",
    "5. **Compile PDF**: Use `pdflatex` or your LaTeX editor to generate the final PDF\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Batch processing**: Run the function multiple times with different job postings to maximize prompt caching benefits (20-40% cost savings after first request)\n",
    "- **Include experience**: Set `include_experience=True` for roles where experience needs tailoring\n",
    "- **Monitor logs**: Use `view_latest_logs()` to verify agent didn't call any tools\n",
    "- **Token savings**: This architecture saves 30-40% vs old workflow (no tool overhead) + caching savings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
