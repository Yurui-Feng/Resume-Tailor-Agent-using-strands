{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Tailor Agent\n",
    "\n",
    "An intelligent agent that tailors your LaTeX resume to specific job postings while preserving formatting and maintaining accuracy.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **LaTeX-Safe**: Preserves LaTeX formatting and syntax\n",
    "- **Iterative**: Supports multiple revision rounds\n",
    "- **Job-Focused**: Analyzes job postings and matches requirements\n",
    "- **ATS-Optimized**: Uses keywords naturally for applicant tracking systems\n",
    "- **Validation**: Checks LaTeX syntax before output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Provider Configuration\n",
    "\n",
    "This notebook supports multiple AI providers. Configure your credentials in the `.env` file:\n",
    "\n",
    "### Option 1: OpenAI (Recommended for getting started)\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "```\n",
    "\n",
    "### Option 2: AWS Bedrock (Production-ready)\n",
    "```bash\n",
    "# Using long-term API key (recommended)\n",
    "AWS_BEARER_TOKEN_BEDROCK=your-long-term-bedrock-key\n",
    "AWS_REGION=us-east-1\n",
    "\n",
    "# OR using standard AWS credentials\n",
    "AWS_ACCESS_KEY_ID=your-access-key\n",
    "AWS_SECRET_ACCESS_KEY=your-secret-key\n",
    "AWS_REGION=us-east-1\n",
    "```\n",
    "\n",
    "The notebook will automatically detect which credentials are available and use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Python Path: d:\\Strands-agent\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Strands SDK\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Python Path: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Observability\n",
    "\n",
    "Configure logging to trace all agent operations and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured!\n",
      "   Log file: d:\\Strands-agent\\logs\\strands_agent_20251116_113948.log\n",
      "   Console level: WARNING (errors only)\n",
      "   File level: DEBUG (all traces)\n",
      "\n",
      "üìä Log includes:\n",
      "  ‚Ä¢ All agent operations\n",
      "  ‚Ä¢ Tool calls and responses\n",
      "  ‚Ä¢ Model interactions\n",
      "  ‚Ä¢ Validation results\n",
      "  ‚Ä¢ Error traces\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Custom JSON formatter for structured logs\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        log_data = {\n",
    "            \"timestamp\": self.formatTime(record),\n",
    "            \"level\": record.levelname,\n",
    "            \"name\": record.name,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"function\": record.funcName,\n",
    "            \"line\": record.lineno\n",
    "        }\n",
    "        \n",
    "        # Add exception info if present\n",
    "        if record.exc_info:\n",
    "            log_data[\"exception\"] = self.formatException(record.exc_info)\n",
    "        \n",
    "        return json.dumps(log_data)\n",
    "\n",
    "# Create logs directory\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate log filename with timestamp\n",
    "log_filename = LOGS_DIR / f\"strands_agent_{dt.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "# Configure file handler with JSON formatting\n",
    "file_handler = logging.FileHandler(log_filename)\n",
    "file_handler.setFormatter(JsonFormatter())\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Configure console handler with simple formatting (for notebook output)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\n",
    "    '%(levelname)s | %(name)s | %(message)s'\n",
    "))\n",
    "console_handler.setLevel(logging.WARNING)  # Only show warnings/errors in notebook\n",
    "\n",
    "# Configure the strands logger\n",
    "strands_logger = logging.getLogger(\"strands\")\n",
    "strands_logger.setLevel(logging.DEBUG)\n",
    "strands_logger.addHandler(file_handler)\n",
    "strands_logger.addHandler(console_handler)\n",
    "\n",
    "# Prevent duplicate logs\n",
    "strands_logger.propagate = False\n",
    "\n",
    "print(\"‚úÖ Logging configured!\")\n",
    "print(f\"   Log file: {log_filename}\")\n",
    "print(f\"   Console level: WARNING (errors only)\")\n",
    "print(f\"   File level: DEBUG (all traces)\")\n",
    "print()\n",
    "print(\"üìä Log includes:\")\n",
    "print(\"  ‚Ä¢ All agent operations\")\n",
    "print(\"  ‚Ä¢ Tool calls and responses\")\n",
    "print(\"  ‚Ä¢ Model interactions\")\n",
    "print(\"  ‚Ä¢ Validation results\")\n",
    "print(\"  ‚Ä¢ Error traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/Strands-agent/logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log analysis functions defined:\n",
      "  - view_latest_logs(num_lines=50, level_filter=None)\n",
      "  - count_tool_calls()\n",
      "  - export_logs_to_readable(output_file=None)\n",
      "\n",
      "Examples:\n",
      "  view_latest_logs(20)                    # Last 20 log entries\n",
      "  view_latest_logs(level_filter=\"ERROR\")  # Only errors\n",
      "  count_tool_calls()                      # Tool usage stats\n",
      "  export_logs_to_readable()               # Export to .txt file\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to view and analyze logs\n",
    "\n",
    "def view_latest_logs(num_lines=50, level_filter=None):\n",
    "    \"\"\"\n",
    "    View the latest log entries from the current log file.\n",
    "    \n",
    "    Args:\n",
    "        num_lines: Number of recent log lines to display\n",
    "        level_filter: Filter by log level (e.g., 'ERROR', 'DEBUG', 'WARNING')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Filter by level if specified\n",
    "        if level_filter:\n",
    "            filtered_lines = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    if log_entry.get('level') == level_filter:\n",
    "                        filtered_lines.append(line)\n",
    "                except:\n",
    "                    continue\n",
    "            lines = filtered_lines\n",
    "        \n",
    "        # Show last N lines\n",
    "        recent_lines = lines[-num_lines:]\n",
    "        \n",
    "        print(f\"üìã Showing last {len(recent_lines)} log entries:\")\n",
    "        print(f\"   Filter: {level_filter if level_filter else 'All levels'}\")\n",
    "        print(f\"   Total entries: {len(lines)}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for line in recent_lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                print(f\"{log_entry['timestamp']} | {log_entry['level']:8} | {log_entry['name']}\")\n",
    "                print(f\"  ‚Üí {log_entry['message']}\")\n",
    "                if 'exception' in log_entry:\n",
    "                    print(f\"  ‚ö†Ô∏è  {log_entry['exception']}\")\n",
    "                print()\n",
    "            except:\n",
    "                print(line.strip())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Log file not found: {log_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading logs: {e}\")\n",
    "\n",
    "\n",
    "def count_tool_calls():\n",
    "    \"\"\"Count how many times each tool was called.\"\"\"\n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        tool_calls = {}\n",
    "        for line in lines:\n",
    "            try:\n",
    "                log_entry = json.loads(line)\n",
    "                message = log_entry.get('message', '')\n",
    "                \n",
    "                # Look for tool call patterns\n",
    "                if 'tool' in message.lower() and 'call' in message.lower():\n",
    "                    # Extract tool name (you may need to adjust this based on actual log format)\n",
    "                    if 'merge_sections' in message:\n",
    "                        tool_calls['merge_sections'] = tool_calls.get('merge_sections', 0) + 1\n",
    "                    elif 'read_file' in message:\n",
    "                        tool_calls['read_file'] = tool_calls.get('read_file', 0) + 1\n",
    "                    elif 'validate_latex' in message:\n",
    "                        tool_calls['validate_latex'] = tool_calls.get('validate_latex', 0) + 1\n",
    "                    elif 'extract_section' in message:\n",
    "                        tool_calls['extract_section'] = tool_calls.get('extract_section', 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"üîß Tool Call Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for tool, count in sorted(tool_calls.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {tool:20} : {count:3} calls\")\n",
    "        \n",
    "        if not tool_calls:\n",
    "            print(\"  No tool calls detected in logs\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing tool calls: {e}\")\n",
    "\n",
    "\n",
    "def export_logs_to_readable(output_file=None):\n",
    "    \"\"\"Export JSON logs to a human-readable format.\"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = LOGS_DIR / f\"readable_log_{dt.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(log_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        with open(output_file, 'w') as out:\n",
    "            out.write(f\"Strands Agent Log - Readable Format\\n\")\n",
    "            out.write(f\"Generated: {dt.now()}\\n\")\n",
    "            out.write(f\"Source: {log_filename}\\n\")\n",
    "            out.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for line in lines:\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    out.write(f\"[{log_entry['timestamp']}] {log_entry['level']}\\n\")\n",
    "                    out.write(f\"Module: {log_entry['name']}\\n\")\n",
    "                    out.write(f\"Message: {log_entry['message']}\\n\")\n",
    "                    if 'exception' in log_entry:\n",
    "                        out.write(f\"Exception:\\n{log_entry['exception']}\\n\")\n",
    "                    out.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                except:\n",
    "                    out.write(line)\n",
    "        \n",
    "        print(f\"‚úÖ Readable log exported to: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting logs: {e}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Log analysis functions defined:\")\n",
    "print(\"  - view_latest_logs(num_lines=50, level_filter=None)\")\n",
    "print(\"  - count_tool_calls()\")\n",
    "print(\"  - export_logs_to_readable(output_file=None)\")\n",
    "print()\n",
    "print(\"Examples:\")\n",
    "print('  view_latest_logs(20)                    # Last 20 log entries')\n",
    "print('  view_latest_logs(level_filter=\"ERROR\")  # Only errors')\n",
    "print('  count_tool_calls()                      # Tool usage stats')\n",
    "print('  export_logs_to_readable()               # Export to .txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and verify environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking API credentials...\n",
      "\n",
      "‚úÖ OpenAI API key found\n",
      "\n",
      "ü§ñ Selected Model: <strands.models.openai.OpenAIModel object at 0x0000010D37980050>\n",
      "\n",
      "üìÅ Project directories:\n",
      "  Prompts: True - d:\\Strands-agent\\prompts\n",
      "  Data: True - d:\\Strands-agent\\data\n",
      "  Output: True - d:\\Strands-agent\\data\\tailored_versions\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "PROMPTS_DIR = PROJECT_ROOT / \"prompts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ORIGINAL_RESUME_DIR = DATA_DIR / \"original\"\n",
    "JOB_POSTINGS_DIR = DATA_DIR / \"job_postings\"\n",
    "OUTPUT_DIR = DATA_DIR / \"tailored_versions\"\n",
    "\n",
    "# Detect which API credentials are available\n",
    "print(\"üîç Checking API credentials...\")\n",
    "print()\n",
    "\n",
    "has_openai = bool(os.getenv('OPENAI_API_KEY'))\n",
    "has_bedrock_token = bool(os.getenv('AWS_BEARER_TOKEN_BEDROCK'))\n",
    "has_aws_creds = bool(os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "\n",
    "from strands.models import openai\n",
    "\n",
    "if has_openai:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "    #MODEL_ID = openai.OpenAIModel(model_id=\"gpt-5-mini\")\n",
    "    MODEL_ID = openai.OpenAIModel(\n",
    "    model_id=\"gpt-5-mini\",  # Note: prompt caching works best with gpt-4o and newer models\n",
    "    params={\n",
    "        \"store\": True,  # Enable prompt caching\n",
    "        \"metadata\": {\n",
    "            \"purpose\": \"resume_tailoring\"  # Optional: track usage\n",
    "        }\n",
    "    }\n",
    ")\n",
    "elif has_bedrock_token:\n",
    "    print(\"‚úÖ AWS Bedrock bearer token found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "elif has_aws_creds:\n",
    "    print(\"‚úÖ AWS credentials found\")\n",
    "    MODEL_PROVIDER = \"bedrock\"\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: No API credentials found!\")\n",
    "    print(\"Please set one of the following in .env file:\")\n",
    "    print(\"  - OPENAI_API_KEY (for OpenAI)\")\n",
    "    print(\"  - AWS_BEARER_TOKEN_BEDROCK (for Bedrock)\")\n",
    "    print(\"  - AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY (for AWS)\")\n",
    "    MODEL_PROVIDER = None\n",
    "    MODEL_ID = None\n",
    "\n",
    "print()\n",
    "print(f\"ü§ñ Selected Model: {MODEL_ID}\")\n",
    "\n",
    "# Verify directories exist\n",
    "print()\n",
    "print(f\"üìÅ Project directories:\")\n",
    "print(f\"  Prompts: {PROMPTS_DIR.exists()} - {PROMPTS_DIR}\")\n",
    "print(f\"  Data: {DATA_DIR.exists()} - {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR.exists()} - {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load System Prompts\n",
    "\n",
    "Load agent instructions from separate files for easy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded system_prompt.txt (7417 chars)\n",
      "\n",
      "üìù Full system prompt: 7416 characters\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(filename: str) -> str:\n",
    "    \"\"\"Load a prompt from the prompts directory.\"\"\"\n",
    "    prompt_path = PROMPTS_DIR / filename\n",
    "    if not prompt_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {filename} not found. Using default prompt.\")\n",
    "        return \"\"\n",
    "    \n",
    "    with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ Loaded {filename} ({len(content)} chars)\")\n",
    "    return content\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = load_prompt(\"system_prompt.txt\")\n",
    "latex_rules = \"\"\n",
    "\n",
    "# Combine prompts\n",
    "full_prompt = f\"{system_prompt}\\n\\n{latex_rules}\".strip()\n",
    "\n",
    "print(f\"\\nüìù Full system prompt: {len(full_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools for Resume Tailoring\n",
    "\n",
    "Define specialized tools for LaTeX resume processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom tools defined:\n",
      "  - read_file()\n",
      "  - write_file()\n",
      "  - validate_latex()\n",
      "  - extract_keywords()\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a file and return its contents.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "    \n",
    "    Returns:\n",
    "        The file contents as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_file(filepath: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write content to a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file (relative to project root or absolute)\n",
    "        content: Content to write\n",
    "    \n",
    "    Returns:\n",
    "        Success message with file path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        if not path.is_absolute():\n",
    "            path = PROJECT_ROOT / filepath\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return f\"Successfully wrote {len(content)} characters to {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_latex(latex_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Validate LaTeX syntax by checking for common issues.\n",
    "    \n",
    "    Args:\n",
    "        latex_content: The LaTeX content to validate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results (is_valid, errors, warnings)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for balanced braces\n",
    "    if latex_content.count('{') != latex_content.count('}'):\n",
    "        errors.append(\"Unbalanced curly braces { }\")\n",
    "    \n",
    "    # Check for balanced brackets\n",
    "    if latex_content.count('[') != latex_content.count(']'):\n",
    "        errors.append(\"Unbalanced square brackets [ ]\")\n",
    "    \n",
    "    # Check for document structure\n",
    "    if '\\\\documentclass' not in latex_content:\n",
    "        warnings.append(\"No \\\\documentclass found\")\n",
    "    \n",
    "    if '\\\\begin{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\begin{document}\")\n",
    "    \n",
    "    if '\\\\end{document}' not in latex_content:\n",
    "        errors.append(\"Missing \\\\end{document}\")\n",
    "    \n",
    "    # Check for common LaTeX commands\n",
    "    lines = latex_content.split('\\n')\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        # Check for unescaped special characters in regular text\n",
    "        if '%' in line and '\\\\%' not in line:\n",
    "            # This might be a comment, so it's just a warning\n",
    "            pass\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": is_valid,\n",
    "        \"errors\": errors,\n",
    "        \"warnings\": warnings,\n",
    "        \"summary\": f\"{'‚úÖ Valid' if is_valid else '‚ùå Invalid'} LaTeX ({len(errors)} errors, {len(warnings)} warnings)\"\n",
    "    }\n",
    "\n",
    "\n",
    "@tool\n",
    "def extract_keywords(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract important keywords from text (job posting or resume section).\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract keywords from\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords (skills, technologies, requirements)\n",
    "    \"\"\"\n",
    "    # Common technical keywords and skills\n",
    "    import re\n",
    "    \n",
    "    # Simple keyword extraction (can be enhanced with NLP)\n",
    "    keywords = set()\n",
    "    \n",
    "    # Common technical skills patterns\n",
    "    patterns = [\n",
    "        r'\\b(Python|Java|JavaScript|TypeScript|C\\+\\+|Ruby|Go|Rust|Swift)\\b',\n",
    "        r'\\b(AWS|Azure|GCP|Docker|Kubernetes|Jenkins)\\b',\n",
    "        r'\\b(React|Angular|Vue|Node\\.js|Django|Flask|Spring)\\b',\n",
    "        r'\\b(SQL|PostgreSQL|MySQL|MongoDB|Redis)\\b',\n",
    "        r'\\b(Git|CI/CD|Agile|Scrum|DevOps|REST|API)\\b',\n",
    "        r'\\b(Machine Learning|AI|Data Science|Analytics)\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            keywords.add(match.group(1))\n",
    "    \n",
    "    return sorted(list(keywords))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom tools defined:\")\n",
    "print(\"  - read_file()\")\n",
    "print(\"  - write_file()\")\n",
    "print(\"  - validate_latex()\")\n",
    "print(\"  - extract_keywords()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Resume Tailor Agent\n",
    "\n",
    "Initialize the agent with system prompts and custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resume Tailor Agent created!\n",
      "   Provider: OpenAI\n",
      "   Model: <strands.models.openai.OpenAIModel object at 0x0000010D37980050>\n",
      "   Tools: 9 tools available\n",
      "   System prompt: 7416 characters\n",
      "\n",
      "Available tools:\n",
      "  ‚Ä¢ read_file, write_file, validate_latex, extract_keywords\n",
      "  ‚Ä¢ extract_section, replace_section, update_subtitle\n",
      "  ‚Ä¢ merge_sections, get_section_names\n",
      "\n",
      "üí° Tip: You can change the model by editing MODEL_PROVIDER and MODEL_ID in the configuration cell above\n"
     ]
    }
   ],
   "source": [
    "# Create agent with automatic provider detection\n",
    "MODEL_PROVIDER = \"OpenAI\"\n",
    "if MODEL_PROVIDER is None:\n",
    "    print(\"‚ùå Cannot create agent: No API credentials found\")\n",
    "    print(\"Please configure API credentials in .env file\")\n",
    "else:\n",
    "    # Import section updater tools\n",
    "    from tools.section_updater import (\n",
    "        extract_section,\n",
    "        replace_section,\n",
    "        update_subtitle,\n",
    "        merge_sections,\n",
    "        get_section_names\n",
    "    )\n",
    "    \n",
    "    # Create agent with detected provider and ALL tools\n",
    "    resume_agent = Agent(\n",
    "        model=MODEL_ID,\n",
    "        system_prompt=full_prompt if full_prompt else \"You are a helpful resume tailoring assistant.\",\n",
    "        tools=[\n",
    "            read_file,\n",
    "            write_file,\n",
    "            validate_latex,\n",
    "            extract_keywords,\n",
    "            # Section updater tools\n",
    "            extract_section,\n",
    "            replace_section,\n",
    "            update_subtitle,\n",
    "            merge_sections,\n",
    "            get_section_names\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Resume Tailor Agent created!\")\n",
    "    print(f\"   Provider: {MODEL_PROVIDER}\")\n",
    "    print(f\"   Model: {MODEL_ID}\")\n",
    "    print(f\"   Tools: {len(resume_agent.tool_names)} tools available\")\n",
    "    print(f\"   System prompt: {len(full_prompt) if full_prompt else 0} characters\")\n",
    "    print()\n",
    "    print(\"Available tools:\")\n",
    "    print(\"  ‚Ä¢ read_file, write_file, validate_latex, extract_keywords\")\n",
    "    print(\"  ‚Ä¢ extract_section, replace_section, update_subtitle\")\n",
    "    print(\"  ‚Ä¢ merge_sections, get_section_names\")\n",
    "    print()\n",
    "    print(\"üí° Tip: You can change the model by editing MODEL_PROVIDER and MODEL_ID in the configuration cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "Below are examples of how to use the resume tailor agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Resume Tailoring Workflow\n",
    "\n",
    "**Note**: You'll need to place your actual LaTeX resume in `data/original/resume.tex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing resume and job posting...\n",
      "\n",
      "\n",
      "Tool #1: read_file\n",
      "\n",
      "Tool #2: read_file\n",
      "I have loaded both files and reviewed the job posting and your LaTeX resume. Below is a focused analysis, mapping of requirements ‚Üí resume content, concrete edit recommendations (including exact rephrases you can drop into the LaTeX), and identified gaps.\n",
      "\n",
      "1) Job posting ‚Äî key requirements & keywords\n",
      "- Must-have / high-priority:\n",
      "  - 100% coding in Python; proven ability to build frameworks, APIs, or developer platforms\n",
      "  - Hands-on experience building AI solutions + supporting infrastructure\n",
      "  - Multi-agent workflows and orchestration frameworks\n",
      "  - RAG patterns and retrieval pipelines\n",
      "  - Docker, containers, Kubernetes; deploying containers and managing enterprise-grade K8s\n",
      "  - LLM orchestration frameworks, multi-agent architectures, conversational AI pipelines\n",
      "  - Emphasis on custom development (developer-facing code) over purely infra setup\n",
      "\n",
      "- Nice-to-have / specific product names:\n",
      "  - MCP-Context-Forge (agentic AI orchestration & context mgmt)\n",
      "  - Semantic Kernel\n",
      "  - Experience managing enterprise-grade K8s environments (explicit ops experience)\n",
      "\n",
      "2) How your current resume maps to the posting\n",
      "- Strong matches:\n",
      "  - Python: listed as primary language in Technical Proficiencies and appears in summary ‚Äî good match for \"100% coding in Python\".\n",
      "  - RAG / Agentic AI: you already mention RAG and \"Agentic AI patterns (tool use, orchestration, memory)\" in the summary and Bedrock experience in both summary and Tech Profs.\n",
      "  - LLMs & Bedrock / SageMaker: you have direct experience packaging/deploying models to SageMaker and Bedrock, and exploring GenAI patterns on Bedrock.\n",
      "  - Containers: Docker is present; you built Dockerized apps at Meta. ECS/Fargate and EKS are in the Cloud & Infra row.\n",
      "  - Orchestration: Step Functions and experience designing workflows on AWS is present (Step Functions, Glue, EMR).\n",
      "  - Developer-facing services: Built Flask/React services and app playgrounds, built internal tooling ‚Äî evidence of developer platform/API work.\n",
      "\n",
      "- Partial matches / areas that need emphasis:\n",
      "  - Kubernetes/EKS exists in Technical Proficiencies, but your current role bullets emphasize ECS/Fargate and Lambda more than K8s. Job asks for enterprise-grade K8s deployments ‚Äî you should bring EKS/Kubernetes experience to the foreground.\n",
      "  - Multi-agent / orchestration frameworks: you mention agentic patterns and exploring them on Bedrock; the job expects hands-on multi-agent orchestration frameworks. Strengthen wording to show orchestration experience (Step Functions, agentic prototypes, orchestration design).\n",
      "  - LLM orchestration frameworks (named frameworks like Semantic Kernel / MCP-Context-Forge) ‚Äî these specific frameworks are NOT in your resume.\n",
      "\n",
      "3) Concrete edit suggestions (what to change and exact rephrases you can use)\n",
      "- Subtitle\n",
      "  - Update the subtitle to exactly match the job title (LaTeX-esape &). Use: AI Cloud \\& Infrastructure Engineer\n",
      "\n",
      "- Professional Summary ‚Äî rewrite to target role (‚â§150 words)\n",
      "  Suggested new summary (one paragraph, ~100‚Äì140 words):\n",
      "  \"AI Cloud & Infrastructure Engineer who codes primarily in \\textbf{Python} to build developer-facing AI frameworks, APIs, and production model services. Hands-on with end-to-end ML systems on AWS ‚Äî packaging and deploying models to \\textbf{SageMaker} and \\textbf{Bedrock}, implementing batch and low-latency inference, and defining observability/monitoring. Built containerized developer platforms (Docker), and operate with Kubernetes (\\textbf{EKS}) and container services (ECS/Fargate) for scalable model hosting. Experienced with RAG/retrieval pipelines and agentic/multi-agent orchestration patterns (tool use, orchestration, memory) and translating DS/product requirements into IaC-backed work packages (Terraform/CloudFormation). Focused on custom developer tooling and orchestration to transition POCs to production.\"\n",
      "\n",
      "  - Notes: this uses only technologies present in your resume and highlights Python-first development, orchestration, RAG, containers/K8s, and developer-facing platforms.\n",
      "\n",
      "- Technical Proficiencies ‚Äî reorder & emphasize categories that match job\n",
      "  Create 4‚Äì5 rows (3‚Äì5 rows required). Suggested rows and exact wording you can use inside your existing \\resumeEntryS lines (bold key items to match ATS):\n",
      "\n",
      "  - Languages: \\textbf{Python} (primary), SQL, Bash, JavaScript\n",
      "  - Containers \\& Orchestration: \\textbf{Docker}, Kubernetes (\\textbf{EKS}), ECS/Fargate, containerized deployments\n",
      "  - MLOps \\& LLMs: \\textbf{SageMaker}, \\textbf{Bedrock}, RAG / retrieval pipelines, agentic/multi-agent patterns, PyTorch, HuggingFace\n",
      "  - Cloud \\& Infra: \\textbf{AWS} (VPC, IAM, S3, Lambda, API Gateway, Step Functions, CloudWatch), \\textbf{Terraform}/CloudFormation, Docker, CI/CD\n",
      "  - Big Data & Pipelines (optional): Apache Spark (EMR), Airflow (MWAA), Glue, Presto\n",
      "\n",
      "  - Notes: This emphasizes containers/k8s and LLM/RAG experience, and keeps AWS/IaC visibility. Keep categories consistent with your original structure but reorder lines so Containers & Orchestration and MLOps are prominent.\n",
      "\n",
      "- Experience bullets ‚Äî reorder and rephrase to surface most relevant items at top\n",
      "  For AWS Cloud Support Engineer ‚Äî suggested new bullet order (and exact rephrases you can paste in):\n",
      "\n",
      "  1) Replace first bullet with (emphasize Python, containers, K8s):\n",
      "     \"Built and supported end-to-end ML systems on AWS using \\textbf{Python}: packaged and deployed models to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}; implemented batch pipelines (Glue/EMR + Step Functions) and low-latency inference via API Gateway + Lambda and containerized services on ECS/Fargate and \\textbf{EKS}.\"\n",
      "\n",
      "  2) Rephrase IaC/developer-platform bullet:\n",
      "     \"Translated product and data-science requirements into cloud work packages and IaC (\\textbf{Terraform}/CloudFormation; CDK), delivering developer-facing APIs and frameworks that cover networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.\"\n",
      "\n",
      "  3) Rephrase monitoring/MLOps bullet to highlight RAG/LLM readiness:\n",
      "     \"Define and review monitoring and model-quality plans (CloudWatch logs/metrics/alarms, retries, canaries) and advise on data/model checks for production readiness ‚Äî including guidance for RAG/retrieval pipelines and LLM orchestration trade-offs.\"\n",
      "\n",
      "  4) Consolidate/strengthen multi-agent & GenAI bullet:\n",
      "     \"Designed and prototyped agentic/multi-agent orchestration patterns (tool use, context management, memory) on \\textbf{Bedrock} and Step Functions; provided feasibility guidance for integrating LLM orchestration and retrieval-based flows into existing microservices.\"\n",
      "\n",
      "  - Rationale: these four bullets push Python, containers & K8s, RAG and multi-agent orchestration to the top ‚Äî matching the job focus on developer-centric AI infra.\n",
      "\n",
      "  For Prompt Engineer & Software Engineer (Meta) ‚Äî rephrase to highlight developer platform, APIs, and Python:\n",
      "  - Replace second bullet with:\n",
      "    \"Built a Dockerized app playground with a \\textbf{Flask} (Python) backend and React UI to launch/stop containerized services via REST ‚Äî added status tracking, error handling, and audit logs to standardize model hosting and experimentation environments.\"\n",
      "\n",
      "  - Keep the Chrome/JS bullet but you can bring the Docker/Flask bullet above it.\n",
      "\n",
      "- Technical Proficiencies ‚Äî bold the most relevant words (when generating LaTeX later) so ATS picks up Python, Docker, EKS/Kubernetes, Bedrock, SageMaker, RAG, agentic.\n",
      "\n",
      "4) Reordering suggestions\n",
      "- Move the Technical Proficiencies section slightly higher or ensure it‚Äôs visible above the Experience section on the page so the ATS and recruiter immediately see Python / Containers / K8s / MLOps emphasis. If keeping 1-page layout, you can instead reorder bullets inside Experience so the most relevant items are at the top of each role (as suggested above).\n",
      "\n",
      "5) Gaps to call out (do not add or claim these on resume)\n",
      "- MCP-Context-Forge: Not present on your resume. The posting lists it explicitly; do NOT invent this. Prepare to discuss how your agentic/Bedrock experience maps to MCP concepts and how you would learn/adapt quickly.\n",
      "- Semantic Kernel: Not present. Similarly, do not add; be ready to say you have relevant agentic orchestration experience and can learn Semantic Kernel quickly.\n",
      "- Enterprise-grade Kubernetes ops detail: You list EKS, but you do not currently show explicit cluster ops (Helm, k8s cluster admin, monitoring, CI/CD for k8s). The job emphasizes managing enterprise-grade K8s environments ‚Äî be prepared to talk about any EKS operational experience you have, or what parts of EKS/ECS you've managed (deployments, scaling, security), and your familiarity with container lifecycle and CI/CD.\n",
      "- Named LLM orchestration frameworks (LangChain, etc.): Not present beyond Bedrock and agentic patterns. Again, do not invent their presence.\n",
      "\n",
      "6) Interview / mitigation suggestions\n",
      "- If you lack MCP-Context-Forge and Semantic Kernel, plan a short statement: emphasize you have built agentic patterns on Bedrock, designed orchestration using Step Functions and custom frameworks, and have hands-on container/K8s experience ‚Äî and that you can quickly learn/experiment with MCP-Context-Forge or Semantic Kernel.\n",
      "- Prepare 1‚Äì2 concise examples of Python code you wrote for orchestration / developer tooling (scripts, Flask APIs, Docker tooling) ‚Äî interviewers will want to validate the \"100% Python\" claim.\n",
      "\n",
      "7) Summary of exact edits to apply (quick checklist)\n",
      "- Update subtitle to: AI Cloud \\& Infrastructure Engineer\n",
      "- Replace Professional Summary with the suggested targeted paragraph above (‚â§150 words).\n",
      "- Reorder and rewrite Technical Proficiencies rows to emphasize Python, Containers & Orchestration (Docker, EKS), MLOps & LLMs (SageMaker, Bedrock, RAG), Cloud & IaC (AWS, Terraform).\n",
      "- Modify AWS Cloud Support Engineer bullets as suggested (4 bullets), bringing container/K8s and Python to top and emphasizing RAG & multi-agent orchestration.\n",
      "- Modify Meta role Dockerized-app bullet to explicitly say Flask (Python) backend and developer platform intent.\n",
      "- Bold the high-value keywords in Technical Proficiencies when updating LaTeX (\\textbf{Python}, \\textbf{Docker}, \\textbf{EKS}, \\textbf{SageMaker}, \\textbf{Bedrock}, RAG).\n",
      "\n",
      "If you want, I can now:\n",
      "- Generate the three updated sections required by the SECTION-ONLY mode (SUBTITLE, PROFESSIONAL SUMMARY, TECHNICAL PROFICIENCIES) in the exact LaTeX macro format from your template and then call merge_sections(). Tell me to \"generate\" when you want me to produce those LaTeX sections.I have loaded both files and reviewed the job posting and your LaTeX resume. Below is a focused analysis, mapping of requirements ‚Üí resume content, concrete edit recommendations (including exact rephrases you can drop into the LaTeX), and identified gaps.\n",
      "\n",
      "1) Job posting ‚Äî key requirements & keywords\n",
      "- Must-have / high-priority:\n",
      "  - 100% coding in Python; proven ability to build frameworks, APIs, or developer platforms\n",
      "  - Hands-on experience building AI solutions + supporting infrastructure\n",
      "  - Multi-agent workflows and orchestration frameworks\n",
      "  - RAG patterns and retrieval pipelines\n",
      "  - Docker, containers, Kubernetes; deploying containers and managing enterprise-grade K8s\n",
      "  - LLM orchestration frameworks, multi-agent architectures, conversational AI pipelines\n",
      "  - Emphasis on custom development (developer-facing code) over purely infra setup\n",
      "\n",
      "- Nice-to-have / specific product names:\n",
      "  - MCP-Context-Forge (agentic AI orchestration & context mgmt)\n",
      "  - Semantic Kernel\n",
      "  - Experience managing enterprise-grade K8s environments (explicit ops experience)\n",
      "\n",
      "2) How your current resume maps to the posting\n",
      "- Strong matches:\n",
      "  - Python: listed as primary language in Technical Proficiencies and appears in summary ‚Äî good match for \"100% coding in Python\".\n",
      "  - RAG / Agentic AI: you already mention RAG and \"Agentic AI patterns (tool use, orchestration, memory)\" in the summary and Bedrock experience in both summary and Tech Profs.\n",
      "  - LLMs & Bedrock / SageMaker: you have direct experience packaging/deploying models to SageMaker and Bedrock, and exploring GenAI patterns on Bedrock.\n",
      "  - Containers: Docker is present; you built Dockerized apps at Meta. ECS/Fargate and EKS are in the Cloud & Infra row.\n",
      "  - Orchestration: Step Functions and experience designing workflows on AWS is present (Step Functions, Glue, EMR).\n",
      "  - Developer-facing services: Built Flask/React services and app playgrounds, built internal tooling ‚Äî evidence of developer platform/API work.\n",
      "\n",
      "- Partial matches / areas that need emphasis:\n",
      "  - Kubernetes/EKS exists in Technical Proficiencies, but your current role bullets emphasize ECS/Fargate and Lambda more than K8s. Job asks for enterprise-grade K8s deployments ‚Äî you should bring EKS/Kubernetes experience to the foreground.\n",
      "  - Multi-agent / orchestration frameworks: you mention agentic patterns and exploring them on Bedrock; the job expects hands-on multi-agent orchestration frameworks. Strengthen wording to show orchestration experience (Step Functions, agentic prototypes, orchestration design).\n",
      "  - LLM orchestration frameworks (named frameworks like Semantic Kernel / MCP-Context-Forge) ‚Äî these specific frameworks are NOT in your resume.\n",
      "\n",
      "3) Concrete edit suggestions (what to change and exact rephrases you can use)\n",
      "- Subtitle\n",
      "  - Update the subtitle to exactly match the job title (LaTeX-esape &). Use: AI Cloud \\& Infrastructure Engineer\n",
      "\n",
      "- Professional Summary ‚Äî rewrite to target role (‚â§150 words)\n",
      "  Suggested new summary (one paragraph, ~100‚Äì140 words):\n",
      "  \"AI Cloud & Infrastructure Engineer who codes primarily in \\textbf{Python} to build developer-facing AI frameworks, APIs, and production model services. Hands-on with end-to-end ML systems on AWS ‚Äî packaging and deploying models to \\textbf{SageMaker} and \\textbf{Bedrock}, implementing batch and low-latency inference, and defining observability/monitoring. Built containerized developer platforms (Docker), and operate with Kubernetes (\\textbf{EKS}) and container services (ECS/Fargate) for scalable model hosting. Experienced with RAG/retrieval pipelines and agentic/multi-agent orchestration patterns (tool use, orchestration, memory) and translating DS/product requirements into IaC-backed work packages (Terraform/CloudFormation). Focused on custom developer tooling and orchestration to transition POCs to production.\"\n",
      "\n",
      "  - Notes: this uses only technologies present in your resume and highlights Python-first development, orchestration, RAG, containers/K8s, and developer-facing platforms.\n",
      "\n",
      "- Technical Proficiencies ‚Äî reorder & emphasize categories that match job\n",
      "  Create 4‚Äì5 rows (3‚Äì5 rows required). Suggested rows and exact wording you can use inside your existing \\resumeEntryS lines (bold key items to match ATS):\n",
      "\n",
      "  - Languages: \\textbf{Python} (primary), SQL, Bash, JavaScript\n",
      "  - Containers \\& Orchestration: \\textbf{Docker}, Kubernetes (\\textbf{EKS}), ECS/Fargate, containerized deployments\n",
      "  - MLOps \\& LLMs: \\textbf{SageMaker}, \\textbf{Bedrock}, RAG / retrieval pipelines, agentic/multi-agent patterns, PyTorch, HuggingFace\n",
      "  - Cloud \\& Infra: \\textbf{AWS} (VPC, IAM, S3, Lambda, API Gateway, Step Functions, CloudWatch), \\textbf{Terraform}/CloudFormation, Docker, CI/CD\n",
      "  - Big Data & Pipelines (optional): Apache Spark (EMR), Airflow (MWAA), Glue, Presto\n",
      "\n",
      "  - Notes: This emphasizes containers/k8s and LLM/RAG experience, and keeps AWS/IaC visibility. Keep categories consistent with your original structure but reorder lines so Containers & Orchestration and MLOps are prominent.\n",
      "\n",
      "- Experience bullets ‚Äî reorder and rephrase to surface most relevant items at top\n",
      "  For AWS Cloud Support Engineer ‚Äî suggested new bullet order (and exact rephrases you can paste in):\n",
      "\n",
      "  1) Replace first bullet with (emphasize Python, containers, K8s):\n",
      "     \"Built and supported end-to-end ML systems on AWS using \\textbf{Python}: packaged and deployed models to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}; implemented batch pipelines (Glue/EMR + Step Functions) and low-latency inference via API Gateway + Lambda and containerized services on ECS/Fargate and \\textbf{EKS}.\"\n",
      "\n",
      "  2) Rephrase IaC/developer-platform bullet:\n",
      "     \"Translated product and data-science requirements into cloud work packages and IaC (\\textbf{Terraform}/CloudFormation; CDK), delivering developer-facing APIs and frameworks that cover networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.\"\n",
      "\n",
      "  3) Rephrase monitoring/MLOps bullet to highlight RAG/LLM readiness:\n",
      "     \"Define and review monitoring and model-quality plans (CloudWatch logs/metrics/alarms, retries, canaries) and advise on data/model checks for production readiness ‚Äî including guidance for RAG/retrieval pipelines and LLM orchestration trade-offs.\"\n",
      "\n",
      "  4) Consolidate/strengthen multi-agent & GenAI bullet:\n",
      "     \"Designed and prototyped agentic/multi-agent orchestration patterns (tool use, context management, memory) on \\textbf{Bedrock} and Step Functions; provided feasibility guidance for integrating LLM orchestration and retrieval-based flows into existing microservices.\"\n",
      "\n",
      "  - Rationale: these four bullets push Python, containers & K8s, RAG and multi-agent orchestration to the top ‚Äî matching the job focus on developer-centric AI infra.\n",
      "\n",
      "  For Prompt Engineer & Software Engineer (Meta) ‚Äî rephrase to highlight developer platform, APIs, and Python:\n",
      "  - Replace second bullet with:\n",
      "    \"Built a Dockerized app playground with a \\textbf{Flask} (Python) backend and React UI to launch/stop containerized services via REST ‚Äî added status tracking, error handling, and audit logs to standardize model hosting and experimentation environments.\"\n",
      "\n",
      "  - Keep the Chrome/JS bullet but you can bring the Docker/Flask bullet above it.\n",
      "\n",
      "- Technical Proficiencies ‚Äî bold the most relevant words (when generating LaTeX later) so ATS picks up Python, Docker, EKS/Kubernetes, Bedrock, SageMaker, RAG, agentic.\n",
      "\n",
      "4) Reordering suggestions\n",
      "- Move the Technical Proficiencies section slightly higher or ensure it‚Äôs visible above the Experience section on the page so the ATS and recruiter immediately see Python / Containers / K8s / MLOps emphasis. If keeping 1-page layout, you can instead reorder bullets inside Experience so the most relevant items are at the top of each role (as suggested above).\n",
      "\n",
      "5) Gaps to call out (do not add or claim these on resume)\n",
      "- MCP-Context-Forge: Not present on your resume. The posting lists it explicitly; do NOT invent this. Prepare to discuss how your agentic/Bedrock experience maps to MCP concepts and how you would learn/adapt quickly.\n",
      "- Semantic Kernel: Not present. Similarly, do not add; be ready to say you have relevant agentic orchestration experience and can learn Semantic Kernel quickly.\n",
      "- Enterprise-grade Kubernetes ops detail: You list EKS, but you do not currently show explicit cluster ops (Helm, k8s cluster admin, monitoring, CI/CD for k8s). The job emphasizes managing enterprise-grade K8s environments ‚Äî be prepared to talk about any EKS operational experience you have, or what parts of EKS/ECS you've managed (deployments, scaling, security), and your familiarity with container lifecycle and CI/CD.\n",
      "- Named LLM orchestration frameworks (LangChain, etc.): Not present beyond Bedrock and agentic patterns. Again, do not invent their presence.\n",
      "\n",
      "6) Interview / mitigation suggestions\n",
      "- If you lack MCP-Context-Forge and Semantic Kernel, plan a short statement: emphasize you have built agentic patterns on Bedrock, designed orchestration using Step Functions and custom frameworks, and have hands-on container/K8s experience ‚Äî and that you can quickly learn/experiment with MCP-Context-Forge or Semantic Kernel.\n",
      "- Prepare 1‚Äì2 concise examples of Python code you wrote for orchestration / developer tooling (scripts, Flask APIs, Docker tooling) ‚Äî interviewers will want to validate the \"100% Python\" claim.\n",
      "\n",
      "7) Summary of exact edits to apply (quick checklist)\n",
      "- Update subtitle to: AI Cloud \\& Infrastructure Engineer\n",
      "- Replace Professional Summary with the suggested targeted paragraph above (‚â§150 words).\n",
      "- Reorder and rewrite Technical Proficiencies rows to emphasize Python, Containers & Orchestration (Docker, EKS), MLOps & LLMs (SageMaker, Bedrock, RAG), Cloud & IaC (AWS, Terraform).\n",
      "- Modify AWS Cloud Support Engineer bullets as suggested (4 bullets), bringing container/K8s and Python to top and emphasizing RAG & multi-agent orchestration.\n",
      "- Modify Meta role Dockerized-app bullet to explicitly say Flask (Python) backend and developer platform intent.\n",
      "- Bold the high-value keywords in Technical Proficiencies when updating LaTeX (\\textbf{Python}, \\textbf{Docker}, \\textbf{EKS}, \\textbf{SageMaker}, \\textbf{Bedrock}, RAG).\n",
      "\n",
      "If you want, I can now:\n",
      "- Generate the three updated sections required by the SECTION-ONLY mode (SUBTITLE, PROFESSIONAL SUMMARY, TECHNICAL PROFICIENCIES) in the exact LaTeX macro format from your template and then call merge_sections(). Tell me to \"generate\" when you want me to produce those LaTeX sections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file paths \n",
    "original_resume = \"data/original/Data_engineer.tex.tex\"      # Your resume here\n",
    "job_posting = \"data/job_postings/quanlom.txt\"         # Job posting\n",
    "output_file = \"data/tailored_versions/new_mini_resume.tex\"  # Output\n",
    "\n",
    "# Instructions for the agent\n",
    "tailoring_request = f\"\"\"\n",
    "You are now in ANALYSIS MODE.\n",
    "\n",
    "First, you MUST load the inputs using your tools:\n",
    "\n",
    "1. Call the `read_file` tool with the path \"{original_resume}\" to load my LaTeX resume.\n",
    "2. Call the `read_file` tool with the path \"{job_posting}\" to load the job posting text.\n",
    "\n",
    "After you have loaded BOTH files, do the following:\n",
    "\n",
    "1. Analyze the job posting and extract key requirements, responsibilities, and skills.\n",
    "2. Compare them with the content of the LaTeX resume.\n",
    "3. Suggest specific, concrete edits to the resume, for example:\n",
    "   - Which bullets to rephrase (and provide the new wording).\n",
    "   - Which skills to emphasize or de-emphasize.\n",
    "   - Any reordering of sections or bullets that would improve alignment.\n",
    "4. Point out any major gaps where the job requires something NOT present in the resume.\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT output LaTeX in this step‚Äîonly natural-language analysis and suggestions.\n",
    "- Do NOT assume the resume or job posting are already in the conversation; you must read them using `read_file` first.\n",
    "\"\"\"\n",
    "\n",
    "# Get initial analysis\n",
    "print(\"Analyzing resume and job posting...\\n\")\n",
    "analysis = resume_agent(tailoring_request)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Iterative Refinement\n",
    "\n",
    "After getting suggestions, you can iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Continue the conversation for refinement\n",
    "# refinement_request = \"\"\"\n",
    "# Based on your analysis, please:\n",
    "# 1. Focus on highlighting my AWS and Python experience\n",
    "# 2. Emphasize any machine learning projects\n",
    "# 3. Ensure keywords match the job posting for ATS\n",
    "# 4. Keep the resume to 1 page if possible\n",
    "\n",
    "# Show me the specific sections that should change.\n",
    "# \"\"\"\n",
    "\n",
    "# refinement = resume_agent(refinement_request)\n",
    "# print(refinement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Generate Final Tailored Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: read_file\n",
      "\n",
      "Tool #4: get_section_names\n",
      "\n",
      "Tool #5: extract_section\n",
      "\n",
      "Tool #6: replace_section\n",
      "\n",
      "Tool #7: replace_section\n",
      "\n",
      "Tool #8: merge_sections\n",
      "\n",
      "Tool #9: update_subtitle\n",
      "\n",
      "Tool #10: write_file\n",
      "\n",
      "Tool #11: read_file\n",
      "\n",
      "Tool #12: validate_latex\n",
      "\n",
      "Tool #13: read_file\n",
      "\n",
      "Tool #14: validate_latex\n",
      "%-------------------------------------\n",
      "% LaTeX Resume for Software Engineers\n",
      "% Author : Leslie Cheng\n",
      "% License : MIT\n",
      "%-------------------------------------\n",
      "\n",
      "\\documentclass[a4paper,10.5pt]{article}[leftmargin=*]\n",
      "\n",
      "\\usepackage[empty]{fullpage}\n",
      "\\usepackage{enumitem}\n",
      "\\usepackage{ifxetex}\n",
      "\\ifxetex\n",
      "  \\usepackage{fontspec}\n",
      "  \\usepackage[xetex]{hyperref}\n",
      "\\else\n",
      "  \\usepackage[utf8]{inputenc}\n",
      "  \\usepackage[T1]{fontenc}\n",
      "  \\usepackage[pdftex]{hyperref}\n",
      "\\fi\n",
      "\\usepackage{fontawesome}\n",
      "\\usepackage[sfdefault,light]{FiraSans}\n",
      "\\usepackage{anyfontsize}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{tabularx}\n",
      "\n",
      "%-------------------------------------------------- SETTINGS HERE --------------------------------------------------\n",
      "% Header settings\n",
      "\\def \\fullname {Yurui Feng}\n",
      "\\def \\subtitle {AI Cloud \\& Infrastructure Engineer}\n",
      "\n",
      "\\def \\linkedinicon {\\faLinkedin}\n",
      "\\def \\linkedinlink {https://linkedin.com/in/yurui-feng/}\n",
      "\\def \\linkedintext {/yurui-feng}\n",
      "\n",
      "\\def \\phoneicon {\\faPhone}\n",
      "\\def \\phonetext {+1 778-323-9562}\n",
      "\n",
      "\\def \\emailicon {\\faEnvelope}\n",
      "\\def \\emaillink {mailto:yurui8rigby@gmail.com}\n",
      "\\def \\emailtext {yurui8rigby@gmail.com}\n",
      "\n",
      "\\def \\githubicon {\\faGithub}\n",
      "\\def \\githublink {https://github.com/Yurui-Feng}\n",
      "\\def \\githubtext {/Yurui-Feng}\n",
      "\n",
      "\\def \\locationicon{\\faHome}\n",
      "\\def \\locationtext {Toronto, ON, M5J 0C3}\n",
      "\n",
      "\\def \\websiteicon {\\faGlobe}\n",
      "\\def \\websitelink {https://www.fuyuri.com/}\n",
      "\\def \\websitetext {fuyuri.com}\n",
      "\n",
      "\n",
      "\\def \\headertype {\\doublecol} % \\singlecol or \\doublecol\n",
      "\n",
      "% Misc settings\n",
      "\\def \\entryspacing {0pt}\n",
      "\n",
      "\\def \\bulletstyle {\\faAngleRight}\n",
      "\n",
      "% Define colours\n",
      "\\definecolor{primary}{HTML}{000000}\n",
      "\\definecolor{secondary}{HTML}{0D47A1}\n",
      "\\definecolor{accent}{HTML}{263238}\n",
      "\\definecolor{links}{HTML}{1565C0}\n",
      "\n",
      "%------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "% Defines to make listing easier\n",
      "\\def \\linkedin {\\linkedinicon \\hspace{3pt}\\href{\\linkedinlink}{\\linkedintext}}\n",
      "\\def \\phone {\\phoneicon \\hspace{3pt}{ \\phonetext}}\n",
      "\\def \\email {\\emailicon \\hspace{3pt}\\href{\\emaillink}{\\emailtext}}\n",
      "\\def \\github {\\githubicon \\hspace{3pt}\\href{\\githublink}{\\githubtext}}\n",
      "\\def \\location {\\locationicon \\hspace{3pt}{ \\locationtext}}\n",
      "\\def \\website {\\websiteicon \\hspace{3pt}\\href{\\websitelink}{\\websitetext}}\n",
      "\n",
      "% Adjust margins\n",
      "\\addtolength{\\oddsidemargin}{-0.55in}\n",
      "\\addtolength{\\evensidemargin}{-0.55in}\n",
      "\\addtolength{\\textwidth}{1.1in}\n",
      "\\addtolength{\\topmargin}{-0.7in}\n",
      "\\addtolength{\\textheight}{1.1in}\n",
      "\n",
      "% Define the link colours\n",
      "\\hypersetup{\n",
      "    colorlinks=true,\n",
      "    urlcolor=links,\n",
      "}\n",
      "\n",
      "% Set the margin alignment \n",
      "\\raggedbottom\n",
      "\\raggedright\n",
      "\\setlength{\\tabcolsep}{0in}\n",
      "\n",
      "%-------------------------\n",
      "% Custom commands\n",
      "\n",
      "\n",
      "% Sections\n",
      "\\renewcommand{\\section}[2]{\\vspace{7pt}\n",
      "  \\colorbox{secondary}{\\color{white}\\raggedbottom\\normalsize\\textbf{{#1}{\\hspace{8pt}#2}}}\n",
      "}\n",
      "\n",
      "% Entry start and end, for spacing\n",
      "\\newcommand{\\resumeEntryStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeEntryEnd}{\\end{itemize}\\vspace{\\entryspacing}}\n",
      "\n",
      "% Itemized list for the bullet points under an entry, if necessary\n",
      "\\newcommand{\\resumeItemListStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeItemListEnd}{\\end{itemize}}\n",
      "\n",
      "% Resume item\n",
      "\\renewcommand{\\labelitemii}{\\bulletstyle}\n",
      "\\newcommand{\\resumeItem}[1]{\n",
      "  \\item\\small{\n",
      "    {#1 \\vspace{-2pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Entry with title, subheading, date(s), and location\n",
      "\\newcommand{\\resumeEntryTSDL}[4]{\n",
      "  \\vspace{-3pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "      \\textit{\\color{accent}\\small#3} & \\textit{\\color{accent}\\small#4} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry with title and date(s)\n",
      "\\newcommand{\\resumeEntryTD}[2]{\n",
      "  \\vspace{-1pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry for special (skills)\n",
      "\\newcommand{\\resumeEntryS}[2]{\n",
      "  \\item[]\\small{\n",
      "    \\textbf{\\color{primary}#1 }{ #2 \\vspace{-6pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Double column header\n",
      "\\newcommand{\\doublecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l@{\\hspace{1.5em}}l}\n",
      "        {\\small#4} & {\\small#1} \\\\\n",
      "        {\\small#5} & {\\small#2} \\\\\n",
      "        {\\small#6} & {\\small#3}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "% Single column header\n",
      "\\newcommand{\\singlecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[b]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        {\\small#1} \\\\\n",
      "        {\\small#2} \\\\\n",
      "        {\\small#3} \\\\\n",
      "        {\\small#4} \\\\\n",
      "        {\\small#5} \\\\\n",
      "        {\\small#6}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "%-------------------------------------------------- BEGIN HERE --------------------------------------------------\n",
      "\n",
      "%---------------------------------------------------- HEADER ----------------------------------------------------\n",
      "\n",
      "\\headertype{\\linkedin}{\\github}{\\website}{\\phone}{\\location}{\\email}{} % Set the order of items here\n",
      "\\vspace{-10pt} % Set a negative value to push the body up, and the opposite\n",
      "\n",
      "% ======= TARGETED SUMMARY =======\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{AI Cloud \\& Infrastructure Engineer who codes primarily in \\textbf{Python} to build developer-facing AI frameworks, APIs, and production model services. Hands-on with end-to-end ML systems on AWS ‚Äî packaging and deploying models to \\textbf{SageMaker} and \\textbf{Bedrock}, implementing batch and low-latency inference, and defining observability and monitoring. Built containerized developer platforms with \\textbf{Docker} and operate with Kubernetes (\\textbf{EKS}) and ECS/Fargate for scalable model hosting. Experienced with RAG/retrieval pipelines and agentic/multi-agent orchestration patterns (tool use, context management, memory). Translate DS/product requirements into IaC-backed work packages (\\textbf{Terraform}/CloudFormation) and focus on custom developer tooling to transition POCs to production.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- EDUCATION --------------------------------------------------\n",
      "\\section{\\faGraduationCap}{Education}\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of British Columbia}{09/2022 -- 07/2023}\n",
      "      {Master of Data Science (GPA: 4.0/4.0)}{Vancouver, BC}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {Boston University}{09/2020 -- 05/2022}\n",
      "      {Master of Arts, Psychology (GPA: 3.9/4.0)}{Boston, MA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of California, San Diego}{09/2017 -- 03/2020}\n",
      "      {B.S., Cognitive and Behavioral Neuroscience (GPA:3.8/4.0)}{San Diego, CA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- PROFESSIONAL EXPERIENCE --------------------------------------------------\n",
      "\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Built and supported end-to-end ML systems on AWS using \\textbf{Python}: packaged and deployed models to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}; implemented batch pipelines (Glue/EMR + \\textbf{Step Functions}) and low-latency inference via API Gateway + Lambda and containerized services on ECS/Fargate and \\textbf{EKS}.}\n",
      "    \\resumeItem{Translated product and data-science requirements into \\textbf{cloud work packages} and IaC (\\textbf{Terraform}/CloudFormation; CDK), delivering developer-facing APIs and frameworks covering networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.}\n",
      "    \\resumeItem{Defined and reviewed monitoring and model-quality plans (CloudWatch logs/metrics/alarms, retries, canaries) and advised on data/model checks for production readiness ‚Äî including guidance for RAG/retrieval pipelines and LLM orchestration trade-offs.}\n",
      "    \\resumeItem{Designed and prototyped agentic/multi-agent orchestration patterns (tool use, context management, memory) on \\textbf{Bedrock} and Step Functions; provided feasibility guidance for integrating LLM orchestration and retrieval-based flows into existing microservices.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Improved LLM outcomes by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an \\textbf{agile} environment to productize evaluation workflows.}\n",
      "    \\resumeItem{Built a \\textbf{Dockerized} app playground with a \\textbf{Flask} (Python) backend and \\textbf{React} UI to launch/stop containerized services via REST ‚Äî added status tracking, error handling, and audit logs to standardize model hosting and experimentation environments.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome} tool (JavaScript/DOM) for high-throughput dataset operations supporting model experimentation.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed a time-series/spatial \\textbf{ML pipeline} over 5M rows to model urban sound distribution; partnered with stakeholders to move analyses into a client-facing app and define \\textbf{monitoring/visualization} needs.}\n",
      "    \\resumeItem{Delivered an \\textbf{Altair} heat-map component for decision support and downstream consumers.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "% ======= TECHNICAL SKILLS =======\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{\\textbf{Python} (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{Containers \\& Orchestration}{\\textbf{Docker}, Kubernetes (\\textbf{EKS}), ECS/Fargate, containerized deployments}\n",
      "  \\resumeEntryS{MLOps \\& LLMs}{\\textbf{SageMaker}, \\textbf{Bedrock}, RAG / retrieval pipelines, agentic/multi-agent patterns, PyTorch, HuggingFace}\n",
      "  \\resumeEntryS{Cloud \\& Infra}{\\textbf{AWS} (VPC, IAM, S3, API Gateway, Lambda, CloudWatch, Step Functions), \\textbf{Terraform}/CloudFormation, CI/CD}\n",
      "  \\resumeEntryS{Big Data \\& Pipelines}{\\textbf{Apache Spark} (EMR), Airflow (MWAA), AWS Glue, Presto}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\n",
      "%-------------------------------------------------- CERTIFICATIONS --------------------------------------------------\n",
      "\\section{\\faCertificate}{Certifications}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{AWS Certified Machine Learning - Specialty } {Amazon Web Services Training and Certification - 2023}\n",
      "  \\resumeEntryS{AWS Certified Solutions Architect - Associate } {Amazon Web Services Training and Certification - 2024}\n",
      "  \\resumeEntryS{Computing in Python IV: Objects \\& Algorithms } {Georgia Institute of Technology - edX}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\\end{document}%-------------------------------------\n",
      "% LaTeX Resume for Software Engineers\n",
      "% Author : Leslie Cheng\n",
      "% License : MIT\n",
      "%-------------------------------------\n",
      "\n",
      "\\documentclass[a4paper,10.5pt]{article}[leftmargin=*]\n",
      "\n",
      "\\usepackage[empty]{fullpage}\n",
      "\\usepackage{enumitem}\n",
      "\\usepackage{ifxetex}\n",
      "\\ifxetex\n",
      "  \\usepackage{fontspec}\n",
      "  \\usepackage[xetex]{hyperref}\n",
      "\\else\n",
      "  \\usepackage[utf8]{inputenc}\n",
      "  \\usepackage[T1]{fontenc}\n",
      "  \\usepackage[pdftex]{hyperref}\n",
      "\\fi\n",
      "\\usepackage{fontawesome}\n",
      "\\usepackage[sfdefault,light]{FiraSans}\n",
      "\\usepackage{anyfontsize}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{tabularx}\n",
      "\n",
      "%-------------------------------------------------- SETTINGS HERE --------------------------------------------------\n",
      "% Header settings\n",
      "\\def \\fullname {Yurui Feng}\n",
      "\\def \\subtitle {AI Cloud \\& Infrastructure Engineer}\n",
      "\n",
      "\\def \\linkedinicon {\\faLinkedin}\n",
      "\\def \\linkedinlink {https://linkedin.com/in/yurui-feng/}\n",
      "\\def \\linkedintext {/yurui-feng}\n",
      "\n",
      "\\def \\phoneicon {\\faPhone}\n",
      "\\def \\phonetext {+1 778-323-9562}\n",
      "\n",
      "\\def \\emailicon {\\faEnvelope}\n",
      "\\def \\emaillink {mailto:yurui8rigby@gmail.com}\n",
      "\\def \\emailtext {yurui8rigby@gmail.com}\n",
      "\n",
      "\\def \\githubicon {\\faGithub}\n",
      "\\def \\githublink {https://github.com/Yurui-Feng}\n",
      "\\def \\githubtext {/Yurui-Feng}\n",
      "\n",
      "\\def \\locationicon{\\faHome}\n",
      "\\def \\locationtext {Toronto, ON, M5J 0C3}\n",
      "\n",
      "\\def \\websiteicon {\\faGlobe}\n",
      "\\def \\websitelink {https://www.fuyuri.com/}\n",
      "\\def \\websitetext {fuyuri.com}\n",
      "\n",
      "\n",
      "\\def \\headertype {\\doublecol} % \\singlecol or \\doublecol\n",
      "\n",
      "% Misc settings\n",
      "\\def \\entryspacing {0pt}\n",
      "\n",
      "\\def \\bulletstyle {\\faAngleRight}\n",
      "\n",
      "% Define colours\n",
      "\\definecolor{primary}{HTML}{000000}\n",
      "\\definecolor{secondary}{HTML}{0D47A1}\n",
      "\\definecolor{accent}{HTML}{263238}\n",
      "\\definecolor{links}{HTML}{1565C0}\n",
      "\n",
      "%------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "% Defines to make listing easier\n",
      "\\def \\linkedin {\\linkedinicon \\hspace{3pt}\\href{\\linkedinlink}{\\linkedintext}}\n",
      "\\def \\phone {\\phoneicon \\hspace{3pt}{ \\phonetext}}\n",
      "\\def \\email {\\emailicon \\hspace{3pt}\\href{\\emaillink}{\\emailtext}}\n",
      "\\def \\github {\\githubicon \\hspace{3pt}\\href{\\githublink}{\\githubtext}}\n",
      "\\def \\location {\\locationicon \\hspace{3pt}{ \\locationtext}}\n",
      "\\def \\website {\\websiteicon \\hspace{3pt}\\href{\\websitelink}{\\websitetext}}\n",
      "\n",
      "% Adjust margins\n",
      "\\addtolength{\\oddsidemargin}{-0.55in}\n",
      "\\addtolength{\\evensidemargin}{-0.55in}\n",
      "\\addtolength{\\textwidth}{1.1in}\n",
      "\\addtolength{\\topmargin}{-0.7in}\n",
      "\\addtolength{\\textheight}{1.1in}\n",
      "\n",
      "% Define the link colours\n",
      "\\hypersetup{\n",
      "    colorlinks=true,\n",
      "    urlcolor=links,\n",
      "}\n",
      "\n",
      "% Set the margin alignment \n",
      "\\raggedbottom\n",
      "\\raggedright\n",
      "\\setlength{\\tabcolsep}{0in}\n",
      "\n",
      "%-------------------------\n",
      "% Custom commands\n",
      "\n",
      "\n",
      "% Sections\n",
      "\\renewcommand{\\section}[2]{\\vspace{7pt}\n",
      "  \\colorbox{secondary}{\\color{white}\\raggedbottom\\normalsize\\textbf{{#1}{\\hspace{8pt}#2}}}\n",
      "}\n",
      "\n",
      "% Entry start and end, for spacing\n",
      "\\newcommand{\\resumeEntryStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeEntryEnd}{\\end{itemize}\\vspace{\\entryspacing}}\n",
      "\n",
      "% Itemized list for the bullet points under an entry, if necessary\n",
      "\\newcommand{\\resumeItemListStart}{\\begin{itemize}[leftmargin=0mm]}\n",
      "\\newcommand{\\resumeItemListEnd}{\\end{itemize}}\n",
      "\n",
      "% Resume item\n",
      "\\renewcommand{\\labelitemii}{\\bulletstyle}\n",
      "\\newcommand{\\resumeItem}[1]{\n",
      "  \\item\\small{\n",
      "    {#1 \\vspace{-2pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Entry with title, subheading, date(s), and location\n",
      "\\newcommand{\\resumeEntryTSDL}[4]{\n",
      "  \\vspace{-3pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "      \\textit{\\color{accent}\\small#3} & \\textit{\\color{accent}\\small#4} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry with title and date(s)\n",
      "\\newcommand{\\resumeEntryTD}[2]{\n",
      "  \\vspace{-1pt}\\item[]\n",
      "    \\begin{tabularx}{0.97\\textwidth}{X@{\\hspace{60pt}}r}\n",
      "      \\textbf{\\color{primary}#1} & {\\firabook\\color{accent}\\small#2} \\\\\n",
      "    \\end{tabularx}\\vspace{-6pt}\n",
      "}\n",
      "\n",
      "% Entry for special (skills)\n",
      "\\newcommand{\\resumeEntryS}[2]{\n",
      "  \\item[]\\small{\n",
      "    \\textbf{\\color{primary}#1 }{ #2 \\vspace{-6pt}}\n",
      "  }\n",
      "}\n",
      "\n",
      "% Double column header\n",
      "\\newcommand{\\doublecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l@{\\hspace{1.5em}}l}\n",
      "        {\\small#4} & {\\small#1} \\\\\n",
      "        {\\small#5} & {\\small#2} \\\\\n",
      "        {\\small#6} & {\\small#3}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "% Single column header\n",
      "\\newcommand{\\singlecol}[6]{\n",
      "  \\begin{tabularx}{\\textwidth}{Xr}\n",
      "    {\n",
      "      \\begin{tabular}[b]{l}\n",
      "        \\fontsize{35}{45}\\selectfont{\\color{primary}{{\\textbf{\\fullname}}}} \\\\\n",
      "        {\\textit{\\subtitle}} % You could add a subtitle here\n",
      "      \\end{tabular}\n",
      "    } & {\n",
      "      \\begin{tabular}[c]{l}\n",
      "        {\\small#1} \\\\\n",
      "        {\\small#2} \\\\\n",
      "        {\\small#3} \\\\\n",
      "        {\\small#4} \\\\\n",
      "        {\\small#5} \\\\\n",
      "        {\\small#6}\n",
      "      \\end{tabular}\n",
      "    }\n",
      "  \\end{tabularx}\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "%-------------------------------------------------- BEGIN HERE --------------------------------------------------\n",
      "\n",
      "%---------------------------------------------------- HEADER ----------------------------------------------------\n",
      "\n",
      "\\headertype{\\linkedin}{\\github}{\\website}{\\phone}{\\location}{\\email}{} % Set the order of items here\n",
      "\\vspace{-10pt} % Set a negative value to push the body up, and the opposite\n",
      "\n",
      "% ======= TARGETED SUMMARY =======\n",
      "\\section{\\faUser}{Professional Summary}\n",
      "\\resumeEntryStart\n",
      "\\resumeEntryS{}{AI Cloud \\& Infrastructure Engineer who codes primarily in \\textbf{Python} to build developer-facing AI frameworks, APIs, and production model services. Hands-on with end-to-end ML systems on AWS ‚Äî packaging and deploying models to \\textbf{SageMaker} and \\textbf{Bedrock}, implementing batch and low-latency inference, and defining observability and monitoring. Built containerized developer platforms with \\textbf{Docker} and operate with Kubernetes (\\textbf{EKS}) and ECS/Fargate for scalable model hosting. Experienced with RAG/retrieval pipelines and agentic/multi-agent orchestration patterns (tool use, context management, memory). Translate DS/product requirements into IaC-backed work packages (\\textbf{Terraform}/CloudFormation) and focus on custom developer tooling to transition POCs to production.}\n",
      "\\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- EDUCATION --------------------------------------------------\n",
      "\\section{\\faGraduationCap}{Education}\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of British Columbia}{09/2022 -- 07/2023}\n",
      "      {Master of Data Science (GPA: 4.0/4.0)}{Vancouver, BC}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {Boston University}{09/2020 -- 05/2022}\n",
      "      {Master of Arts, Psychology (GPA: 3.9/4.0)}{Boston, MA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "  \\resumeEntryStart\n",
      "    \\resumeEntryTSDL\n",
      "      {University of California, San Diego}{09/2017 -- 03/2020}\n",
      "      {B.S., Cognitive and Behavioral Neuroscience (GPA:3.8/4.0)}{San Diego, CA}\n",
      "  \\resumeEntryEnd\n",
      "\n",
      "%-------------------------------------------------- PROFESSIONAL EXPERIENCE --------------------------------------------------\n",
      "\n",
      "\\section{\\faBriefcase}{Professional Experience}\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Cloud Support Engineer ‚Äì Big Data Specialty}{01/2025 -- Present}\n",
      "    {Amazon Web Services}{Toronto, ON | Full-time}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Built and supported end-to-end ML systems on AWS using \\textbf{Python}: packaged and deployed models to \\textbf{SageMaker} endpoints and \\textbf{Bedrock}; implemented batch pipelines (Glue/EMR + \\textbf{Step Functions}) and low-latency inference via API Gateway + Lambda and containerized services on ECS/Fargate and \\textbf{EKS}.}\n",
      "    \\resumeItem{Translated product and data-science requirements into \\textbf{cloud work packages} and IaC (\\textbf{Terraform}/CloudFormation; CDK), delivering developer-facing APIs and frameworks covering networking (VPC, IAM), storage (\\textbf{S3}), security, scaling, and cost controls.}\n",
      "    \\resumeItem{Defined and reviewed monitoring and model-quality plans (CloudWatch logs/metrics/alarms, retries, canaries) and advised on data/model checks for production readiness ‚Äî including guidance for RAG/retrieval pipelines and LLM orchestration trade-offs.}\n",
      "    \\resumeItem{Designed and prototyped agentic/multi-agent orchestration patterns (tool use, context management, memory) on \\textbf{Bedrock} and Step Functions; provided feasibility guidance for integrating LLM orchestration and retrieval-based flows into existing microservices.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {Prompt Engineer \\& Software Engineer}{05/2024 -- 12/2024}\n",
      "    {Meta (Contract via TEKsystems)}{Toronto, ON}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Improved LLM outcomes by refining prompts and evaluation guidelines; collaborated with researchers and engineers in an \\textbf{agile} environment to productize evaluation workflows.}\n",
      "    \\resumeItem{Built a \\textbf{Dockerized} app playground with a \\textbf{Flask} (Python) backend and \\textbf{React} UI to launch/stop containerized services via REST ‚Äî added status tracking, error handling, and audit logs to standardize model hosting and experimentation environments.}\n",
      "    \\resumeItem{Enhanced an internal \\textbf{Chrome} tool (JavaScript/DOM) for high-throughput dataset operations supporting model experimentation.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "\\resumeEntryStart\n",
      "  \\resumeEntryTSDL\n",
      "    {ML Scientist (MDS Capstone) ‚Äî Citysage}{05/2023 -- 07/2023}\n",
      "    {Vancouver, BC}{}\n",
      "  \\resumeItemListStart\n",
      "    \\resumeItem{Developed a time-series/spatial \\textbf{ML pipeline} over 5M rows to model urban sound distribution; partnered with stakeholders to move analyses into a client-facing app and define \\textbf{monitoring/visualization} needs.}\n",
      "    \\resumeItem{Delivered an \\textbf{Altair} heat-map component for decision support and downstream consumers.}\n",
      "  \\resumeItemListEnd\n",
      "\\resumeEntryEnd\n",
      "\n",
      "% ======= TECHNICAL SKILLS =======\n",
      "\\section{\\faGears}{Technical Proficiencies}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{Languages}{\\textbf{Python} (primary), SQL, Bash, JavaScript}\n",
      "  \\resumeEntryS{Containers \\& Orchestration}{\\textbf{Docker}, Kubernetes (\\textbf{EKS}), ECS/Fargate, containerized deployments}\n",
      "  \\resumeEntryS{MLOps \\& LLMs}{\\textbf{SageMaker}, \\textbf{Bedrock}, RAG / retrieval pipelines, agentic/multi-agent patterns, PyTorch, HuggingFace}\n",
      "  \\resumeEntryS{Cloud \\& Infra}{\\textbf{AWS} (VPC, IAM, S3, API Gateway, Lambda, CloudWatch, Step Functions), \\textbf{Terraform}/CloudFormation, CI/CD}\n",
      "  \\resumeEntryS{Big Data \\& Pipelines}{\\textbf{Apache Spark} (EMR), Airflow (MWAA), AWS Glue, Presto}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\n",
      "%-------------------------------------------------- CERTIFICATIONS --------------------------------------------------\n",
      "\\section{\\faCertificate}{Certifications}\n",
      " \\resumeEntryStart\n",
      "  \\resumeEntryS{AWS Certified Machine Learning - Specialty } {Amazon Web Services Training and Certification - 2023}\n",
      "  \\resumeEntryS{AWS Certified Solutions Architect - Associate } {Amazon Web Services Training and Certification - 2024}\n",
      "  \\resumeEntryS{Computing in Python IV: Objects \\& Algorithms } {Georgia Institute of Technology - edX}\n",
      " \\resumeEntryEnd\n",
      "\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the final version\n",
    "final_request = f\"\"\"\n",
    "You are now in GENERATE MODE.\n",
    "\n",
    "Your goal is to produce a tailored LaTeX resume for the job posting we analyzed earlier and save it to \"{output_file}\".\n",
    "\n",
    "You have these tools: read_file, write_file, validate_latex, extract_section, replace_section, update_subtitle, merge_sections, get_section_names.\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. Call `read_file` with \"{original_resume}\" to load the ORIGINAL LaTeX resume.\n",
    "2. Identify the sections that should be updated for this role\n",
    "   (typically \"Professional Summary\", \"Professional Experience\", and \"Technical Proficiencies\").\n",
    "   You may use `get_section_names` and `extract_section` to inspect them.\n",
    "3. For each relevant section, rewrite the LaTeX content using the SAME macros and structure\n",
    "   (e.g., \\\\resumeEntryStart, \\\\resumeItem, \\\\resumeEntryS, etc.) so that it better matches\n",
    "   the job posting requirements, based on your previous analysis.\n",
    "4. Use `merge_sections` to merge your updated section LaTeX into the original file:\n",
    "   - original_file = \"{original_resume}\"\n",
    "   - updated_sections = {{\n",
    "       \"Professional Summary\": <your new LaTeX for that section>,\n",
    "       \"Professional Experience\": <your new LaTeX for that section>,\n",
    "       \"Technical Proficiencies\": <your new LaTeX for that section>,\n",
    "       \"subtitle\": <optional new subtitle/job title>\n",
    "     }}\n",
    "   - output_file = \"{output_file}\"\n",
    "\n",
    "\n",
    "FINAL RESPONSE REQUIREMENTS:\n",
    "- In your final assistant message, RETURN ONLY the COMPLETE UPDATED LATEX SOURCE\n",
    "  (the contents of \"{output_file}\") with NO commentary, NO markdown fences, and NO tool logs.\n",
    "- Do NOT modify the LaTeX preamble (\\\\documentclass, \\\\usepackage, \\\\def, \\\\newcommand).\n",
    "- Only modify text inside existing commands and environments (sections, bullets, skills, etc.).\n",
    "\"\"\"\n",
    "\n",
    "# This is burning tokens, shouldn't let the agent do this.\n",
    "# \"5. After merging, call `read_file` on \"{output_file}\" to get the full updated LaTeX content.\n",
    "# 6. Call `validate_latex` on the full updated LaTeX to check for obvious syntax issues\n",
    "#    (unbalanced braces, missing \\\\begin{{document}} / \\\\end{{document}}, etc.).\n",
    "#    If validation reports errors, fix the LaTeX and re-run `validate_latex` until it passes.\"\n",
    "\n",
    "result = resume_agent(final_request)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Valid: N/A\n",
      "  Errors: 0\n",
      "  Warnings: 0\n"
     ]
    }
   ],
   "source": [
    "# Direct tool invocation for validation\n",
    "if Path(output_file).exists():\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        tailored_content = f.read()\n",
    "    \n",
    "    # Validate using tool directly\n",
    "    validation = resume_agent.tool.validate_latex(latex_content=tailored_content)\n",
    "    \n",
    "    # Handle different return formats (dict, string, or other)\n",
    "    import json\n",
    "    if isinstance(validation, dict):\n",
    "        validation_dict = validation\n",
    "    elif isinstance(validation, str):\n",
    "        # Try to parse as JSON if it's a string\n",
    "        try:\n",
    "            validation_dict = json.loads(validation)\n",
    "        except json.JSONDecodeError:\n",
    "            # If not JSON, print the raw result\n",
    "            print(\"Validation Results (raw):\")\n",
    "            print(validation)\n",
    "            validation_dict = None\n",
    "    else:\n",
    "        print(f\"Validation returned unexpected type: {type(validation)}\")\n",
    "        print(f\"Value: {validation}\")\n",
    "        validation_dict = None\n",
    "    \n",
    "    if validation_dict:\n",
    "        print(\"Validation Results:\")\n",
    "        print(f\"  Valid: {validation_dict.get('is_valid', 'N/A')}\")\n",
    "        print(f\"  Errors: {len(validation_dict.get('errors', []))}\")\n",
    "        print(f\"  Warnings: {len(validation_dict.get('warnings', []))}\")\n",
    "        \n",
    "        if validation_dict.get('errors'):\n",
    "            print(\"\\nErrors found:\")\n",
    "            for error in validation_dict['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "        \n",
    "        if validation_dict.get('warnings'):\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in validation_dict['warnings']:\n",
    "                print(f\"  - {warning}\")\n",
    "        \n",
    "        if validation_dict.get('summary'):\n",
    "            print(f\"\\n{validation_dict['summary']}\")\n",
    "else:\n",
    "    print(f\"File not found: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "Utility functions for common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quick_tailor(resume_path: str, job_path: str, output_path: str, instructions: str = \"\"):\n",
    "#     \"\"\"\n",
    "#     Quick one-shot resume tailoring.\n",
    "    \n",
    "#     Args:\n",
    "#         resume_path: Path to original resume\n",
    "#         job_path: Path to job posting\n",
    "#         output_path: Path for tailored resume\n",
    "#         instructions: Additional instructions for the agent\n",
    "#     \"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "# Tailor my resume for this job posting.\n",
    "\n",
    "# Resume: {resume_path}\n",
    "# Job Posting: {job_path}\n",
    "# Output: {output_path}\n",
    "\n",
    "# Steps:\n",
    "# 1. Read both files\n",
    "# 2. Analyze job requirements\n",
    "# 3. Tailor resume content (preserve LaTeX formatting)\n",
    "# 4. Validate LaTeX syntax\n",
    "# 5. Save to output path\n",
    "\n",
    "# {instructions if instructions else ''}\n",
    "# \"\"\"\n",
    "    \n",
    "#     response = resume_agent(prompt)\n",
    "#     return response\n",
    "\n",
    "\n",
    "# def batch_tailor(resume_path: str, job_folder: str, output_folder: str):\n",
    "#     \"\"\"\n",
    "#     Tailor resume for multiple job postings.\n",
    "    \n",
    "#     Args:\n",
    "#         resume_path: Path to original resume\n",
    "#         job_folder: Folder containing job posting files\n",
    "#         output_folder: Folder for tailored resumes\n",
    "#     \"\"\"\n",
    "#     job_dir = Path(job_folder)\n",
    "#     output_dir = Path(output_folder)\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for job_file in job_dir.glob(\"*.txt\"):\n",
    "#         output_name = f\"resume_{job_file.stem}.tex\"\n",
    "#         output_path = output_dir / output_name\n",
    "        \n",
    "#         print(f\"\\nTailoring for: {job_file.name}\")\n",
    "#         result = quick_tailor(resume_path, str(job_file), str(output_path))\n",
    "#         results.append({\"job\": job_file.name, \"output\": output_name, \"result\": result})\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# print(\"‚úÖ Helper functions defined:\")\n",
    "# print(\"  - quick_tailor()\")\n",
    "# print(\"  - batch_tailor()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add your resume**: Place your LaTeX resume in `data/original/resume.tex`\n",
    "2. **Add job postings**: Save job postings as `.txt` files in `data/job_postings/`\n",
    "3. **Run tailoring**: Use the examples above to tailor your resume\n",
    "4. **Iterate**: Work with the agent to refine the output\n",
    "5. **Validate**: Check LaTeX syntax before compiling\n",
    "6. **Compile**: Use `pdflatex` or your LaTeX editor to generate PDF\n",
    "\n",
    "### Tips for Best Results\n",
    "\n",
    "- Start with analysis and suggestions before generating the full resume\n",
    "- Be specific about what aspects to highlight\n",
    "- Review the agent's suggestions before applying them\n",
    "- Always validate LaTeX syntax\n",
    "- Keep conversation context for iterative improvements\n",
    "- Save different versions for different job types\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **LaTeX errors**: Use `validate_latex()` tool to check syntax\n",
    "- **Agent not following instructions**: Refine the system prompt in `prompts/system_prompt.txt`\n",
    "- **Missing features**: Add custom tools as needed\n",
    "- **Context lost**: Use conversation memory or save intermediate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
